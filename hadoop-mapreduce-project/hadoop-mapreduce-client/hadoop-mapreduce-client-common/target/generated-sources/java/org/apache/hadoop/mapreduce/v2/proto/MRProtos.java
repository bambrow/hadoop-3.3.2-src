// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: mr_protos.proto

package org.apache.hadoop.mapreduce.v2.proto;

public final class MRProtos {
  private MRProtos() {}
  public static void registerAllExtensions(
      org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * Protobuf enum {@code hadoop.mapreduce.TaskTypeProto}
   */
  public enum TaskTypeProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>MAP = 1;</code>
     */
    MAP(1),
    /**
     * <code>REDUCE = 2;</code>
     */
    REDUCE(2),
    ;

    /**
     * <code>MAP = 1;</code>
     */
    public static final int MAP_VALUE = 1;
    /**
     * <code>REDUCE = 2;</code>
     */
    public static final int REDUCE_VALUE = 2;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static TaskTypeProto valueOf(int value) {
      return forNumber(value);
    }

    public static TaskTypeProto forNumber(int value) {
      switch (value) {
        case 1: return MAP;
        case 2: return REDUCE;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskTypeProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        TaskTypeProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskTypeProto>() {
            public TaskTypeProto findValueByNumber(int number) {
              return TaskTypeProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final TaskTypeProto[] VALUES = values();

    public static TaskTypeProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private TaskTypeProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.mapreduce.TaskTypeProto)
  }

  /**
   * Protobuf enum {@code hadoop.mapreduce.TaskStateProto}
   */
  public enum TaskStateProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>TS_NEW = 1;</code>
     */
    TS_NEW(1),
    /**
     * <code>TS_SCHEDULED = 2;</code>
     */
    TS_SCHEDULED(2),
    /**
     * <code>TS_RUNNING = 3;</code>
     */
    TS_RUNNING(3),
    /**
     * <code>TS_SUCCEEDED = 4;</code>
     */
    TS_SUCCEEDED(4),
    /**
     * <code>TS_FAILED = 5;</code>
     */
    TS_FAILED(5),
    /**
     * <code>TS_KILLED = 6;</code>
     */
    TS_KILLED(6),
    ;

    /**
     * <code>TS_NEW = 1;</code>
     */
    public static final int TS_NEW_VALUE = 1;
    /**
     * <code>TS_SCHEDULED = 2;</code>
     */
    public static final int TS_SCHEDULED_VALUE = 2;
    /**
     * <code>TS_RUNNING = 3;</code>
     */
    public static final int TS_RUNNING_VALUE = 3;
    /**
     * <code>TS_SUCCEEDED = 4;</code>
     */
    public static final int TS_SUCCEEDED_VALUE = 4;
    /**
     * <code>TS_FAILED = 5;</code>
     */
    public static final int TS_FAILED_VALUE = 5;
    /**
     * <code>TS_KILLED = 6;</code>
     */
    public static final int TS_KILLED_VALUE = 6;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static TaskStateProto valueOf(int value) {
      return forNumber(value);
    }

    public static TaskStateProto forNumber(int value) {
      switch (value) {
        case 1: return TS_NEW;
        case 2: return TS_SCHEDULED;
        case 3: return TS_RUNNING;
        case 4: return TS_SUCCEEDED;
        case 5: return TS_FAILED;
        case 6: return TS_KILLED;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        TaskStateProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskStateProto>() {
            public TaskStateProto findValueByNumber(int number) {
              return TaskStateProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.getDescriptor().getEnumTypes().get(1);
    }

    private static final TaskStateProto[] VALUES = values();

    public static TaskStateProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private TaskStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.mapreduce.TaskStateProto)
  }

  /**
   * Protobuf enum {@code hadoop.mapreduce.PhaseProto}
   */
  public enum PhaseProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>P_STARTING = 1;</code>
     */
    P_STARTING(1),
    /**
     * <code>P_MAP = 2;</code>
     */
    P_MAP(2),
    /**
     * <code>P_SHUFFLE = 3;</code>
     */
    P_SHUFFLE(3),
    /**
     * <code>P_SORT = 4;</code>
     */
    P_SORT(4),
    /**
     * <code>P_REDUCE = 5;</code>
     */
    P_REDUCE(5),
    /**
     * <code>P_CLEANUP = 6;</code>
     */
    P_CLEANUP(6),
    ;

    /**
     * <code>P_STARTING = 1;</code>
     */
    public static final int P_STARTING_VALUE = 1;
    /**
     * <code>P_MAP = 2;</code>
     */
    public static final int P_MAP_VALUE = 2;
    /**
     * <code>P_SHUFFLE = 3;</code>
     */
    public static final int P_SHUFFLE_VALUE = 3;
    /**
     * <code>P_SORT = 4;</code>
     */
    public static final int P_SORT_VALUE = 4;
    /**
     * <code>P_REDUCE = 5;</code>
     */
    public static final int P_REDUCE_VALUE = 5;
    /**
     * <code>P_CLEANUP = 6;</code>
     */
    public static final int P_CLEANUP_VALUE = 6;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static PhaseProto valueOf(int value) {
      return forNumber(value);
    }

    public static PhaseProto forNumber(int value) {
      switch (value) {
        case 1: return P_STARTING;
        case 2: return P_MAP;
        case 3: return P_SHUFFLE;
        case 4: return P_SORT;
        case 5: return P_REDUCE;
        case 6: return P_CLEANUP;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<PhaseProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        PhaseProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<PhaseProto>() {
            public PhaseProto findValueByNumber(int number) {
              return PhaseProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.getDescriptor().getEnumTypes().get(2);
    }

    private static final PhaseProto[] VALUES = values();

    public static PhaseProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private PhaseProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.mapreduce.PhaseProto)
  }

  /**
   * Protobuf enum {@code hadoop.mapreduce.TaskAttemptStateProto}
   */
  public enum TaskAttemptStateProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>TA_NEW = 1;</code>
     */
    TA_NEW(1),
    /**
     * <code>TA_STARTING = 2;</code>
     */
    TA_STARTING(2),
    /**
     * <code>TA_RUNNING = 3;</code>
     */
    TA_RUNNING(3),
    /**
     * <code>TA_COMMIT_PENDING = 4;</code>
     */
    TA_COMMIT_PENDING(4),
    /**
     * <code>TA_SUCCEEDED = 5;</code>
     */
    TA_SUCCEEDED(5),
    /**
     * <code>TA_FAILED = 6;</code>
     */
    TA_FAILED(6),
    /**
     * <code>TA_KILLED = 7;</code>
     */
    TA_KILLED(7),
    ;

    /**
     * <code>TA_NEW = 1;</code>
     */
    public static final int TA_NEW_VALUE = 1;
    /**
     * <code>TA_STARTING = 2;</code>
     */
    public static final int TA_STARTING_VALUE = 2;
    /**
     * <code>TA_RUNNING = 3;</code>
     */
    public static final int TA_RUNNING_VALUE = 3;
    /**
     * <code>TA_COMMIT_PENDING = 4;</code>
     */
    public static final int TA_COMMIT_PENDING_VALUE = 4;
    /**
     * <code>TA_SUCCEEDED = 5;</code>
     */
    public static final int TA_SUCCEEDED_VALUE = 5;
    /**
     * <code>TA_FAILED = 6;</code>
     */
    public static final int TA_FAILED_VALUE = 6;
    /**
     * <code>TA_KILLED = 7;</code>
     */
    public static final int TA_KILLED_VALUE = 7;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static TaskAttemptStateProto valueOf(int value) {
      return forNumber(value);
    }

    public static TaskAttemptStateProto forNumber(int value) {
      switch (value) {
        case 1: return TA_NEW;
        case 2: return TA_STARTING;
        case 3: return TA_RUNNING;
        case 4: return TA_COMMIT_PENDING;
        case 5: return TA_SUCCEEDED;
        case 6: return TA_FAILED;
        case 7: return TA_KILLED;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskAttemptStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        TaskAttemptStateProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskAttemptStateProto>() {
            public TaskAttemptStateProto findValueByNumber(int number) {
              return TaskAttemptStateProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.getDescriptor().getEnumTypes().get(3);
    }

    private static final TaskAttemptStateProto[] VALUES = values();

    public static TaskAttemptStateProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private TaskAttemptStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.mapreduce.TaskAttemptStateProto)
  }

  /**
   * Protobuf enum {@code hadoop.mapreduce.JobStateProto}
   */
  public enum JobStateProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>J_NEW = 1;</code>
     */
    J_NEW(1),
    /**
     * <code>J_INITED = 2;</code>
     */
    J_INITED(2),
    /**
     * <code>J_RUNNING = 3;</code>
     */
    J_RUNNING(3),
    /**
     * <code>J_SUCCEEDED = 4;</code>
     */
    J_SUCCEEDED(4),
    /**
     * <code>J_FAILED = 5;</code>
     */
    J_FAILED(5),
    /**
     * <code>J_KILLED = 6;</code>
     */
    J_KILLED(6),
    /**
     * <code>J_ERROR = 7;</code>
     */
    J_ERROR(7),
    ;

    /**
     * <code>J_NEW = 1;</code>
     */
    public static final int J_NEW_VALUE = 1;
    /**
     * <code>J_INITED = 2;</code>
     */
    public static final int J_INITED_VALUE = 2;
    /**
     * <code>J_RUNNING = 3;</code>
     */
    public static final int J_RUNNING_VALUE = 3;
    /**
     * <code>J_SUCCEEDED = 4;</code>
     */
    public static final int J_SUCCEEDED_VALUE = 4;
    /**
     * <code>J_FAILED = 5;</code>
     */
    public static final int J_FAILED_VALUE = 5;
    /**
     * <code>J_KILLED = 6;</code>
     */
    public static final int J_KILLED_VALUE = 6;
    /**
     * <code>J_ERROR = 7;</code>
     */
    public static final int J_ERROR_VALUE = 7;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static JobStateProto valueOf(int value) {
      return forNumber(value);
    }

    public static JobStateProto forNumber(int value) {
      switch (value) {
        case 1: return J_NEW;
        case 2: return J_INITED;
        case 3: return J_RUNNING;
        case 4: return J_SUCCEEDED;
        case 5: return J_FAILED;
        case 6: return J_KILLED;
        case 7: return J_ERROR;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<JobStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        JobStateProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<JobStateProto>() {
            public JobStateProto findValueByNumber(int number) {
              return JobStateProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.getDescriptor().getEnumTypes().get(4);
    }

    private static final JobStateProto[] VALUES = values();

    public static JobStateProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private JobStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.mapreduce.JobStateProto)
  }

  /**
   * Protobuf enum {@code hadoop.mapreduce.TaskAttemptCompletionEventStatusProto}
   */
  public enum TaskAttemptCompletionEventStatusProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>TACE_FAILED = 1;</code>
     */
    TACE_FAILED(1),
    /**
     * <code>TACE_KILLED = 2;</code>
     */
    TACE_KILLED(2),
    /**
     * <code>TACE_SUCCEEDED = 3;</code>
     */
    TACE_SUCCEEDED(3),
    /**
     * <code>TACE_OBSOLETE = 4;</code>
     */
    TACE_OBSOLETE(4),
    /**
     * <code>TACE_TIPFAILED = 5;</code>
     */
    TACE_TIPFAILED(5),
    ;

    /**
     * <code>TACE_FAILED = 1;</code>
     */
    public static final int TACE_FAILED_VALUE = 1;
    /**
     * <code>TACE_KILLED = 2;</code>
     */
    public static final int TACE_KILLED_VALUE = 2;
    /**
     * <code>TACE_SUCCEEDED = 3;</code>
     */
    public static final int TACE_SUCCEEDED_VALUE = 3;
    /**
     * <code>TACE_OBSOLETE = 4;</code>
     */
    public static final int TACE_OBSOLETE_VALUE = 4;
    /**
     * <code>TACE_TIPFAILED = 5;</code>
     */
    public static final int TACE_TIPFAILED_VALUE = 5;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static TaskAttemptCompletionEventStatusProto valueOf(int value) {
      return forNumber(value);
    }

    public static TaskAttemptCompletionEventStatusProto forNumber(int value) {
      switch (value) {
        case 1: return TACE_FAILED;
        case 2: return TACE_KILLED;
        case 3: return TACE_SUCCEEDED;
        case 4: return TACE_OBSOLETE;
        case 5: return TACE_TIPFAILED;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskAttemptCompletionEventStatusProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        TaskAttemptCompletionEventStatusProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<TaskAttemptCompletionEventStatusProto>() {
            public TaskAttemptCompletionEventStatusProto findValueByNumber(int number) {
              return TaskAttemptCompletionEventStatusProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.getDescriptor().getEnumTypes().get(5);
    }

    private static final TaskAttemptCompletionEventStatusProto[] VALUES = values();

    public static TaskAttemptCompletionEventStatusProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private TaskAttemptCompletionEventStatusProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.mapreduce.TaskAttemptCompletionEventStatusProto)
  }

  public interface JobIdProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.JobIdProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    boolean hasAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder();

    /**
     * <code>optional int32 id = 2;</code>
     */
    boolean hasId();
    /**
     * <code>optional int32 id = 2;</code>
     */
    int getId();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.JobIdProto}
   */
  public  static final class JobIdProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.JobIdProto)
      JobIdProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use JobIdProto.newBuilder() to construct.
    private JobIdProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private JobIdProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private JobIdProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = appId_.toBuilder();
              }
              appId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(appId_);
                appId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              id_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobIdProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobIdProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder.class);
    }

    private int bitField0_;
    public static final int APP_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    public boolean hasAppId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }

    public static final int ID_FIELD_NUMBER = 2;
    private int id_;
    /**
     * <code>optional int32 id = 2;</code>
     */
    public boolean hasId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 id = 2;</code>
     */
    public int getId() {
      return id_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAppId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, id_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAppId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(2, id_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto) obj;

      if (hasAppId() != other.hasAppId()) return false;
      if (hasAppId()) {
        if (!getAppId()
            .equals(other.getAppId())) return false;
      }
      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (getId()
            != other.getId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAppId()) {
        hash = (37 * hash) + APP_ID_FIELD_NUMBER;
        hash = (53 * hash) + getAppId().hashCode();
      }
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.JobIdProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.JobIdProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobIdProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobIdProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAppIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (appIdBuilder_ == null) {
          appId_ = null;
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobIdProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (appIdBuilder_ == null) {
            result.appId_ = appId_;
          } else {
            result.appId_ = appIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.id_ = id_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance()) return this;
        if (other.hasAppId()) {
          mergeAppId(other.getAppId());
        }
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> appIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public boolean hasAppId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
        if (appIdBuilder_ == null) {
          return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        } else {
          return appIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder setAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          appId_ = value;
          onChanged();
        } else {
          appIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder setAppId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (appIdBuilder_ == null) {
          appId_ = builderForValue.build();
          onChanged();
        } else {
          appIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder mergeAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              appId_ != null &&
              appId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            appId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(appId_).mergeFrom(value).buildPartial();
          } else {
            appId_ = value;
          }
          onChanged();
        } else {
          appIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder clearAppId() {
        if (appIdBuilder_ == null) {
          appId_ = null;
          onChanged();
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getAppIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAppIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
        if (appIdBuilder_ != null) {
          return appIdBuilder_.getMessageOrBuilder();
        } else {
          return appId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getAppIdFieldBuilder() {
        if (appIdBuilder_ == null) {
          appIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  getAppId(),
                  getParentForChildren(),
                  isClean());
          appId_ = null;
        }
        return appIdBuilder_;
      }

      private int id_ ;
      /**
       * <code>optional int32 id = 2;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 id = 2;</code>
       */
      public int getId() {
        return id_;
      }
      /**
       * <code>optional int32 id = 2;</code>
       */
      public Builder setId(int value) {
        bitField0_ |= 0x00000002;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 id = 2;</code>
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        id_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.JobIdProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.JobIdProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<JobIdProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<JobIdProto>() {
      @java.lang.Override
      public JobIdProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new JobIdProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<JobIdProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<JobIdProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskIdProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.TaskIdProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getJobId();
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder getJobIdOrBuilder();

    /**
     * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
     */
    boolean hasTaskType();
    /**
     * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto getTaskType();

    /**
     * <code>optional int32 id = 3;</code>
     */
    boolean hasId();
    /**
     * <code>optional int32 id = 3;</code>
     */
    int getId();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.TaskIdProto}
   */
  public  static final class TaskIdProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.TaskIdProto)
      TaskIdProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskIdProto.newBuilder() to construct.
    private TaskIdProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskIdProto() {
      taskType_ = 1;
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskIdProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto value = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                taskType_ = rawValue;
              }
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              id_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskIdProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskIdProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder.class);
    }

    private int bitField0_;
    public static final int JOB_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto jobId_;
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getJobId() {
      return jobId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
    }
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
    }

    public static final int TASK_TYPE_FIELD_NUMBER = 2;
    private int taskType_;
    /**
     * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
     */
    public boolean hasTaskType() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto getTaskType() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto.valueOf(taskType_);
      return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto.MAP : result;
    }

    public static final int ID_FIELD_NUMBER = 3;
    private int id_;
    /**
     * <code>optional int32 id = 3;</code>
     */
    public boolean hasId() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int32 id = 3;</code>
     */
    public int getId() {
      return id_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getJobId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, taskType_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt32(3, id_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getJobId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, taskType_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(3, id_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto) obj;

      if (hasJobId() != other.hasJobId()) return false;
      if (hasJobId()) {
        if (!getJobId()
            .equals(other.getJobId())) return false;
      }
      if (hasTaskType() != other.hasTaskType()) return false;
      if (hasTaskType()) {
        if (taskType_ != other.taskType_) return false;
      }
      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (getId()
            != other.getId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOB_ID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      if (hasTaskType()) {
        hash = (37 * hash) + TASK_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + taskType_;
      }
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.TaskIdProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.TaskIdProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskIdProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskIdProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = null;
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        taskType_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        id_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskIdProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (jobIdBuilder_ == null) {
            result.jobId_ = jobId_;
          } else {
            result.jobId_ = jobIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.taskType_ = taskType_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.id_ = id_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        if (other.hasTaskType()) {
          setTaskType(other.getTaskType());
        }
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto jobId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              jobId_ != null &&
              jobId_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = null;
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder>(
                  getJobId(),
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      private int taskType_ = 1;
      /**
       * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
       */
      public boolean hasTaskType() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto getTaskType() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto.valueOf(taskType_);
        return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto.MAP : result;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
       */
      public Builder setTaskType(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskTypeProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        taskType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskTypeProto task_type = 2;</code>
       */
      public Builder clearTaskType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        taskType_ = 1;
        onChanged();
        return this;
      }

      private int id_ ;
      /**
       * <code>optional int32 id = 3;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int32 id = 3;</code>
       */
      public int getId() {
        return id_;
      }
      /**
       * <code>optional int32 id = 3;</code>
       */
      public Builder setId(int value) {
        bitField0_ |= 0x00000004;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 id = 3;</code>
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        id_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.TaskIdProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.TaskIdProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<TaskIdProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<TaskIdProto>() {
      @java.lang.Override
      public TaskIdProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new TaskIdProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<TaskIdProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<TaskIdProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskAttemptIdProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.TaskAttemptIdProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    boolean hasTaskId();
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getTaskId();
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder getTaskIdOrBuilder();

    /**
     * <code>optional int32 id = 2;</code>
     */
    boolean hasId();
    /**
     * <code>optional int32 id = 2;</code>
     */
    int getId();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.TaskAttemptIdProto}
   */
  public  static final class TaskAttemptIdProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.TaskAttemptIdProto)
      TaskAttemptIdProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskAttemptIdProto.newBuilder() to construct.
    private TaskAttemptIdProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskAttemptIdProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskAttemptIdProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = taskId_.toBuilder();
              }
              taskId_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(taskId_);
                taskId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              id_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptIdProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptIdProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder.class);
    }

    private int bitField0_;
    public static final int TASK_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto taskId_;
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    public boolean hasTaskId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getTaskId() {
      return taskId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder getTaskIdOrBuilder() {
      return taskId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
    }

    public static final int ID_FIELD_NUMBER = 2;
    private int id_;
    /**
     * <code>optional int32 id = 2;</code>
     */
    public boolean hasId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 id = 2;</code>
     */
    public int getId() {
      return id_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getTaskId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, id_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getTaskId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(2, id_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto) obj;

      if (hasTaskId() != other.hasTaskId()) return false;
      if (hasTaskId()) {
        if (!getTaskId()
            .equals(other.getTaskId())) return false;
      }
      if (hasId() != other.hasId()) return false;
      if (hasId()) {
        if (getId()
            != other.getId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTaskId()) {
        hash = (37 * hash) + TASK_ID_FIELD_NUMBER;
        hash = (53 * hash) + getTaskId().hashCode();
      }
      if (hasId()) {
        hash = (37 * hash) + ID_FIELD_NUMBER;
        hash = (53 * hash) + getId();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.TaskAttemptIdProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.TaskAttemptIdProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptIdProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptIdProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getTaskIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (taskIdBuilder_ == null) {
          taskId_ = null;
        } else {
          taskIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        id_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptIdProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (taskIdBuilder_ == null) {
            result.taskId_ = taskId_;
          } else {
            result.taskId_ = taskIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.id_ = id_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance()) return this;
        if (other.hasTaskId()) {
          mergeTaskId(other.getTaskId());
        }
        if (other.hasId()) {
          setId(other.getId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto taskId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder> taskIdBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public boolean hasTaskId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getTaskId() {
        if (taskIdBuilder_ == null) {
          return taskId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
        } else {
          return taskIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder setTaskId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto value) {
        if (taskIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          taskId_ = value;
          onChanged();
        } else {
          taskIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder setTaskId(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder builderForValue) {
        if (taskIdBuilder_ == null) {
          taskId_ = builderForValue.build();
          onChanged();
        } else {
          taskIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder mergeTaskId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto value) {
        if (taskIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              taskId_ != null &&
              taskId_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance()) {
            taskId_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.newBuilder(taskId_).mergeFrom(value).buildPartial();
          } else {
            taskId_ = value;
          }
          onChanged();
        } else {
          taskIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder clearTaskId() {
        if (taskIdBuilder_ == null) {
          taskId_ = null;
          onChanged();
        } else {
          taskIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder getTaskIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getTaskIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder getTaskIdOrBuilder() {
        if (taskIdBuilder_ != null) {
          return taskIdBuilder_.getMessageOrBuilder();
        } else {
          return taskId_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder> 
          getTaskIdFieldBuilder() {
        if (taskIdBuilder_ == null) {
          taskIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder>(
                  getTaskId(),
                  getParentForChildren(),
                  isClean());
          taskId_ = null;
        }
        return taskIdBuilder_;
      }

      private int id_ ;
      /**
       * <code>optional int32 id = 2;</code>
       */
      public boolean hasId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 id = 2;</code>
       */
      public int getId() {
        return id_;
      }
      /**
       * <code>optional int32 id = 2;</code>
       */
      public Builder setId(int value) {
        bitField0_ |= 0x00000002;
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 id = 2;</code>
       */
      public Builder clearId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        id_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.TaskAttemptIdProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.TaskAttemptIdProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptIdProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<TaskAttemptIdProto>() {
      @java.lang.Override
      public TaskAttemptIdProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new TaskAttemptIdProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptIdProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptIdProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CounterProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.CounterProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string name = 1;</code>
     */
    boolean hasName();
    /**
     * <code>optional string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <code>optional string name = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNameBytes();

    /**
     * <code>optional string display_name = 2;</code>
     */
    boolean hasDisplayName();
    /**
     * <code>optional string display_name = 2;</code>
     */
    java.lang.String getDisplayName();
    /**
     * <code>optional string display_name = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDisplayNameBytes();

    /**
     * <code>optional int64 value = 3;</code>
     */
    boolean hasValue();
    /**
     * <code>optional int64 value = 3;</code>
     */
    long getValue();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.CounterProto}
   */
  public  static final class CounterProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.CounterProto)
      CounterProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CounterProto.newBuilder() to construct.
    private CounterProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CounterProto() {
      name_ = "";
      displayName_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CounterProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              name_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              displayName_ = bs;
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              value_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>optional string name = 1;</code>
     */
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          name_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string name = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int DISPLAY_NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object displayName_;
    /**
     * <code>optional string display_name = 2;</code>
     */
    public boolean hasDisplayName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string display_name = 2;</code>
     */
    public java.lang.String getDisplayName() {
      java.lang.Object ref = displayName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          displayName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string display_name = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDisplayNameBytes() {
      java.lang.Object ref = displayName_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        displayName_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 3;
    private long value_;
    /**
     * <code>optional int64 value = 3;</code>
     */
    public boolean hasValue() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int64 value = 3;</code>
     */
    public long getValue() {
      return value_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 2, displayName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(3, value_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(2, displayName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(3, value_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasDisplayName() != other.hasDisplayName()) return false;
      if (hasDisplayName()) {
        if (!getDisplayName()
            .equals(other.getDisplayName())) return false;
      }
      if (hasValue() != other.hasValue()) return false;
      if (hasValue()) {
        if (getValue()
            != other.getValue()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasDisplayName()) {
        hash = (37 * hash) + DISPLAY_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getDisplayName().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getValue());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.CounterProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.CounterProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        displayName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        value_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.displayName_ = displayName_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.value_ = value_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasDisplayName()) {
          bitField0_ |= 0x00000002;
          displayName_ = other.displayName_;
          onChanged();
        }
        if (other.hasValue()) {
          setValue(other.getValue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>optional string name = 1;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            name_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public Builder setNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object displayName_ = "";
      /**
       * <code>optional string display_name = 2;</code>
       */
      public boolean hasDisplayName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public java.lang.String getDisplayName() {
        java.lang.Object ref = displayName_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            displayName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDisplayNameBytes() {
        java.lang.Object ref = displayName_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          displayName_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public Builder setDisplayName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        displayName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public Builder clearDisplayName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        displayName_ = getDefaultInstance().getDisplayName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public Builder setDisplayNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        displayName_ = value;
        onChanged();
        return this;
      }

      private long value_ ;
      /**
       * <code>optional int64 value = 3;</code>
       */
      public boolean hasValue() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int64 value = 3;</code>
       */
      public long getValue() {
        return value_;
      }
      /**
       * <code>optional int64 value = 3;</code>
       */
      public Builder setValue(long value) {
        bitField0_ |= 0x00000004;
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 value = 3;</code>
       */
      public Builder clearValue() {
        bitField0_ = (bitField0_ & ~0x00000004);
        value_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.CounterProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.CounterProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<CounterProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<CounterProto>() {
      @java.lang.Override
      public CounterProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new CounterProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<CounterProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<CounterProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CounterGroupProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.CounterGroupProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string name = 1;</code>
     */
    boolean hasName();
    /**
     * <code>optional string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <code>optional string name = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNameBytes();

    /**
     * <code>optional string display_name = 2;</code>
     */
    boolean hasDisplayName();
    /**
     * <code>optional string display_name = 2;</code>
     */
    java.lang.String getDisplayName();
    /**
     * <code>optional string display_name = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDisplayNameBytes();

    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto> 
        getCountersList();
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto getCounters(int index);
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    int getCountersCount();
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder> 
        getCountersOrBuilderList();
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder getCountersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.CounterGroupProto}
   */
  public  static final class CounterGroupProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.CounterGroupProto)
      CounterGroupProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CounterGroupProto.newBuilder() to construct.
    private CounterGroupProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CounterGroupProto() {
      name_ = "";
      displayName_ = "";
      counters_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CounterGroupProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              name_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              displayName_ = bs;
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                counters_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              counters_.add(
                  input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          counters_ = java.util.Collections.unmodifiableList(counters_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterGroupProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterGroupProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>optional string name = 1;</code>
     */
    public boolean hasName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          name_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string name = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int DISPLAY_NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object displayName_;
    /**
     * <code>optional string display_name = 2;</code>
     */
    public boolean hasDisplayName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string display_name = 2;</code>
     */
    public java.lang.String getDisplayName() {
      java.lang.Object ref = displayName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          displayName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string display_name = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDisplayNameBytes() {
      java.lang.Object ref = displayName_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        displayName_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int COUNTERS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto> counters_;
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto> getCountersList() {
      return counters_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder> 
        getCountersOrBuilderList() {
      return counters_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    public int getCountersCount() {
      return counters_.size();
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto getCounters(int index) {
      return counters_.get(index);
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder getCountersOrBuilder(
        int index) {
      return counters_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 2, displayName_);
      }
      for (int i = 0; i < counters_.size(); i++) {
        output.writeMessage(3, counters_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(2, displayName_);
      }
      for (int i = 0; i < counters_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(3, counters_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto) obj;

      if (hasName() != other.hasName()) return false;
      if (hasName()) {
        if (!getName()
            .equals(other.getName())) return false;
      }
      if (hasDisplayName() != other.hasDisplayName()) return false;
      if (hasDisplayName()) {
        if (!getDisplayName()
            .equals(other.getDisplayName())) return false;
      }
      if (!getCountersList()
          .equals(other.getCountersList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasName()) {
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
      }
      if (hasDisplayName()) {
        hash = (37 * hash) + DISPLAY_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getDisplayName().hashCode();
      }
      if (getCountersCount() > 0) {
        hash = (37 * hash) + COUNTERS_FIELD_NUMBER;
        hash = (53 * hash) + getCountersList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.CounterGroupProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.CounterGroupProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterGroupProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterGroupProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCountersFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        displayName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (countersBuilder_ == null) {
          counters_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          countersBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CounterGroupProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.name_ = name_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.displayName_ = displayName_;
        if (countersBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            counters_ = java.util.Collections.unmodifiableList(counters_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.counters_ = counters_;
        } else {
          result.counters_ = countersBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance()) return this;
        if (other.hasName()) {
          bitField0_ |= 0x00000001;
          name_ = other.name_;
          onChanged();
        }
        if (other.hasDisplayName()) {
          bitField0_ |= 0x00000002;
          displayName_ = other.displayName_;
          onChanged();
        }
        if (countersBuilder_ == null) {
          if (!other.counters_.isEmpty()) {
            if (counters_.isEmpty()) {
              counters_ = other.counters_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureCountersIsMutable();
              counters_.addAll(other.counters_);
            }
            onChanged();
          }
        } else {
          if (!other.counters_.isEmpty()) {
            if (countersBuilder_.isEmpty()) {
              countersBuilder_.dispose();
              countersBuilder_ = null;
              counters_ = other.counters_;
              bitField0_ = (bitField0_ & ~0x00000004);
              countersBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getCountersFieldBuilder() : null;
            } else {
              countersBuilder_.addAllMessages(other.counters_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <code>optional string name = 1;</code>
       */
      public boolean hasName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            name_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public Builder clearName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string name = 1;</code>
       */
      public Builder setNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object displayName_ = "";
      /**
       * <code>optional string display_name = 2;</code>
       */
      public boolean hasDisplayName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public java.lang.String getDisplayName() {
        java.lang.Object ref = displayName_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            displayName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDisplayNameBytes() {
        java.lang.Object ref = displayName_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          displayName_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public Builder setDisplayName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        displayName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public Builder clearDisplayName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        displayName_ = getDefaultInstance().getDisplayName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string display_name = 2;</code>
       */
      public Builder setDisplayNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        displayName_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto> counters_ =
        java.util.Collections.emptyList();
      private void ensureCountersIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          counters_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto>(counters_);
          bitField0_ |= 0x00000004;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder> countersBuilder_;

      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto> getCountersList() {
        if (countersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(counters_);
        } else {
          return countersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public int getCountersCount() {
        if (countersBuilder_ == null) {
          return counters_.size();
        } else {
          return countersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto getCounters(int index) {
        if (countersBuilder_ == null) {
          return counters_.get(index);
        } else {
          return countersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder setCounters(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto value) {
        if (countersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCountersIsMutable();
          counters_.set(index, value);
          onChanged();
        } else {
          countersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder setCounters(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder builderForValue) {
        if (countersBuilder_ == null) {
          ensureCountersIsMutable();
          counters_.set(index, builderForValue.build());
          onChanged();
        } else {
          countersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder addCounters(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto value) {
        if (countersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCountersIsMutable();
          counters_.add(value);
          onChanged();
        } else {
          countersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder addCounters(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto value) {
        if (countersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCountersIsMutable();
          counters_.add(index, value);
          onChanged();
        } else {
          countersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder addCounters(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder builderForValue) {
        if (countersBuilder_ == null) {
          ensureCountersIsMutable();
          counters_.add(builderForValue.build());
          onChanged();
        } else {
          countersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder addCounters(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder builderForValue) {
        if (countersBuilder_ == null) {
          ensureCountersIsMutable();
          counters_.add(index, builderForValue.build());
          onChanged();
        } else {
          countersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder addAllCounters(
          java.lang.Iterable<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto> values) {
        if (countersBuilder_ == null) {
          ensureCountersIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, counters_);
          onChanged();
        } else {
          countersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder clearCounters() {
        if (countersBuilder_ == null) {
          counters_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          countersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public Builder removeCounters(int index) {
        if (countersBuilder_ == null) {
          ensureCountersIsMutable();
          counters_.remove(index);
          onChanged();
        } else {
          countersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder getCountersBuilder(
          int index) {
        return getCountersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder getCountersOrBuilder(
          int index) {
        if (countersBuilder_ == null) {
          return counters_.get(index);  } else {
          return countersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder> 
           getCountersOrBuilderList() {
        if (countersBuilder_ != null) {
          return countersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(counters_);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder addCountersBuilder() {
        return getCountersFieldBuilder().addBuilder(
            org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder addCountersBuilder(
          int index) {
        return getCountersFieldBuilder().addBuilder(
            index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterMapProto counters = 3;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder> 
           getCountersBuilderList() {
        return getCountersFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder> 
          getCountersFieldBuilder() {
        if (countersBuilder_ == null) {
          countersBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder>(
                  counters_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          counters_ = null;
        }
        return countersBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.CounterGroupProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.CounterGroupProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<CounterGroupProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<CounterGroupProto>() {
      @java.lang.Override
      public CounterGroupProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new CounterGroupProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<CounterGroupProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<CounterGroupProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CountersProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.CountersProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto> 
        getCounterGroupsList();
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto getCounterGroups(int index);
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    int getCounterGroupsCount();
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder> 
        getCounterGroupsOrBuilderList();
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder getCounterGroupsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.CountersProto}
   */
  public  static final class CountersProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.CountersProto)
      CountersProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CountersProto.newBuilder() to construct.
    private CountersProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CountersProto() {
      counterGroups_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CountersProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                counterGroups_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              counterGroups_.add(
                  input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          counterGroups_ = java.util.Collections.unmodifiableList(counterGroups_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CountersProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CountersProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder.class);
    }

    public static final int COUNTER_GROUPS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto> counterGroups_;
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto> getCounterGroupsList() {
      return counterGroups_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder> 
        getCounterGroupsOrBuilderList() {
      return counterGroups_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    public int getCounterGroupsCount() {
      return counterGroups_.size();
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto getCounterGroups(int index) {
      return counterGroups_.get(index);
    }
    /**
     * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder getCounterGroupsOrBuilder(
        int index) {
      return counterGroups_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < counterGroups_.size(); i++) {
        output.writeMessage(1, counterGroups_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < counterGroups_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, counterGroups_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto) obj;

      if (!getCounterGroupsList()
          .equals(other.getCounterGroupsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getCounterGroupsCount() > 0) {
        hash = (37 * hash) + COUNTER_GROUPS_FIELD_NUMBER;
        hash = (53 * hash) + getCounterGroupsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.CountersProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.CountersProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CountersProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CountersProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCounterGroupsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (counterGroupsBuilder_ == null) {
          counterGroups_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          counterGroupsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_CountersProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto(this);
        int from_bitField0_ = bitField0_;
        if (counterGroupsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            counterGroups_ = java.util.Collections.unmodifiableList(counterGroups_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.counterGroups_ = counterGroups_;
        } else {
          result.counterGroups_ = counterGroupsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance()) return this;
        if (counterGroupsBuilder_ == null) {
          if (!other.counterGroups_.isEmpty()) {
            if (counterGroups_.isEmpty()) {
              counterGroups_ = other.counterGroups_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureCounterGroupsIsMutable();
              counterGroups_.addAll(other.counterGroups_);
            }
            onChanged();
          }
        } else {
          if (!other.counterGroups_.isEmpty()) {
            if (counterGroupsBuilder_.isEmpty()) {
              counterGroupsBuilder_.dispose();
              counterGroupsBuilder_ = null;
              counterGroups_ = other.counterGroups_;
              bitField0_ = (bitField0_ & ~0x00000001);
              counterGroupsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getCounterGroupsFieldBuilder() : null;
            } else {
              counterGroupsBuilder_.addAllMessages(other.counterGroups_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto> counterGroups_ =
        java.util.Collections.emptyList();
      private void ensureCounterGroupsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          counterGroups_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto>(counterGroups_);
          bitField0_ |= 0x00000001;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder> counterGroupsBuilder_;

      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto> getCounterGroupsList() {
        if (counterGroupsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(counterGroups_);
        } else {
          return counterGroupsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public int getCounterGroupsCount() {
        if (counterGroupsBuilder_ == null) {
          return counterGroups_.size();
        } else {
          return counterGroupsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto getCounterGroups(int index) {
        if (counterGroupsBuilder_ == null) {
          return counterGroups_.get(index);
        } else {
          return counterGroupsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder setCounterGroups(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto value) {
        if (counterGroupsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCounterGroupsIsMutable();
          counterGroups_.set(index, value);
          onChanged();
        } else {
          counterGroupsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder setCounterGroups(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder builderForValue) {
        if (counterGroupsBuilder_ == null) {
          ensureCounterGroupsIsMutable();
          counterGroups_.set(index, builderForValue.build());
          onChanged();
        } else {
          counterGroupsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder addCounterGroups(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto value) {
        if (counterGroupsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCounterGroupsIsMutable();
          counterGroups_.add(value);
          onChanged();
        } else {
          counterGroupsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder addCounterGroups(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto value) {
        if (counterGroupsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCounterGroupsIsMutable();
          counterGroups_.add(index, value);
          onChanged();
        } else {
          counterGroupsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder addCounterGroups(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder builderForValue) {
        if (counterGroupsBuilder_ == null) {
          ensureCounterGroupsIsMutable();
          counterGroups_.add(builderForValue.build());
          onChanged();
        } else {
          counterGroupsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder addCounterGroups(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder builderForValue) {
        if (counterGroupsBuilder_ == null) {
          ensureCounterGroupsIsMutable();
          counterGroups_.add(index, builderForValue.build());
          onChanged();
        } else {
          counterGroupsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder addAllCounterGroups(
          java.lang.Iterable<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto> values) {
        if (counterGroupsBuilder_ == null) {
          ensureCounterGroupsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, counterGroups_);
          onChanged();
        } else {
          counterGroupsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder clearCounterGroups() {
        if (counterGroupsBuilder_ == null) {
          counterGroups_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          counterGroupsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public Builder removeCounterGroups(int index) {
        if (counterGroupsBuilder_ == null) {
          ensureCounterGroupsIsMutable();
          counterGroups_.remove(index);
          onChanged();
        } else {
          counterGroupsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder getCounterGroupsBuilder(
          int index) {
        return getCounterGroupsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder getCounterGroupsOrBuilder(
          int index) {
        if (counterGroupsBuilder_ == null) {
          return counterGroups_.get(index);  } else {
          return counterGroupsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder> 
           getCounterGroupsOrBuilderList() {
        if (counterGroupsBuilder_ != null) {
          return counterGroupsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(counterGroups_);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder addCounterGroupsBuilder() {
        return getCounterGroupsFieldBuilder().addBuilder(
            org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder addCounterGroupsBuilder(
          int index) {
        return getCounterGroupsFieldBuilder().addBuilder(
            index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.StringCounterGroupMapProto counter_groups = 1;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder> 
           getCounterGroupsBuilderList() {
        return getCounterGroupsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder> 
          getCounterGroupsFieldBuilder() {
        if (counterGroupsBuilder_ == null) {
          counterGroupsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder>(
                  counterGroups_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          counterGroups_ = null;
        }
        return counterGroupsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.CountersProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.CountersProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<CountersProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<CountersProto>() {
      @java.lang.Override
      public CountersProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new CountersProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<CountersProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<CountersProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskReportProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.TaskReportProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    boolean hasTaskId();
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getTaskId();
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder getTaskIdOrBuilder();

    /**
     * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
     */
    boolean hasTaskState();
    /**
     * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto getTaskState();

    /**
     * <code>optional float progress = 3;</code>
     */
    boolean hasProgress();
    /**
     * <code>optional float progress = 3;</code>
     */
    float getProgress();

    /**
     * <code>optional int64 start_time = 4;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    long getStartTime();

    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    boolean hasFinishTime();
    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    long getFinishTime();

    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    boolean hasCounters();
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getCounters();
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder getCountersOrBuilder();

    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto> 
        getRunningAttemptsList();
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getRunningAttempts(int index);
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    int getRunningAttemptsCount();
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
        getRunningAttemptsOrBuilderList();
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getRunningAttemptsOrBuilder(
        int index);

    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
     */
    boolean hasSuccessfulAttempt();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getSuccessfulAttempt();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getSuccessfulAttemptOrBuilder();

    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    java.util.List<java.lang.String>
        getDiagnosticsList();
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    int getDiagnosticsCount();
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    java.lang.String getDiagnostics(int index);
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes(int index);
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.TaskReportProto}
   */
  public  static final class TaskReportProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.TaskReportProto)
      TaskReportProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskReportProto.newBuilder() to construct.
    private TaskReportProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskReportProto() {
      taskState_ = 1;
      runningAttempts_ = java.util.Collections.emptyList();
      diagnostics_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskReportProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = taskId_.toBuilder();
              }
              taskId_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(taskId_);
                taskId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto value = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                taskState_ = rawValue;
              }
              break;
            }
            case 29: {
              bitField0_ |= 0x00000004;
              progress_ = input.readFloat();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              startTime_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              finishTime_ = input.readInt64();
              break;
            }
            case 50: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) != 0)) {
                subBuilder = counters_.toBuilder();
              }
              counters_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(counters_);
                counters_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                runningAttempts_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto>();
                mutable_bitField0_ |= 0x00000040;
              }
              runningAttempts_.add(
                  input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.PARSER, extensionRegistry));
              break;
            }
            case 66: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000040) != 0)) {
                subBuilder = successfulAttempt_.toBuilder();
              }
              successfulAttempt_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(successfulAttempt_);
                successfulAttempt_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000040;
              break;
            }
            case 74: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                diagnostics_ = new org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000100;
              }
              diagnostics_.add(bs);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          runningAttempts_ = java.util.Collections.unmodifiableList(runningAttempts_);
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          diagnostics_ = diagnostics_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskReportProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskReportProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.Builder.class);
    }

    private int bitField0_;
    public static final int TASK_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto taskId_;
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    public boolean hasTaskId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getTaskId() {
      return taskId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder getTaskIdOrBuilder() {
      return taskId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
    }

    public static final int TASK_STATE_FIELD_NUMBER = 2;
    private int taskState_;
    /**
     * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
     */
    public boolean hasTaskState() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto getTaskState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto.valueOf(taskState_);
      return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto.TS_NEW : result;
    }

    public static final int PROGRESS_FIELD_NUMBER = 3;
    private float progress_;
    /**
     * <code>optional float progress = 3;</code>
     */
    public boolean hasProgress() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional float progress = 3;</code>
     */
    public float getProgress() {
      return progress_;
    }

    public static final int START_TIME_FIELD_NUMBER = 4;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    public static final int FINISH_TIME_FIELD_NUMBER = 5;
    private long finishTime_;
    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    public boolean hasFinishTime() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    public long getFinishTime() {
      return finishTime_;
    }

    public static final int COUNTERS_FIELD_NUMBER = 6;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto counters_;
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    public boolean hasCounters() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getCounters() {
      return counters_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
    }
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder getCountersOrBuilder() {
      return counters_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
    }

    public static final int RUNNING_ATTEMPTS_FIELD_NUMBER = 7;
    private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto> runningAttempts_;
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto> getRunningAttemptsList() {
      return runningAttempts_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
        getRunningAttemptsOrBuilderList() {
      return runningAttempts_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    public int getRunningAttemptsCount() {
      return runningAttempts_.size();
    }
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getRunningAttempts(int index) {
      return runningAttempts_.get(index);
    }
    /**
     * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getRunningAttemptsOrBuilder(
        int index) {
      return runningAttempts_.get(index);
    }

    public static final int SUCCESSFUL_ATTEMPT_FIELD_NUMBER = 8;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto successfulAttempt_;
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
     */
    public boolean hasSuccessfulAttempt() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getSuccessfulAttempt() {
      return successfulAttempt_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : successfulAttempt_;
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getSuccessfulAttemptOrBuilder() {
      return successfulAttempt_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : successfulAttempt_;
    }

    public static final int DIAGNOSTICS_FIELD_NUMBER = 9;
    private org.apache.hadoop.thirdparty.protobuf.LazyStringList diagnostics_;
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ProtocolStringList
        getDiagnosticsList() {
      return diagnostics_;
    }
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    public int getDiagnosticsCount() {
      return diagnostics_.size();
    }
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    public java.lang.String getDiagnostics(int index) {
      return diagnostics_.get(index);
    }
    /**
     * <code>repeated string diagnostics = 9;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes(int index) {
      return diagnostics_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getTaskId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, taskState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeFloat(3, progress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, startTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt64(5, finishTime_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeMessage(6, getCounters());
      }
      for (int i = 0; i < runningAttempts_.size(); i++) {
        output.writeMessage(7, runningAttempts_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeMessage(8, getSuccessfulAttempt());
      }
      for (int i = 0; i < diagnostics_.size(); i++) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 9, diagnostics_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getTaskId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, taskState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeFloatSize(3, progress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(4, startTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(5, finishTime_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(6, getCounters());
      }
      for (int i = 0; i < runningAttempts_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(7, runningAttempts_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(8, getSuccessfulAttempt());
      }
      {
        int dataSize = 0;
        for (int i = 0; i < diagnostics_.size(); i++) {
          dataSize += computeStringSizeNoTag(diagnostics_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getDiagnosticsList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto) obj;

      if (hasTaskId() != other.hasTaskId()) return false;
      if (hasTaskId()) {
        if (!getTaskId()
            .equals(other.getTaskId())) return false;
      }
      if (hasTaskState() != other.hasTaskState()) return false;
      if (hasTaskState()) {
        if (taskState_ != other.taskState_) return false;
      }
      if (hasProgress() != other.hasProgress()) return false;
      if (hasProgress()) {
        if (java.lang.Float.floatToIntBits(getProgress())
            != java.lang.Float.floatToIntBits(
                other.getProgress())) return false;
      }
      if (hasStartTime() != other.hasStartTime()) return false;
      if (hasStartTime()) {
        if (getStartTime()
            != other.getStartTime()) return false;
      }
      if (hasFinishTime() != other.hasFinishTime()) return false;
      if (hasFinishTime()) {
        if (getFinishTime()
            != other.getFinishTime()) return false;
      }
      if (hasCounters() != other.hasCounters()) return false;
      if (hasCounters()) {
        if (!getCounters()
            .equals(other.getCounters())) return false;
      }
      if (!getRunningAttemptsList()
          .equals(other.getRunningAttemptsList())) return false;
      if (hasSuccessfulAttempt() != other.hasSuccessfulAttempt()) return false;
      if (hasSuccessfulAttempt()) {
        if (!getSuccessfulAttempt()
            .equals(other.getSuccessfulAttempt())) return false;
      }
      if (!getDiagnosticsList()
          .equals(other.getDiagnosticsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTaskId()) {
        hash = (37 * hash) + TASK_ID_FIELD_NUMBER;
        hash = (53 * hash) + getTaskId().hashCode();
      }
      if (hasTaskState()) {
        hash = (37 * hash) + TASK_STATE_FIELD_NUMBER;
        hash = (53 * hash) + taskState_;
      }
      if (hasProgress()) {
        hash = (37 * hash) + PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getProgress());
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getStartTime());
      }
      if (hasFinishTime()) {
        hash = (37 * hash) + FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getFinishTime());
      }
      if (hasCounters()) {
        hash = (37 * hash) + COUNTERS_FIELD_NUMBER;
        hash = (53 * hash) + getCounters().hashCode();
      }
      if (getRunningAttemptsCount() > 0) {
        hash = (37 * hash) + RUNNING_ATTEMPTS_FIELD_NUMBER;
        hash = (53 * hash) + getRunningAttemptsList().hashCode();
      }
      if (hasSuccessfulAttempt()) {
        hash = (37 * hash) + SUCCESSFUL_ATTEMPT_FIELD_NUMBER;
        hash = (53 * hash) + getSuccessfulAttempt().hashCode();
      }
      if (getDiagnosticsCount() > 0) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnosticsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.TaskReportProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.TaskReportProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskReportProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskReportProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getTaskIdFieldBuilder();
          getCountersFieldBuilder();
          getRunningAttemptsFieldBuilder();
          getSuccessfulAttemptFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (taskIdBuilder_ == null) {
          taskId_ = null;
        } else {
          taskIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        taskState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        progress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000004);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        finishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        if (countersBuilder_ == null) {
          counters_ = null;
        } else {
          countersBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        if (runningAttemptsBuilder_ == null) {
          runningAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
        } else {
          runningAttemptsBuilder_.clear();
        }
        if (successfulAttemptBuilder_ == null) {
          successfulAttempt_ = null;
        } else {
          successfulAttemptBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        diagnostics_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskReportProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (taskIdBuilder_ == null) {
            result.taskId_ = taskId_;
          } else {
            result.taskId_ = taskIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.taskState_ = taskState_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.progress_ = progress_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.startTime_ = startTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.finishTime_ = finishTime_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          if (countersBuilder_ == null) {
            result.counters_ = counters_;
          } else {
            result.counters_ = countersBuilder_.build();
          }
          to_bitField0_ |= 0x00000020;
        }
        if (runningAttemptsBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0)) {
            runningAttempts_ = java.util.Collections.unmodifiableList(runningAttempts_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.runningAttempts_ = runningAttempts_;
        } else {
          result.runningAttempts_ = runningAttemptsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (successfulAttemptBuilder_ == null) {
            result.successfulAttempt_ = successfulAttempt_;
          } else {
            result.successfulAttempt_ = successfulAttemptBuilder_.build();
          }
          to_bitField0_ |= 0x00000040;
        }
        if (((bitField0_ & 0x00000100) != 0)) {
          diagnostics_ = diagnostics_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000100);
        }
        result.diagnostics_ = diagnostics_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto.getDefaultInstance()) return this;
        if (other.hasTaskId()) {
          mergeTaskId(other.getTaskId());
        }
        if (other.hasTaskState()) {
          setTaskState(other.getTaskState());
        }
        if (other.hasProgress()) {
          setProgress(other.getProgress());
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasFinishTime()) {
          setFinishTime(other.getFinishTime());
        }
        if (other.hasCounters()) {
          mergeCounters(other.getCounters());
        }
        if (runningAttemptsBuilder_ == null) {
          if (!other.runningAttempts_.isEmpty()) {
            if (runningAttempts_.isEmpty()) {
              runningAttempts_ = other.runningAttempts_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureRunningAttemptsIsMutable();
              runningAttempts_.addAll(other.runningAttempts_);
            }
            onChanged();
          }
        } else {
          if (!other.runningAttempts_.isEmpty()) {
            if (runningAttemptsBuilder_.isEmpty()) {
              runningAttemptsBuilder_.dispose();
              runningAttemptsBuilder_ = null;
              runningAttempts_ = other.runningAttempts_;
              bitField0_ = (bitField0_ & ~0x00000040);
              runningAttemptsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getRunningAttemptsFieldBuilder() : null;
            } else {
              runningAttemptsBuilder_.addAllMessages(other.runningAttempts_);
            }
          }
        }
        if (other.hasSuccessfulAttempt()) {
          mergeSuccessfulAttempt(other.getSuccessfulAttempt());
        }
        if (!other.diagnostics_.isEmpty()) {
          if (diagnostics_.isEmpty()) {
            diagnostics_ = other.diagnostics_;
            bitField0_ = (bitField0_ & ~0x00000100);
          } else {
            ensureDiagnosticsIsMutable();
            diagnostics_.addAll(other.diagnostics_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto taskId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder> taskIdBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public boolean hasTaskId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto getTaskId() {
        if (taskIdBuilder_ == null) {
          return taskId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
        } else {
          return taskIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder setTaskId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto value) {
        if (taskIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          taskId_ = value;
          onChanged();
        } else {
          taskIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder setTaskId(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder builderForValue) {
        if (taskIdBuilder_ == null) {
          taskId_ = builderForValue.build();
          onChanged();
        } else {
          taskIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder mergeTaskId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto value) {
        if (taskIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              taskId_ != null &&
              taskId_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance()) {
            taskId_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.newBuilder(taskId_).mergeFrom(value).buildPartial();
          } else {
            taskId_ = value;
          }
          onChanged();
        } else {
          taskIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public Builder clearTaskId() {
        if (taskIdBuilder_ == null) {
          taskId_ = null;
          onChanged();
        } else {
          taskIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder getTaskIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getTaskIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder getTaskIdOrBuilder() {
        if (taskIdBuilder_ != null) {
          return taskIdBuilder_.getMessageOrBuilder();
        } else {
          return taskId_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.getDefaultInstance() : taskId_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskIdProto task_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder> 
          getTaskIdFieldBuilder() {
        if (taskIdBuilder_ == null) {
          taskIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskIdProtoOrBuilder>(
                  getTaskId(),
                  getParentForChildren(),
                  isClean());
          taskId_ = null;
        }
        return taskIdBuilder_;
      }

      private int taskState_ = 1;
      /**
       * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
       */
      public boolean hasTaskState() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto getTaskState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto.valueOf(taskState_);
        return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto.TS_NEW : result;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
       */
      public Builder setTaskState(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        taskState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskStateProto task_state = 2;</code>
       */
      public Builder clearTaskState() {
        bitField0_ = (bitField0_ & ~0x00000002);
        taskState_ = 1;
        onChanged();
        return this;
      }

      private float progress_ ;
      /**
       * <code>optional float progress = 3;</code>
       */
      public boolean hasProgress() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional float progress = 3;</code>
       */
      public float getProgress() {
        return progress_;
      }
      /**
       * <code>optional float progress = 3;</code>
       */
      public Builder setProgress(float value) {
        bitField0_ |= 0x00000004;
        progress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float progress = 3;</code>
       */
      public Builder clearProgress() {
        bitField0_ = (bitField0_ & ~0x00000004);
        progress_ = 0F;
        onChanged();
        return this;
      }

      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000008;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private long finishTime_ ;
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public boolean hasFinishTime() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public long getFinishTime() {
        return finishTime_;
      }
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public Builder setFinishTime(long value) {
        bitField0_ |= 0x00000010;
        finishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public Builder clearFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000010);
        finishTime_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto counters_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder> countersBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public boolean hasCounters() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getCounters() {
        if (countersBuilder_ == null) {
          return counters_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
        } else {
          return countersBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder setCounters(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto value) {
        if (countersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          counters_ = value;
          onChanged();
        } else {
          countersBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder setCounters(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder builderForValue) {
        if (countersBuilder_ == null) {
          counters_ = builderForValue.build();
          onChanged();
        } else {
          countersBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder mergeCounters(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto value) {
        if (countersBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0) &&
              counters_ != null &&
              counters_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance()) {
            counters_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.newBuilder(counters_).mergeFrom(value).buildPartial();
          } else {
            counters_ = value;
          }
          onChanged();
        } else {
          countersBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder clearCounters() {
        if (countersBuilder_ == null) {
          counters_ = null;
          onChanged();
        } else {
          countersBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder getCountersBuilder() {
        bitField0_ |= 0x00000020;
        onChanged();
        return getCountersFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder getCountersOrBuilder() {
        if (countersBuilder_ != null) {
          return countersBuilder_.getMessageOrBuilder();
        } else {
          return counters_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder> 
          getCountersFieldBuilder() {
        if (countersBuilder_ == null) {
          countersBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder>(
                  getCounters(),
                  getParentForChildren(),
                  isClean());
          counters_ = null;
        }
        return countersBuilder_;
      }

      private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto> runningAttempts_ =
        java.util.Collections.emptyList();
      private void ensureRunningAttemptsIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          runningAttempts_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto>(runningAttempts_);
          bitField0_ |= 0x00000040;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> runningAttemptsBuilder_;

      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto> getRunningAttemptsList() {
        if (runningAttemptsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(runningAttempts_);
        } else {
          return runningAttemptsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public int getRunningAttemptsCount() {
        if (runningAttemptsBuilder_ == null) {
          return runningAttempts_.size();
        } else {
          return runningAttemptsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getRunningAttempts(int index) {
        if (runningAttemptsBuilder_ == null) {
          return runningAttempts_.get(index);
        } else {
          return runningAttemptsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder setRunningAttempts(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (runningAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRunningAttemptsIsMutable();
          runningAttempts_.set(index, value);
          onChanged();
        } else {
          runningAttemptsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder setRunningAttempts(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder builderForValue) {
        if (runningAttemptsBuilder_ == null) {
          ensureRunningAttemptsIsMutable();
          runningAttempts_.set(index, builderForValue.build());
          onChanged();
        } else {
          runningAttemptsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder addRunningAttempts(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (runningAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRunningAttemptsIsMutable();
          runningAttempts_.add(value);
          onChanged();
        } else {
          runningAttemptsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder addRunningAttempts(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (runningAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRunningAttemptsIsMutable();
          runningAttempts_.add(index, value);
          onChanged();
        } else {
          runningAttemptsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder addRunningAttempts(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder builderForValue) {
        if (runningAttemptsBuilder_ == null) {
          ensureRunningAttemptsIsMutable();
          runningAttempts_.add(builderForValue.build());
          onChanged();
        } else {
          runningAttemptsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder addRunningAttempts(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder builderForValue) {
        if (runningAttemptsBuilder_ == null) {
          ensureRunningAttemptsIsMutable();
          runningAttempts_.add(index, builderForValue.build());
          onChanged();
        } else {
          runningAttemptsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder addAllRunningAttempts(
          java.lang.Iterable<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto> values) {
        if (runningAttemptsBuilder_ == null) {
          ensureRunningAttemptsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, runningAttempts_);
          onChanged();
        } else {
          runningAttemptsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder clearRunningAttempts() {
        if (runningAttemptsBuilder_ == null) {
          runningAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
        } else {
          runningAttemptsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public Builder removeRunningAttempts(int index) {
        if (runningAttemptsBuilder_ == null) {
          ensureRunningAttemptsIsMutable();
          runningAttempts_.remove(index);
          onChanged();
        } else {
          runningAttemptsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder getRunningAttemptsBuilder(
          int index) {
        return getRunningAttemptsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getRunningAttemptsOrBuilder(
          int index) {
        if (runningAttemptsBuilder_ == null) {
          return runningAttempts_.get(index);  } else {
          return runningAttemptsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
           getRunningAttemptsOrBuilderList() {
        if (runningAttemptsBuilder_ != null) {
          return runningAttemptsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(runningAttempts_);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder addRunningAttemptsBuilder() {
        return getRunningAttemptsFieldBuilder().addBuilder(
            org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder addRunningAttemptsBuilder(
          int index) {
        return getRunningAttemptsFieldBuilder().addBuilder(
            index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.TaskAttemptIdProto running_attempts = 7;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder> 
           getRunningAttemptsBuilderList() {
        return getRunningAttemptsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
          getRunningAttemptsFieldBuilder() {
        if (runningAttemptsBuilder_ == null) {
          runningAttemptsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder>(
                  runningAttempts_,
                  ((bitField0_ & 0x00000040) != 0),
                  getParentForChildren(),
                  isClean());
          runningAttempts_ = null;
        }
        return runningAttemptsBuilder_;
      }

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto successfulAttempt_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> successfulAttemptBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public boolean hasSuccessfulAttempt() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getSuccessfulAttempt() {
        if (successfulAttemptBuilder_ == null) {
          return successfulAttempt_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : successfulAttempt_;
        } else {
          return successfulAttemptBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public Builder setSuccessfulAttempt(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (successfulAttemptBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          successfulAttempt_ = value;
          onChanged();
        } else {
          successfulAttemptBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public Builder setSuccessfulAttempt(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder builderForValue) {
        if (successfulAttemptBuilder_ == null) {
          successfulAttempt_ = builderForValue.build();
          onChanged();
        } else {
          successfulAttemptBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public Builder mergeSuccessfulAttempt(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (successfulAttemptBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              successfulAttempt_ != null &&
              successfulAttempt_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance()) {
            successfulAttempt_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.newBuilder(successfulAttempt_).mergeFrom(value).buildPartial();
          } else {
            successfulAttempt_ = value;
          }
          onChanged();
        } else {
          successfulAttemptBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public Builder clearSuccessfulAttempt() {
        if (successfulAttemptBuilder_ == null) {
          successfulAttempt_ = null;
          onChanged();
        } else {
          successfulAttemptBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder getSuccessfulAttemptBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getSuccessfulAttemptFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getSuccessfulAttemptOrBuilder() {
        if (successfulAttemptBuilder_ != null) {
          return successfulAttemptBuilder_.getMessageOrBuilder();
        } else {
          return successfulAttempt_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : successfulAttempt_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto successful_attempt = 8;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
          getSuccessfulAttemptFieldBuilder() {
        if (successfulAttemptBuilder_ == null) {
          successfulAttemptBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder>(
                  getSuccessfulAttempt(),
                  getParentForChildren(),
                  isClean());
          successfulAttempt_ = null;
        }
        return successfulAttemptBuilder_;
      }

      private org.apache.hadoop.thirdparty.protobuf.LazyStringList diagnostics_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
      private void ensureDiagnosticsIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          diagnostics_ = new org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList(diagnostics_);
          bitField0_ |= 0x00000100;
         }
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ProtocolStringList
          getDiagnosticsList() {
        return diagnostics_.getUnmodifiableView();
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public int getDiagnosticsCount() {
        return diagnostics_.size();
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public java.lang.String getDiagnostics(int index) {
        return diagnostics_.get(index);
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsBytes(int index) {
        return diagnostics_.getByteString(index);
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public Builder setDiagnostics(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureDiagnosticsIsMutable();
        diagnostics_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public Builder addDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureDiagnosticsIsMutable();
        diagnostics_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public Builder addAllDiagnostics(
          java.lang.Iterable<java.lang.String> values) {
        ensureDiagnosticsIsMutable();
        org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
            values, diagnostics_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public Builder clearDiagnostics() {
        diagnostics_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000100);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string diagnostics = 9;</code>
       */
      public Builder addDiagnosticsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureDiagnosticsIsMutable();
        diagnostics_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.TaskReportProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.TaskReportProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<TaskReportProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<TaskReportProto>() {
      @java.lang.Override
      public TaskReportProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new TaskReportProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<TaskReportProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<TaskReportProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskReportProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskAttemptReportProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.TaskAttemptReportProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
     */
    boolean hasTaskAttemptId();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getTaskAttemptId();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getTaskAttemptIdOrBuilder();

    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
     */
    boolean hasTaskAttemptState();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto getTaskAttemptState();

    /**
     * <code>optional float progress = 3;</code>
     */
    boolean hasProgress();
    /**
     * <code>optional float progress = 3;</code>
     */
    float getProgress();

    /**
     * <code>optional int64 start_time = 4;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    long getStartTime();

    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    boolean hasFinishTime();
    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    long getFinishTime();

    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    boolean hasCounters();
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getCounters();
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder getCountersOrBuilder();

    /**
     * <code>optional string diagnostic_info = 7;</code>
     */
    boolean hasDiagnosticInfo();
    /**
     * <code>optional string diagnostic_info = 7;</code>
     */
    java.lang.String getDiagnosticInfo();
    /**
     * <code>optional string diagnostic_info = 7;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticInfoBytes();

    /**
     * <code>optional string state_string = 8;</code>
     */
    boolean hasStateString();
    /**
     * <code>optional string state_string = 8;</code>
     */
    java.lang.String getStateString();
    /**
     * <code>optional string state_string = 8;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getStateStringBytes();

    /**
     * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
     */
    boolean hasPhase();
    /**
     * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto getPhase();

    /**
     * <code>optional int64 shuffle_finish_time = 10;</code>
     */
    boolean hasShuffleFinishTime();
    /**
     * <code>optional int64 shuffle_finish_time = 10;</code>
     */
    long getShuffleFinishTime();

    /**
     * <code>optional int64 sort_finish_time = 11;</code>
     */
    boolean hasSortFinishTime();
    /**
     * <code>optional int64 sort_finish_time = 11;</code>
     */
    long getSortFinishTime();

    /**
     * <code>optional string node_manager_host = 12;</code>
     */
    boolean hasNodeManagerHost();
    /**
     * <code>optional string node_manager_host = 12;</code>
     */
    java.lang.String getNodeManagerHost();
    /**
     * <code>optional string node_manager_host = 12;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodeManagerHostBytes();

    /**
     * <code>optional int32 node_manager_port = 13;</code>
     */
    boolean hasNodeManagerPort();
    /**
     * <code>optional int32 node_manager_port = 13;</code>
     */
    int getNodeManagerPort();

    /**
     * <code>optional int32 node_manager_http_port = 14;</code>
     */
    boolean hasNodeManagerHttpPort();
    /**
     * <code>optional int32 node_manager_http_port = 14;</code>
     */
    int getNodeManagerHttpPort();

    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.TaskAttemptReportProto}
   */
  public  static final class TaskAttemptReportProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.TaskAttemptReportProto)
      TaskAttemptReportProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskAttemptReportProto.newBuilder() to construct.
    private TaskAttemptReportProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskAttemptReportProto() {
      taskAttemptState_ = 1;
      diagnosticInfo_ = "";
      stateString_ = "";
      phase_ = 1;
      nodeManagerHost_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskAttemptReportProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = taskAttemptId_.toBuilder();
              }
              taskAttemptId_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(taskAttemptId_);
                taskAttemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto value = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                taskAttemptState_ = rawValue;
              }
              break;
            }
            case 29: {
              bitField0_ |= 0x00000004;
              progress_ = input.readFloat();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              startTime_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              finishTime_ = input.readInt64();
              break;
            }
            case 50: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) != 0)) {
                subBuilder = counters_.toBuilder();
              }
              counters_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(counters_);
                counters_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 58: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000040;
              diagnosticInfo_ = bs;
              break;
            }
            case 66: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000080;
              stateString_ = bs;
              break;
            }
            case 72: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto value = org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(9, rawValue);
              } else {
                bitField0_ |= 0x00000100;
                phase_ = rawValue;
              }
              break;
            }
            case 80: {
              bitField0_ |= 0x00000200;
              shuffleFinishTime_ = input.readInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000400;
              sortFinishTime_ = input.readInt64();
              break;
            }
            case 98: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000800;
              nodeManagerHost_ = bs;
              break;
            }
            case 104: {
              bitField0_ |= 0x00001000;
              nodeManagerPort_ = input.readInt32();
              break;
            }
            case 112: {
              bitField0_ |= 0x00002000;
              nodeManagerHttpPort_ = input.readInt32();
              break;
            }
            case 122: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00004000) != 0)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00004000;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptReportProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptReportProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.Builder.class);
    }

    private int bitField0_;
    public static final int TASK_ATTEMPT_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto taskAttemptId_;
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
     */
    public boolean hasTaskAttemptId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getTaskAttemptId() {
      return taskAttemptId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : taskAttemptId_;
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getTaskAttemptIdOrBuilder() {
      return taskAttemptId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : taskAttemptId_;
    }

    public static final int TASK_ATTEMPT_STATE_FIELD_NUMBER = 2;
    private int taskAttemptState_;
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
     */
    public boolean hasTaskAttemptState() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto getTaskAttemptState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto.valueOf(taskAttemptState_);
      return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto.TA_NEW : result;
    }

    public static final int PROGRESS_FIELD_NUMBER = 3;
    private float progress_;
    /**
     * <code>optional float progress = 3;</code>
     */
    public boolean hasProgress() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional float progress = 3;</code>
     */
    public float getProgress() {
      return progress_;
    }

    public static final int START_TIME_FIELD_NUMBER = 4;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    public static final int FINISH_TIME_FIELD_NUMBER = 5;
    private long finishTime_;
    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    public boolean hasFinishTime() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int64 finish_time = 5;</code>
     */
    public long getFinishTime() {
      return finishTime_;
    }

    public static final int COUNTERS_FIELD_NUMBER = 6;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto counters_;
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    public boolean hasCounters() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getCounters() {
      return counters_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
    }
    /**
     * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder getCountersOrBuilder() {
      return counters_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
    }

    public static final int DIAGNOSTIC_INFO_FIELD_NUMBER = 7;
    private volatile java.lang.Object diagnosticInfo_;
    /**
     * <code>optional string diagnostic_info = 7;</code>
     */
    public boolean hasDiagnosticInfo() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional string diagnostic_info = 7;</code>
     */
    public java.lang.String getDiagnosticInfo() {
      java.lang.Object ref = diagnosticInfo_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnosticInfo_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostic_info = 7;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticInfoBytes() {
      java.lang.Object ref = diagnosticInfo_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnosticInfo_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int STATE_STRING_FIELD_NUMBER = 8;
    private volatile java.lang.Object stateString_;
    /**
     * <code>optional string state_string = 8;</code>
     */
    public boolean hasStateString() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional string state_string = 8;</code>
     */
    public java.lang.String getStateString() {
      java.lang.Object ref = stateString_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          stateString_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string state_string = 8;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getStateStringBytes() {
      java.lang.Object ref = stateString_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        stateString_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int PHASE_FIELD_NUMBER = 9;
    private int phase_;
    /**
     * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
     */
    public boolean hasPhase() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto getPhase() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto.valueOf(phase_);
      return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto.P_STARTING : result;
    }

    public static final int SHUFFLE_FINISH_TIME_FIELD_NUMBER = 10;
    private long shuffleFinishTime_;
    /**
     * <code>optional int64 shuffle_finish_time = 10;</code>
     */
    public boolean hasShuffleFinishTime() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>optional int64 shuffle_finish_time = 10;</code>
     */
    public long getShuffleFinishTime() {
      return shuffleFinishTime_;
    }

    public static final int SORT_FINISH_TIME_FIELD_NUMBER = 11;
    private long sortFinishTime_;
    /**
     * <code>optional int64 sort_finish_time = 11;</code>
     */
    public boolean hasSortFinishTime() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional int64 sort_finish_time = 11;</code>
     */
    public long getSortFinishTime() {
      return sortFinishTime_;
    }

    public static final int NODE_MANAGER_HOST_FIELD_NUMBER = 12;
    private volatile java.lang.Object nodeManagerHost_;
    /**
     * <code>optional string node_manager_host = 12;</code>
     */
    public boolean hasNodeManagerHost() {
      return ((bitField0_ & 0x00000800) != 0);
    }
    /**
     * <code>optional string node_manager_host = 12;</code>
     */
    public java.lang.String getNodeManagerHost() {
      java.lang.Object ref = nodeManagerHost_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nodeManagerHost_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string node_manager_host = 12;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodeManagerHostBytes() {
      java.lang.Object ref = nodeManagerHost_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nodeManagerHost_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int NODE_MANAGER_PORT_FIELD_NUMBER = 13;
    private int nodeManagerPort_;
    /**
     * <code>optional int32 node_manager_port = 13;</code>
     */
    public boolean hasNodeManagerPort() {
      return ((bitField0_ & 0x00001000) != 0);
    }
    /**
     * <code>optional int32 node_manager_port = 13;</code>
     */
    public int getNodeManagerPort() {
      return nodeManagerPort_;
    }

    public static final int NODE_MANAGER_HTTP_PORT_FIELD_NUMBER = 14;
    private int nodeManagerHttpPort_;
    /**
     * <code>optional int32 node_manager_http_port = 14;</code>
     */
    public boolean hasNodeManagerHttpPort() {
      return ((bitField0_ & 0x00002000) != 0);
    }
    /**
     * <code>optional int32 node_manager_http_port = 14;</code>
     */
    public int getNodeManagerHttpPort() {
      return nodeManagerHttpPort_;
    }

    public static final int CONTAINER_ID_FIELD_NUMBER = 15;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00004000) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getTaskAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, taskAttemptState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeFloat(3, progress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, startTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt64(5, finishTime_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeMessage(6, getCounters());
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 7, diagnosticInfo_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 8, stateString_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeEnum(9, phase_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeInt64(10, shuffleFinishTime_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeInt64(11, sortFinishTime_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 12, nodeManagerHost_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        output.writeInt32(13, nodeManagerPort_);
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        output.writeInt32(14, nodeManagerHttpPort_);
      }
      if (((bitField0_ & 0x00004000) != 0)) {
        output.writeMessage(15, getContainerId());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getTaskAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, taskAttemptState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeFloatSize(3, progress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(4, startTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(5, finishTime_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(6, getCounters());
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(7, diagnosticInfo_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(8, stateString_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(9, phase_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(10, shuffleFinishTime_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(11, sortFinishTime_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(12, nodeManagerHost_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(13, nodeManagerPort_);
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(14, nodeManagerHttpPort_);
      }
      if (((bitField0_ & 0x00004000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(15, getContainerId());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto) obj;

      if (hasTaskAttemptId() != other.hasTaskAttemptId()) return false;
      if (hasTaskAttemptId()) {
        if (!getTaskAttemptId()
            .equals(other.getTaskAttemptId())) return false;
      }
      if (hasTaskAttemptState() != other.hasTaskAttemptState()) return false;
      if (hasTaskAttemptState()) {
        if (taskAttemptState_ != other.taskAttemptState_) return false;
      }
      if (hasProgress() != other.hasProgress()) return false;
      if (hasProgress()) {
        if (java.lang.Float.floatToIntBits(getProgress())
            != java.lang.Float.floatToIntBits(
                other.getProgress())) return false;
      }
      if (hasStartTime() != other.hasStartTime()) return false;
      if (hasStartTime()) {
        if (getStartTime()
            != other.getStartTime()) return false;
      }
      if (hasFinishTime() != other.hasFinishTime()) return false;
      if (hasFinishTime()) {
        if (getFinishTime()
            != other.getFinishTime()) return false;
      }
      if (hasCounters() != other.hasCounters()) return false;
      if (hasCounters()) {
        if (!getCounters()
            .equals(other.getCounters())) return false;
      }
      if (hasDiagnosticInfo() != other.hasDiagnosticInfo()) return false;
      if (hasDiagnosticInfo()) {
        if (!getDiagnosticInfo()
            .equals(other.getDiagnosticInfo())) return false;
      }
      if (hasStateString() != other.hasStateString()) return false;
      if (hasStateString()) {
        if (!getStateString()
            .equals(other.getStateString())) return false;
      }
      if (hasPhase() != other.hasPhase()) return false;
      if (hasPhase()) {
        if (phase_ != other.phase_) return false;
      }
      if (hasShuffleFinishTime() != other.hasShuffleFinishTime()) return false;
      if (hasShuffleFinishTime()) {
        if (getShuffleFinishTime()
            != other.getShuffleFinishTime()) return false;
      }
      if (hasSortFinishTime() != other.hasSortFinishTime()) return false;
      if (hasSortFinishTime()) {
        if (getSortFinishTime()
            != other.getSortFinishTime()) return false;
      }
      if (hasNodeManagerHost() != other.hasNodeManagerHost()) return false;
      if (hasNodeManagerHost()) {
        if (!getNodeManagerHost()
            .equals(other.getNodeManagerHost())) return false;
      }
      if (hasNodeManagerPort() != other.hasNodeManagerPort()) return false;
      if (hasNodeManagerPort()) {
        if (getNodeManagerPort()
            != other.getNodeManagerPort()) return false;
      }
      if (hasNodeManagerHttpPort() != other.hasNodeManagerHttpPort()) return false;
      if (hasNodeManagerHttpPort()) {
        if (getNodeManagerHttpPort()
            != other.getNodeManagerHttpPort()) return false;
      }
      if (hasContainerId() != other.hasContainerId()) return false;
      if (hasContainerId()) {
        if (!getContainerId()
            .equals(other.getContainerId())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTaskAttemptId()) {
        hash = (37 * hash) + TASK_ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getTaskAttemptId().hashCode();
      }
      if (hasTaskAttemptState()) {
        hash = (37 * hash) + TASK_ATTEMPT_STATE_FIELD_NUMBER;
        hash = (53 * hash) + taskAttemptState_;
      }
      if (hasProgress()) {
        hash = (37 * hash) + PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getProgress());
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getStartTime());
      }
      if (hasFinishTime()) {
        hash = (37 * hash) + FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getFinishTime());
      }
      if (hasCounters()) {
        hash = (37 * hash) + COUNTERS_FIELD_NUMBER;
        hash = (53 * hash) + getCounters().hashCode();
      }
      if (hasDiagnosticInfo()) {
        hash = (37 * hash) + DIAGNOSTIC_INFO_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnosticInfo().hashCode();
      }
      if (hasStateString()) {
        hash = (37 * hash) + STATE_STRING_FIELD_NUMBER;
        hash = (53 * hash) + getStateString().hashCode();
      }
      if (hasPhase()) {
        hash = (37 * hash) + PHASE_FIELD_NUMBER;
        hash = (53 * hash) + phase_;
      }
      if (hasShuffleFinishTime()) {
        hash = (37 * hash) + SHUFFLE_FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getShuffleFinishTime());
      }
      if (hasSortFinishTime()) {
        hash = (37 * hash) + SORT_FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getSortFinishTime());
      }
      if (hasNodeManagerHost()) {
        hash = (37 * hash) + NODE_MANAGER_HOST_FIELD_NUMBER;
        hash = (53 * hash) + getNodeManagerHost().hashCode();
      }
      if (hasNodeManagerPort()) {
        hash = (37 * hash) + NODE_MANAGER_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getNodeManagerPort();
      }
      if (hasNodeManagerHttpPort()) {
        hash = (37 * hash) + NODE_MANAGER_HTTP_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getNodeManagerHttpPort();
      }
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.TaskAttemptReportProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.TaskAttemptReportProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptReportProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptReportProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getTaskAttemptIdFieldBuilder();
          getCountersFieldBuilder();
          getContainerIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (taskAttemptIdBuilder_ == null) {
          taskAttemptId_ = null;
        } else {
          taskAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        taskAttemptState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        progress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000004);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        finishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        if (countersBuilder_ == null) {
          counters_ = null;
        } else {
          countersBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        diagnosticInfo_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        stateString_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        phase_ = 1;
        bitField0_ = (bitField0_ & ~0x00000100);
        shuffleFinishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        sortFinishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000400);
        nodeManagerHost_ = "";
        bitField0_ = (bitField0_ & ~0x00000800);
        nodeManagerPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00001000);
        nodeManagerHttpPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00002000);
        if (containerIdBuilder_ == null) {
          containerId_ = null;
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptReportProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (taskAttemptIdBuilder_ == null) {
            result.taskAttemptId_ = taskAttemptId_;
          } else {
            result.taskAttemptId_ = taskAttemptIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.taskAttemptState_ = taskAttemptState_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.progress_ = progress_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.startTime_ = startTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.finishTime_ = finishTime_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          if (countersBuilder_ == null) {
            result.counters_ = counters_;
          } else {
            result.counters_ = countersBuilder_.build();
          }
          to_bitField0_ |= 0x00000020;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          to_bitField0_ |= 0x00000040;
        }
        result.diagnosticInfo_ = diagnosticInfo_;
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000080;
        }
        result.stateString_ = stateString_;
        if (((from_bitField0_ & 0x00000100) != 0)) {
          to_bitField0_ |= 0x00000100;
        }
        result.phase_ = phase_;
        if (((from_bitField0_ & 0x00000200) != 0)) {
          result.shuffleFinishTime_ = shuffleFinishTime_;
          to_bitField0_ |= 0x00000200;
        }
        if (((from_bitField0_ & 0x00000400) != 0)) {
          result.sortFinishTime_ = sortFinishTime_;
          to_bitField0_ |= 0x00000400;
        }
        if (((from_bitField0_ & 0x00000800) != 0)) {
          to_bitField0_ |= 0x00000800;
        }
        result.nodeManagerHost_ = nodeManagerHost_;
        if (((from_bitField0_ & 0x00001000) != 0)) {
          result.nodeManagerPort_ = nodeManagerPort_;
          to_bitField0_ |= 0x00001000;
        }
        if (((from_bitField0_ & 0x00002000) != 0)) {
          result.nodeManagerHttpPort_ = nodeManagerHttpPort_;
          to_bitField0_ |= 0x00002000;
        }
        if (((from_bitField0_ & 0x00004000) != 0)) {
          if (containerIdBuilder_ == null) {
            result.containerId_ = containerId_;
          } else {
            result.containerId_ = containerIdBuilder_.build();
          }
          to_bitField0_ |= 0x00004000;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto.getDefaultInstance()) return this;
        if (other.hasTaskAttemptId()) {
          mergeTaskAttemptId(other.getTaskAttemptId());
        }
        if (other.hasTaskAttemptState()) {
          setTaskAttemptState(other.getTaskAttemptState());
        }
        if (other.hasProgress()) {
          setProgress(other.getProgress());
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasFinishTime()) {
          setFinishTime(other.getFinishTime());
        }
        if (other.hasCounters()) {
          mergeCounters(other.getCounters());
        }
        if (other.hasDiagnosticInfo()) {
          bitField0_ |= 0x00000040;
          diagnosticInfo_ = other.diagnosticInfo_;
          onChanged();
        }
        if (other.hasStateString()) {
          bitField0_ |= 0x00000080;
          stateString_ = other.stateString_;
          onChanged();
        }
        if (other.hasPhase()) {
          setPhase(other.getPhase());
        }
        if (other.hasShuffleFinishTime()) {
          setShuffleFinishTime(other.getShuffleFinishTime());
        }
        if (other.hasSortFinishTime()) {
          setSortFinishTime(other.getSortFinishTime());
        }
        if (other.hasNodeManagerHost()) {
          bitField0_ |= 0x00000800;
          nodeManagerHost_ = other.nodeManagerHost_;
          onChanged();
        }
        if (other.hasNodeManagerPort()) {
          setNodeManagerPort(other.getNodeManagerPort());
        }
        if (other.hasNodeManagerHttpPort()) {
          setNodeManagerHttpPort(other.getNodeManagerHttpPort());
        }
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto taskAttemptId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> taskAttemptIdBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public boolean hasTaskAttemptId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getTaskAttemptId() {
        if (taskAttemptIdBuilder_ == null) {
          return taskAttemptId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : taskAttemptId_;
        } else {
          return taskAttemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public Builder setTaskAttemptId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (taskAttemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          taskAttemptId_ = value;
          onChanged();
        } else {
          taskAttemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public Builder setTaskAttemptId(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder builderForValue) {
        if (taskAttemptIdBuilder_ == null) {
          taskAttemptId_ = builderForValue.build();
          onChanged();
        } else {
          taskAttemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public Builder mergeTaskAttemptId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (taskAttemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              taskAttemptId_ != null &&
              taskAttemptId_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance()) {
            taskAttemptId_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.newBuilder(taskAttemptId_).mergeFrom(value).buildPartial();
          } else {
            taskAttemptId_ = value;
          }
          onChanged();
        } else {
          taskAttemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public Builder clearTaskAttemptId() {
        if (taskAttemptIdBuilder_ == null) {
          taskAttemptId_ = null;
          onChanged();
        } else {
          taskAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder getTaskAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getTaskAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getTaskAttemptIdOrBuilder() {
        if (taskAttemptIdBuilder_ != null) {
          return taskAttemptIdBuilder_.getMessageOrBuilder();
        } else {
          return taskAttemptId_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : taskAttemptId_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto task_attempt_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
          getTaskAttemptIdFieldBuilder() {
        if (taskAttemptIdBuilder_ == null) {
          taskAttemptIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder>(
                  getTaskAttemptId(),
                  getParentForChildren(),
                  isClean());
          taskAttemptId_ = null;
        }
        return taskAttemptIdBuilder_;
      }

      private int taskAttemptState_ = 1;
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
       */
      public boolean hasTaskAttemptState() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto getTaskAttemptState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto.valueOf(taskAttemptState_);
        return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto.TA_NEW : result;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
       */
      public Builder setTaskAttemptState(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        taskAttemptState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptStateProto task_attempt_state = 2;</code>
       */
      public Builder clearTaskAttemptState() {
        bitField0_ = (bitField0_ & ~0x00000002);
        taskAttemptState_ = 1;
        onChanged();
        return this;
      }

      private float progress_ ;
      /**
       * <code>optional float progress = 3;</code>
       */
      public boolean hasProgress() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional float progress = 3;</code>
       */
      public float getProgress() {
        return progress_;
      }
      /**
       * <code>optional float progress = 3;</code>
       */
      public Builder setProgress(float value) {
        bitField0_ |= 0x00000004;
        progress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float progress = 3;</code>
       */
      public Builder clearProgress() {
        bitField0_ = (bitField0_ & ~0x00000004);
        progress_ = 0F;
        onChanged();
        return this;
      }

      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000008;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private long finishTime_ ;
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public boolean hasFinishTime() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public long getFinishTime() {
        return finishTime_;
      }
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public Builder setFinishTime(long value) {
        bitField0_ |= 0x00000010;
        finishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_time = 5;</code>
       */
      public Builder clearFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000010);
        finishTime_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto counters_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder> countersBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public boolean hasCounters() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto getCounters() {
        if (countersBuilder_ == null) {
          return counters_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
        } else {
          return countersBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder setCounters(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto value) {
        if (countersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          counters_ = value;
          onChanged();
        } else {
          countersBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder setCounters(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder builderForValue) {
        if (countersBuilder_ == null) {
          counters_ = builderForValue.build();
          onChanged();
        } else {
          countersBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder mergeCounters(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto value) {
        if (countersBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0) &&
              counters_ != null &&
              counters_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance()) {
            counters_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.newBuilder(counters_).mergeFrom(value).buildPartial();
          } else {
            counters_ = value;
          }
          onChanged();
        } else {
          countersBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public Builder clearCounters() {
        if (countersBuilder_ == null) {
          counters_ = null;
          onChanged();
        } else {
          countersBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder getCountersBuilder() {
        bitField0_ |= 0x00000020;
        onChanged();
        return getCountersFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder getCountersOrBuilder() {
        if (countersBuilder_ != null) {
          return countersBuilder_.getMessageOrBuilder();
        } else {
          return counters_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.getDefaultInstance() : counters_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CountersProto counters = 6;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder> 
          getCountersFieldBuilder() {
        if (countersBuilder_ == null) {
          countersBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CountersProtoOrBuilder>(
                  getCounters(),
                  getParentForChildren(),
                  isClean());
          counters_ = null;
        }
        return countersBuilder_;
      }

      private java.lang.Object diagnosticInfo_ = "";
      /**
       * <code>optional string diagnostic_info = 7;</code>
       */
      public boolean hasDiagnosticInfo() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional string diagnostic_info = 7;</code>
       */
      public java.lang.String getDiagnosticInfo() {
        java.lang.Object ref = diagnosticInfo_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnosticInfo_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostic_info = 7;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticInfoBytes() {
        java.lang.Object ref = diagnosticInfo_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnosticInfo_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostic_info = 7;</code>
       */
      public Builder setDiagnosticInfo(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        diagnosticInfo_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostic_info = 7;</code>
       */
      public Builder clearDiagnosticInfo() {
        bitField0_ = (bitField0_ & ~0x00000040);
        diagnosticInfo_ = getDefaultInstance().getDiagnosticInfo();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostic_info = 7;</code>
       */
      public Builder setDiagnosticInfoBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        diagnosticInfo_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object stateString_ = "";
      /**
       * <code>optional string state_string = 8;</code>
       */
      public boolean hasStateString() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional string state_string = 8;</code>
       */
      public java.lang.String getStateString() {
        java.lang.Object ref = stateString_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            stateString_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string state_string = 8;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getStateStringBytes() {
        java.lang.Object ref = stateString_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          stateString_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string state_string = 8;</code>
       */
      public Builder setStateString(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        stateString_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string state_string = 8;</code>
       */
      public Builder clearStateString() {
        bitField0_ = (bitField0_ & ~0x00000080);
        stateString_ = getDefaultInstance().getStateString();
        onChanged();
        return this;
      }
      /**
       * <code>optional string state_string = 8;</code>
       */
      public Builder setStateStringBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        stateString_ = value;
        onChanged();
        return this;
      }

      private int phase_ = 1;
      /**
       * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
       */
      public boolean hasPhase() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto getPhase() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto.valueOf(phase_);
        return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto.P_STARTING : result;
      }
      /**
       * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
       */
      public Builder setPhase(org.apache.hadoop.mapreduce.v2.proto.MRProtos.PhaseProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000100;
        phase_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.PhaseProto phase = 9;</code>
       */
      public Builder clearPhase() {
        bitField0_ = (bitField0_ & ~0x00000100);
        phase_ = 1;
        onChanged();
        return this;
      }

      private long shuffleFinishTime_ ;
      /**
       * <code>optional int64 shuffle_finish_time = 10;</code>
       */
      public boolean hasShuffleFinishTime() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <code>optional int64 shuffle_finish_time = 10;</code>
       */
      public long getShuffleFinishTime() {
        return shuffleFinishTime_;
      }
      /**
       * <code>optional int64 shuffle_finish_time = 10;</code>
       */
      public Builder setShuffleFinishTime(long value) {
        bitField0_ |= 0x00000200;
        shuffleFinishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 shuffle_finish_time = 10;</code>
       */
      public Builder clearShuffleFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000200);
        shuffleFinishTime_ = 0L;
        onChanged();
        return this;
      }

      private long sortFinishTime_ ;
      /**
       * <code>optional int64 sort_finish_time = 11;</code>
       */
      public boolean hasSortFinishTime() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional int64 sort_finish_time = 11;</code>
       */
      public long getSortFinishTime() {
        return sortFinishTime_;
      }
      /**
       * <code>optional int64 sort_finish_time = 11;</code>
       */
      public Builder setSortFinishTime(long value) {
        bitField0_ |= 0x00000400;
        sortFinishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 sort_finish_time = 11;</code>
       */
      public Builder clearSortFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000400);
        sortFinishTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object nodeManagerHost_ = "";
      /**
       * <code>optional string node_manager_host = 12;</code>
       */
      public boolean hasNodeManagerHost() {
        return ((bitField0_ & 0x00000800) != 0);
      }
      /**
       * <code>optional string node_manager_host = 12;</code>
       */
      public java.lang.String getNodeManagerHost() {
        java.lang.Object ref = nodeManagerHost_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            nodeManagerHost_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string node_manager_host = 12;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNodeManagerHostBytes() {
        java.lang.Object ref = nodeManagerHost_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nodeManagerHost_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string node_manager_host = 12;</code>
       */
      public Builder setNodeManagerHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        nodeManagerHost_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string node_manager_host = 12;</code>
       */
      public Builder clearNodeManagerHost() {
        bitField0_ = (bitField0_ & ~0x00000800);
        nodeManagerHost_ = getDefaultInstance().getNodeManagerHost();
        onChanged();
        return this;
      }
      /**
       * <code>optional string node_manager_host = 12;</code>
       */
      public Builder setNodeManagerHostBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        nodeManagerHost_ = value;
        onChanged();
        return this;
      }

      private int nodeManagerPort_ ;
      /**
       * <code>optional int32 node_manager_port = 13;</code>
       */
      public boolean hasNodeManagerPort() {
        return ((bitField0_ & 0x00001000) != 0);
      }
      /**
       * <code>optional int32 node_manager_port = 13;</code>
       */
      public int getNodeManagerPort() {
        return nodeManagerPort_;
      }
      /**
       * <code>optional int32 node_manager_port = 13;</code>
       */
      public Builder setNodeManagerPort(int value) {
        bitField0_ |= 0x00001000;
        nodeManagerPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 node_manager_port = 13;</code>
       */
      public Builder clearNodeManagerPort() {
        bitField0_ = (bitField0_ & ~0x00001000);
        nodeManagerPort_ = 0;
        onChanged();
        return this;
      }

      private int nodeManagerHttpPort_ ;
      /**
       * <code>optional int32 node_manager_http_port = 14;</code>
       */
      public boolean hasNodeManagerHttpPort() {
        return ((bitField0_ & 0x00002000) != 0);
      }
      /**
       * <code>optional int32 node_manager_http_port = 14;</code>
       */
      public int getNodeManagerHttpPort() {
        return nodeManagerHttpPort_;
      }
      /**
       * <code>optional int32 node_manager_http_port = 14;</code>
       */
      public Builder setNodeManagerHttpPort(int value) {
        bitField0_ |= 0x00002000;
        nodeManagerHttpPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 node_manager_http_port = 14;</code>
       */
      public Builder clearNodeManagerHttpPort() {
        bitField0_ = (bitField0_ & ~0x00002000);
        nodeManagerHttpPort_ = 0;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00004000) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00004000) != 0) &&
              containerId_ != null &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = null;
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00004000;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 15;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  getContainerId(),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.TaskAttemptReportProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.TaskAttemptReportProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptReportProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<TaskAttemptReportProto>() {
      @java.lang.Override
      public TaskAttemptReportProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new TaskAttemptReportProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptReportProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptReportProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptReportProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface JobReportProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.JobReportProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    boolean hasJobId();
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getJobId();
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder getJobIdOrBuilder();

    /**
     * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
     */
    boolean hasJobState();
    /**
     * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto getJobState();

    /**
     * <code>optional float map_progress = 3;</code>
     */
    boolean hasMapProgress();
    /**
     * <code>optional float map_progress = 3;</code>
     */
    float getMapProgress();

    /**
     * <code>optional float reduce_progress = 4;</code>
     */
    boolean hasReduceProgress();
    /**
     * <code>optional float reduce_progress = 4;</code>
     */
    float getReduceProgress();

    /**
     * <code>optional float cleanup_progress = 5;</code>
     */
    boolean hasCleanupProgress();
    /**
     * <code>optional float cleanup_progress = 5;</code>
     */
    float getCleanupProgress();

    /**
     * <code>optional float setup_progress = 6;</code>
     */
    boolean hasSetupProgress();
    /**
     * <code>optional float setup_progress = 6;</code>
     */
    float getSetupProgress();

    /**
     * <code>optional int64 start_time = 7;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 7;</code>
     */
    long getStartTime();

    /**
     * <code>optional int64 finish_time = 8;</code>
     */
    boolean hasFinishTime();
    /**
     * <code>optional int64 finish_time = 8;</code>
     */
    long getFinishTime();

    /**
     * <code>optional string user = 9;</code>
     */
    boolean hasUser();
    /**
     * <code>optional string user = 9;</code>
     */
    java.lang.String getUser();
    /**
     * <code>optional string user = 9;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getUserBytes();

    /**
     * <code>optional string jobName = 10;</code>
     */
    boolean hasJobName();
    /**
     * <code>optional string jobName = 10;</code>
     */
    java.lang.String getJobName();
    /**
     * <code>optional string jobName = 10;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getJobNameBytes();

    /**
     * <code>optional string trackingUrl = 11;</code>
     */
    boolean hasTrackingUrl();
    /**
     * <code>optional string trackingUrl = 11;</code>
     */
    java.lang.String getTrackingUrl();
    /**
     * <code>optional string trackingUrl = 11;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getTrackingUrlBytes();

    /**
     * <code>optional string diagnostics = 12;</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 12;</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 12;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes();

    /**
     * <code>optional string jobFile = 13;</code>
     */
    boolean hasJobFile();
    /**
     * <code>optional string jobFile = 13;</code>
     */
    java.lang.String getJobFile();
    /**
     * <code>optional string jobFile = 13;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getJobFileBytes();

    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto> 
        getAmInfosList();
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto getAmInfos(int index);
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    int getAmInfosCount();
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder> 
        getAmInfosOrBuilderList();
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder getAmInfosOrBuilder(
        int index);

    /**
     * <code>optional int64 submit_time = 15;</code>
     */
    boolean hasSubmitTime();
    /**
     * <code>optional int64 submit_time = 15;</code>
     */
    long getSubmitTime();

    /**
     * <code>optional bool is_uber = 16 [default = false];</code>
     */
    boolean hasIsUber();
    /**
     * <code>optional bool is_uber = 16 [default = false];</code>
     */
    boolean getIsUber();

    /**
     * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
     */
    boolean hasJobPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getJobPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getJobPriorityOrBuilder();

    /**
     * <code>optional string historyFile = 18;</code>
     */
    boolean hasHistoryFile();
    /**
     * <code>optional string historyFile = 18;</code>
     */
    java.lang.String getHistoryFile();
    /**
     * <code>optional string historyFile = 18;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getHistoryFileBytes();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.JobReportProto}
   */
  public  static final class JobReportProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.JobReportProto)
      JobReportProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use JobReportProto.newBuilder() to construct.
    private JobReportProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private JobReportProto() {
      jobState_ = 1;
      user_ = "";
      jobName_ = "";
      trackingUrl_ = "";
      diagnostics_ = "";
      jobFile_ = "";
      amInfos_ = java.util.Collections.emptyList();
      historyFile_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private JobReportProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = jobId_.toBuilder();
              }
              jobId_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobId_);
                jobId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto value = org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                jobState_ = rawValue;
              }
              break;
            }
            case 29: {
              bitField0_ |= 0x00000004;
              mapProgress_ = input.readFloat();
              break;
            }
            case 37: {
              bitField0_ |= 0x00000008;
              reduceProgress_ = input.readFloat();
              break;
            }
            case 45: {
              bitField0_ |= 0x00000010;
              cleanupProgress_ = input.readFloat();
              break;
            }
            case 53: {
              bitField0_ |= 0x00000020;
              setupProgress_ = input.readFloat();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              startTime_ = input.readInt64();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000080;
              finishTime_ = input.readInt64();
              break;
            }
            case 74: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000100;
              user_ = bs;
              break;
            }
            case 82: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000200;
              jobName_ = bs;
              break;
            }
            case 90: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000400;
              trackingUrl_ = bs;
              break;
            }
            case 98: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000800;
              diagnostics_ = bs;
              break;
            }
            case 106: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00001000;
              jobFile_ = bs;
              break;
            }
            case 114: {
              if (!((mutable_bitField0_ & 0x00002000) != 0)) {
                amInfos_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto>();
                mutable_bitField0_ |= 0x00002000;
              }
              amInfos_.add(
                  input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.PARSER, extensionRegistry));
              break;
            }
            case 120: {
              bitField0_ |= 0x00002000;
              submitTime_ = input.readInt64();
              break;
            }
            case 128: {
              bitField0_ |= 0x00004000;
              isUber_ = input.readBool();
              break;
            }
            case 138: {
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00008000) != 0)) {
                subBuilder = jobPriority_.toBuilder();
              }
              jobPriority_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(jobPriority_);
                jobPriority_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00008000;
              break;
            }
            case 146: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00010000;
              historyFile_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00002000) != 0)) {
          amInfos_ = java.util.Collections.unmodifiableList(amInfos_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobReportProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobReportProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.Builder.class);
    }

    private int bitField0_;
    public static final int JOB_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto jobId_;
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    public boolean hasJobId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getJobId() {
      return jobId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
    }
    /**
     * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder getJobIdOrBuilder() {
      return jobId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
    }

    public static final int JOB_STATE_FIELD_NUMBER = 2;
    private int jobState_;
    /**
     * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
     */
    public boolean hasJobState() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto getJobState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto.valueOf(jobState_);
      return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto.J_NEW : result;
    }

    public static final int MAP_PROGRESS_FIELD_NUMBER = 3;
    private float mapProgress_;
    /**
     * <code>optional float map_progress = 3;</code>
     */
    public boolean hasMapProgress() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional float map_progress = 3;</code>
     */
    public float getMapProgress() {
      return mapProgress_;
    }

    public static final int REDUCE_PROGRESS_FIELD_NUMBER = 4;
    private float reduceProgress_;
    /**
     * <code>optional float reduce_progress = 4;</code>
     */
    public boolean hasReduceProgress() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional float reduce_progress = 4;</code>
     */
    public float getReduceProgress() {
      return reduceProgress_;
    }

    public static final int CLEANUP_PROGRESS_FIELD_NUMBER = 5;
    private float cleanupProgress_;
    /**
     * <code>optional float cleanup_progress = 5;</code>
     */
    public boolean hasCleanupProgress() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional float cleanup_progress = 5;</code>
     */
    public float getCleanupProgress() {
      return cleanupProgress_;
    }

    public static final int SETUP_PROGRESS_FIELD_NUMBER = 6;
    private float setupProgress_;
    /**
     * <code>optional float setup_progress = 6;</code>
     */
    public boolean hasSetupProgress() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional float setup_progress = 6;</code>
     */
    public float getSetupProgress() {
      return setupProgress_;
    }

    public static final int START_TIME_FIELD_NUMBER = 7;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 7;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional int64 start_time = 7;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    public static final int FINISH_TIME_FIELD_NUMBER = 8;
    private long finishTime_;
    /**
     * <code>optional int64 finish_time = 8;</code>
     */
    public boolean hasFinishTime() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional int64 finish_time = 8;</code>
     */
    public long getFinishTime() {
      return finishTime_;
    }

    public static final int USER_FIELD_NUMBER = 9;
    private volatile java.lang.Object user_;
    /**
     * <code>optional string user = 9;</code>
     */
    public boolean hasUser() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional string user = 9;</code>
     */
    public java.lang.String getUser() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          user_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string user = 9;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getUserBytes() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        user_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int JOBNAME_FIELD_NUMBER = 10;
    private volatile java.lang.Object jobName_;
    /**
     * <code>optional string jobName = 10;</code>
     */
    public boolean hasJobName() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>optional string jobName = 10;</code>
     */
    public java.lang.String getJobName() {
      java.lang.Object ref = jobName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          jobName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string jobName = 10;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getJobNameBytes() {
      java.lang.Object ref = jobName_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        jobName_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int TRACKINGURL_FIELD_NUMBER = 11;
    private volatile java.lang.Object trackingUrl_;
    /**
     * <code>optional string trackingUrl = 11;</code>
     */
    public boolean hasTrackingUrl() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional string trackingUrl = 11;</code>
     */
    public java.lang.String getTrackingUrl() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          trackingUrl_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string trackingUrl = 11;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getTrackingUrlBytes() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        trackingUrl_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int DIAGNOSTICS_FIELD_NUMBER = 12;
    private volatile java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 12;</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000800) != 0);
    }
    /**
     * <code>optional string diagnostics = 12;</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 12;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int JOBFILE_FIELD_NUMBER = 13;
    private volatile java.lang.Object jobFile_;
    /**
     * <code>optional string jobFile = 13;</code>
     */
    public boolean hasJobFile() {
      return ((bitField0_ & 0x00001000) != 0);
    }
    /**
     * <code>optional string jobFile = 13;</code>
     */
    public java.lang.String getJobFile() {
      java.lang.Object ref = jobFile_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          jobFile_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string jobFile = 13;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getJobFileBytes() {
      java.lang.Object ref = jobFile_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        jobFile_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int AM_INFOS_FIELD_NUMBER = 14;
    private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto> amInfos_;
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto> getAmInfosList() {
      return amInfos_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder> 
        getAmInfosOrBuilderList() {
      return amInfos_;
    }
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    public int getAmInfosCount() {
      return amInfos_.size();
    }
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto getAmInfos(int index) {
      return amInfos_.get(index);
    }
    /**
     * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder getAmInfosOrBuilder(
        int index) {
      return amInfos_.get(index);
    }

    public static final int SUBMIT_TIME_FIELD_NUMBER = 15;
    private long submitTime_;
    /**
     * <code>optional int64 submit_time = 15;</code>
     */
    public boolean hasSubmitTime() {
      return ((bitField0_ & 0x00002000) != 0);
    }
    /**
     * <code>optional int64 submit_time = 15;</code>
     */
    public long getSubmitTime() {
      return submitTime_;
    }

    public static final int IS_UBER_FIELD_NUMBER = 16;
    private boolean isUber_;
    /**
     * <code>optional bool is_uber = 16 [default = false];</code>
     */
    public boolean hasIsUber() {
      return ((bitField0_ & 0x00004000) != 0);
    }
    /**
     * <code>optional bool is_uber = 16 [default = false];</code>
     */
    public boolean getIsUber() {
      return isUber_;
    }

    public static final int JOBPRIORITY_FIELD_NUMBER = 17;
    private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto jobPriority_;
    /**
     * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
     */
    public boolean hasJobPriority() {
      return ((bitField0_ & 0x00008000) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getJobPriority() {
      return jobPriority_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : jobPriority_;
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getJobPriorityOrBuilder() {
      return jobPriority_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : jobPriority_;
    }

    public static final int HISTORYFILE_FIELD_NUMBER = 18;
    private volatile java.lang.Object historyFile_;
    /**
     * <code>optional string historyFile = 18;</code>
     */
    public boolean hasHistoryFile() {
      return ((bitField0_ & 0x00010000) != 0);
    }
    /**
     * <code>optional string historyFile = 18;</code>
     */
    public java.lang.String getHistoryFile() {
      java.lang.Object ref = historyFile_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          historyFile_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string historyFile = 18;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getHistoryFileBytes() {
      java.lang.Object ref = historyFile_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        historyFile_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getJobId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, jobState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeFloat(3, mapProgress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeFloat(4, reduceProgress_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeFloat(5, cleanupProgress_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeFloat(6, setupProgress_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeInt64(7, startTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeInt64(8, finishTime_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 9, user_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 10, jobName_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 11, trackingUrl_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 12, diagnostics_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 13, jobFile_);
      }
      for (int i = 0; i < amInfos_.size(); i++) {
        output.writeMessage(14, amInfos_.get(i));
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        output.writeInt64(15, submitTime_);
      }
      if (((bitField0_ & 0x00004000) != 0)) {
        output.writeBool(16, isUber_);
      }
      if (((bitField0_ & 0x00008000) != 0)) {
        output.writeMessage(17, getJobPriority());
      }
      if (((bitField0_ & 0x00010000) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 18, historyFile_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getJobId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, jobState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeFloatSize(3, mapProgress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeFloatSize(4, reduceProgress_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeFloatSize(5, cleanupProgress_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeFloatSize(6, setupProgress_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(7, startTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(8, finishTime_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(9, user_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(10, jobName_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(11, trackingUrl_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(12, diagnostics_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(13, jobFile_);
      }
      for (int i = 0; i < amInfos_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(14, amInfos_.get(i));
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(15, submitTime_);
      }
      if (((bitField0_ & 0x00004000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(16, isUber_);
      }
      if (((bitField0_ & 0x00008000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(17, getJobPriority());
      }
      if (((bitField0_ & 0x00010000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(18, historyFile_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto) obj;

      if (hasJobId() != other.hasJobId()) return false;
      if (hasJobId()) {
        if (!getJobId()
            .equals(other.getJobId())) return false;
      }
      if (hasJobState() != other.hasJobState()) return false;
      if (hasJobState()) {
        if (jobState_ != other.jobState_) return false;
      }
      if (hasMapProgress() != other.hasMapProgress()) return false;
      if (hasMapProgress()) {
        if (java.lang.Float.floatToIntBits(getMapProgress())
            != java.lang.Float.floatToIntBits(
                other.getMapProgress())) return false;
      }
      if (hasReduceProgress() != other.hasReduceProgress()) return false;
      if (hasReduceProgress()) {
        if (java.lang.Float.floatToIntBits(getReduceProgress())
            != java.lang.Float.floatToIntBits(
                other.getReduceProgress())) return false;
      }
      if (hasCleanupProgress() != other.hasCleanupProgress()) return false;
      if (hasCleanupProgress()) {
        if (java.lang.Float.floatToIntBits(getCleanupProgress())
            != java.lang.Float.floatToIntBits(
                other.getCleanupProgress())) return false;
      }
      if (hasSetupProgress() != other.hasSetupProgress()) return false;
      if (hasSetupProgress()) {
        if (java.lang.Float.floatToIntBits(getSetupProgress())
            != java.lang.Float.floatToIntBits(
                other.getSetupProgress())) return false;
      }
      if (hasStartTime() != other.hasStartTime()) return false;
      if (hasStartTime()) {
        if (getStartTime()
            != other.getStartTime()) return false;
      }
      if (hasFinishTime() != other.hasFinishTime()) return false;
      if (hasFinishTime()) {
        if (getFinishTime()
            != other.getFinishTime()) return false;
      }
      if (hasUser() != other.hasUser()) return false;
      if (hasUser()) {
        if (!getUser()
            .equals(other.getUser())) return false;
      }
      if (hasJobName() != other.hasJobName()) return false;
      if (hasJobName()) {
        if (!getJobName()
            .equals(other.getJobName())) return false;
      }
      if (hasTrackingUrl() != other.hasTrackingUrl()) return false;
      if (hasTrackingUrl()) {
        if (!getTrackingUrl()
            .equals(other.getTrackingUrl())) return false;
      }
      if (hasDiagnostics() != other.hasDiagnostics()) return false;
      if (hasDiagnostics()) {
        if (!getDiagnostics()
            .equals(other.getDiagnostics())) return false;
      }
      if (hasJobFile() != other.hasJobFile()) return false;
      if (hasJobFile()) {
        if (!getJobFile()
            .equals(other.getJobFile())) return false;
      }
      if (!getAmInfosList()
          .equals(other.getAmInfosList())) return false;
      if (hasSubmitTime() != other.hasSubmitTime()) return false;
      if (hasSubmitTime()) {
        if (getSubmitTime()
            != other.getSubmitTime()) return false;
      }
      if (hasIsUber() != other.hasIsUber()) return false;
      if (hasIsUber()) {
        if (getIsUber()
            != other.getIsUber()) return false;
      }
      if (hasJobPriority() != other.hasJobPriority()) return false;
      if (hasJobPriority()) {
        if (!getJobPriority()
            .equals(other.getJobPriority())) return false;
      }
      if (hasHistoryFile() != other.hasHistoryFile()) return false;
      if (hasHistoryFile()) {
        if (!getHistoryFile()
            .equals(other.getHistoryFile())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasJobId()) {
        hash = (37 * hash) + JOB_ID_FIELD_NUMBER;
        hash = (53 * hash) + getJobId().hashCode();
      }
      if (hasJobState()) {
        hash = (37 * hash) + JOB_STATE_FIELD_NUMBER;
        hash = (53 * hash) + jobState_;
      }
      if (hasMapProgress()) {
        hash = (37 * hash) + MAP_PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getMapProgress());
      }
      if (hasReduceProgress()) {
        hash = (37 * hash) + REDUCE_PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getReduceProgress());
      }
      if (hasCleanupProgress()) {
        hash = (37 * hash) + CLEANUP_PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getCleanupProgress());
      }
      if (hasSetupProgress()) {
        hash = (37 * hash) + SETUP_PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getSetupProgress());
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getStartTime());
      }
      if (hasFinishTime()) {
        hash = (37 * hash) + FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getFinishTime());
      }
      if (hasUser()) {
        hash = (37 * hash) + USER_FIELD_NUMBER;
        hash = (53 * hash) + getUser().hashCode();
      }
      if (hasJobName()) {
        hash = (37 * hash) + JOBNAME_FIELD_NUMBER;
        hash = (53 * hash) + getJobName().hashCode();
      }
      if (hasTrackingUrl()) {
        hash = (37 * hash) + TRACKINGURL_FIELD_NUMBER;
        hash = (53 * hash) + getTrackingUrl().hashCode();
      }
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      if (hasJobFile()) {
        hash = (37 * hash) + JOBFILE_FIELD_NUMBER;
        hash = (53 * hash) + getJobFile().hashCode();
      }
      if (getAmInfosCount() > 0) {
        hash = (37 * hash) + AM_INFOS_FIELD_NUMBER;
        hash = (53 * hash) + getAmInfosList().hashCode();
      }
      if (hasSubmitTime()) {
        hash = (37 * hash) + SUBMIT_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getSubmitTime());
      }
      if (hasIsUber()) {
        hash = (37 * hash) + IS_UBER_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getIsUber());
      }
      if (hasJobPriority()) {
        hash = (37 * hash) + JOBPRIORITY_FIELD_NUMBER;
        hash = (53 * hash) + getJobPriority().hashCode();
      }
      if (hasHistoryFile()) {
        hash = (37 * hash) + HISTORYFILE_FIELD_NUMBER;
        hash = (53 * hash) + getHistoryFile().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.JobReportProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.JobReportProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobReportProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobReportProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getJobIdFieldBuilder();
          getAmInfosFieldBuilder();
          getJobPriorityFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (jobIdBuilder_ == null) {
          jobId_ = null;
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        jobState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        mapProgress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000004);
        reduceProgress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000008);
        cleanupProgress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000010);
        setupProgress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000020);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        finishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        user_ = "";
        bitField0_ = (bitField0_ & ~0x00000100);
        jobName_ = "";
        bitField0_ = (bitField0_ & ~0x00000200);
        trackingUrl_ = "";
        bitField0_ = (bitField0_ & ~0x00000400);
        diagnostics_ = "";
        bitField0_ = (bitField0_ & ~0x00000800);
        jobFile_ = "";
        bitField0_ = (bitField0_ & ~0x00001000);
        if (amInfosBuilder_ == null) {
          amInfos_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00002000);
        } else {
          amInfosBuilder_.clear();
        }
        submitTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00004000);
        isUber_ = false;
        bitField0_ = (bitField0_ & ~0x00008000);
        if (jobPriorityBuilder_ == null) {
          jobPriority_ = null;
        } else {
          jobPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00010000);
        historyFile_ = "";
        bitField0_ = (bitField0_ & ~0x00020000);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_JobReportProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (jobIdBuilder_ == null) {
            result.jobId_ = jobId_;
          } else {
            result.jobId_ = jobIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.jobState_ = jobState_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.mapProgress_ = mapProgress_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.reduceProgress_ = reduceProgress_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.cleanupProgress_ = cleanupProgress_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.setupProgress_ = setupProgress_;
          to_bitField0_ |= 0x00000020;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.startTime_ = startTime_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          result.finishTime_ = finishTime_;
          to_bitField0_ |= 0x00000080;
        }
        if (((from_bitField0_ & 0x00000100) != 0)) {
          to_bitField0_ |= 0x00000100;
        }
        result.user_ = user_;
        if (((from_bitField0_ & 0x00000200) != 0)) {
          to_bitField0_ |= 0x00000200;
        }
        result.jobName_ = jobName_;
        if (((from_bitField0_ & 0x00000400) != 0)) {
          to_bitField0_ |= 0x00000400;
        }
        result.trackingUrl_ = trackingUrl_;
        if (((from_bitField0_ & 0x00000800) != 0)) {
          to_bitField0_ |= 0x00000800;
        }
        result.diagnostics_ = diagnostics_;
        if (((from_bitField0_ & 0x00001000) != 0)) {
          to_bitField0_ |= 0x00001000;
        }
        result.jobFile_ = jobFile_;
        if (amInfosBuilder_ == null) {
          if (((bitField0_ & 0x00002000) != 0)) {
            amInfos_ = java.util.Collections.unmodifiableList(amInfos_);
            bitField0_ = (bitField0_ & ~0x00002000);
          }
          result.amInfos_ = amInfos_;
        } else {
          result.amInfos_ = amInfosBuilder_.build();
        }
        if (((from_bitField0_ & 0x00004000) != 0)) {
          result.submitTime_ = submitTime_;
          to_bitField0_ |= 0x00002000;
        }
        if (((from_bitField0_ & 0x00008000) != 0)) {
          result.isUber_ = isUber_;
          to_bitField0_ |= 0x00004000;
        }
        if (((from_bitField0_ & 0x00010000) != 0)) {
          if (jobPriorityBuilder_ == null) {
            result.jobPriority_ = jobPriority_;
          } else {
            result.jobPriority_ = jobPriorityBuilder_.build();
          }
          to_bitField0_ |= 0x00008000;
        }
        if (((from_bitField0_ & 0x00020000) != 0)) {
          to_bitField0_ |= 0x00010000;
        }
        result.historyFile_ = historyFile_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto.getDefaultInstance()) return this;
        if (other.hasJobId()) {
          mergeJobId(other.getJobId());
        }
        if (other.hasJobState()) {
          setJobState(other.getJobState());
        }
        if (other.hasMapProgress()) {
          setMapProgress(other.getMapProgress());
        }
        if (other.hasReduceProgress()) {
          setReduceProgress(other.getReduceProgress());
        }
        if (other.hasCleanupProgress()) {
          setCleanupProgress(other.getCleanupProgress());
        }
        if (other.hasSetupProgress()) {
          setSetupProgress(other.getSetupProgress());
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasFinishTime()) {
          setFinishTime(other.getFinishTime());
        }
        if (other.hasUser()) {
          bitField0_ |= 0x00000100;
          user_ = other.user_;
          onChanged();
        }
        if (other.hasJobName()) {
          bitField0_ |= 0x00000200;
          jobName_ = other.jobName_;
          onChanged();
        }
        if (other.hasTrackingUrl()) {
          bitField0_ |= 0x00000400;
          trackingUrl_ = other.trackingUrl_;
          onChanged();
        }
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000800;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        if (other.hasJobFile()) {
          bitField0_ |= 0x00001000;
          jobFile_ = other.jobFile_;
          onChanged();
        }
        if (amInfosBuilder_ == null) {
          if (!other.amInfos_.isEmpty()) {
            if (amInfos_.isEmpty()) {
              amInfos_ = other.amInfos_;
              bitField0_ = (bitField0_ & ~0x00002000);
            } else {
              ensureAmInfosIsMutable();
              amInfos_.addAll(other.amInfos_);
            }
            onChanged();
          }
        } else {
          if (!other.amInfos_.isEmpty()) {
            if (amInfosBuilder_.isEmpty()) {
              amInfosBuilder_.dispose();
              amInfosBuilder_ = null;
              amInfos_ = other.amInfos_;
              bitField0_ = (bitField0_ & ~0x00002000);
              amInfosBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAmInfosFieldBuilder() : null;
            } else {
              amInfosBuilder_.addAllMessages(other.amInfos_);
            }
          }
        }
        if (other.hasSubmitTime()) {
          setSubmitTime(other.getSubmitTime());
        }
        if (other.hasIsUber()) {
          setIsUber(other.getIsUber());
        }
        if (other.hasJobPriority()) {
          mergeJobPriority(other.getJobPriority());
        }
        if (other.hasHistoryFile()) {
          bitField0_ |= 0x00020000;
          historyFile_ = other.historyFile_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto jobId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder> jobIdBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public boolean hasJobId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto getJobId() {
        if (jobIdBuilder_ == null) {
          return jobId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
        } else {
          return jobIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder setJobId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobId_ = value;
          onChanged();
        } else {
          jobIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder setJobId(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder builderForValue) {
        if (jobIdBuilder_ == null) {
          jobId_ = builderForValue.build();
          onChanged();
        } else {
          jobIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder mergeJobId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto value) {
        if (jobIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              jobId_ != null &&
              jobId_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance()) {
            jobId_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.newBuilder(jobId_).mergeFrom(value).buildPartial();
          } else {
            jobId_ = value;
          }
          onChanged();
        } else {
          jobIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public Builder clearJobId() {
        if (jobIdBuilder_ == null) {
          jobId_ = null;
          onChanged();
        } else {
          jobIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder getJobIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getJobIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder getJobIdOrBuilder() {
        if (jobIdBuilder_ != null) {
          return jobIdBuilder_.getMessageOrBuilder();
        } else {
          return jobId_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.getDefaultInstance() : jobId_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.JobIdProto job_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder> 
          getJobIdFieldBuilder() {
        if (jobIdBuilder_ == null) {
          jobIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobIdProtoOrBuilder>(
                  getJobId(),
                  getParentForChildren(),
                  isClean());
          jobId_ = null;
        }
        return jobIdBuilder_;
      }

      private int jobState_ = 1;
      /**
       * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
       */
      public boolean hasJobState() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto getJobState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto.valueOf(jobState_);
        return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto.J_NEW : result;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
       */
      public Builder setJobState(org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        jobState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.JobStateProto job_state = 2;</code>
       */
      public Builder clearJobState() {
        bitField0_ = (bitField0_ & ~0x00000002);
        jobState_ = 1;
        onChanged();
        return this;
      }

      private float mapProgress_ ;
      /**
       * <code>optional float map_progress = 3;</code>
       */
      public boolean hasMapProgress() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional float map_progress = 3;</code>
       */
      public float getMapProgress() {
        return mapProgress_;
      }
      /**
       * <code>optional float map_progress = 3;</code>
       */
      public Builder setMapProgress(float value) {
        bitField0_ |= 0x00000004;
        mapProgress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float map_progress = 3;</code>
       */
      public Builder clearMapProgress() {
        bitField0_ = (bitField0_ & ~0x00000004);
        mapProgress_ = 0F;
        onChanged();
        return this;
      }

      private float reduceProgress_ ;
      /**
       * <code>optional float reduce_progress = 4;</code>
       */
      public boolean hasReduceProgress() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional float reduce_progress = 4;</code>
       */
      public float getReduceProgress() {
        return reduceProgress_;
      }
      /**
       * <code>optional float reduce_progress = 4;</code>
       */
      public Builder setReduceProgress(float value) {
        bitField0_ |= 0x00000008;
        reduceProgress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float reduce_progress = 4;</code>
       */
      public Builder clearReduceProgress() {
        bitField0_ = (bitField0_ & ~0x00000008);
        reduceProgress_ = 0F;
        onChanged();
        return this;
      }

      private float cleanupProgress_ ;
      /**
       * <code>optional float cleanup_progress = 5;</code>
       */
      public boolean hasCleanupProgress() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional float cleanup_progress = 5;</code>
       */
      public float getCleanupProgress() {
        return cleanupProgress_;
      }
      /**
       * <code>optional float cleanup_progress = 5;</code>
       */
      public Builder setCleanupProgress(float value) {
        bitField0_ |= 0x00000010;
        cleanupProgress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float cleanup_progress = 5;</code>
       */
      public Builder clearCleanupProgress() {
        bitField0_ = (bitField0_ & ~0x00000010);
        cleanupProgress_ = 0F;
        onChanged();
        return this;
      }

      private float setupProgress_ ;
      /**
       * <code>optional float setup_progress = 6;</code>
       */
      public boolean hasSetupProgress() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional float setup_progress = 6;</code>
       */
      public float getSetupProgress() {
        return setupProgress_;
      }
      /**
       * <code>optional float setup_progress = 6;</code>
       */
      public Builder setSetupProgress(float value) {
        bitField0_ |= 0x00000020;
        setupProgress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float setup_progress = 6;</code>
       */
      public Builder clearSetupProgress() {
        bitField0_ = (bitField0_ & ~0x00000020);
        setupProgress_ = 0F;
        onChanged();
        return this;
      }

      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000040;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000040);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private long finishTime_ ;
      /**
       * <code>optional int64 finish_time = 8;</code>
       */
      public boolean hasFinishTime() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional int64 finish_time = 8;</code>
       */
      public long getFinishTime() {
        return finishTime_;
      }
      /**
       * <code>optional int64 finish_time = 8;</code>
       */
      public Builder setFinishTime(long value) {
        bitField0_ |= 0x00000080;
        finishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_time = 8;</code>
       */
      public Builder clearFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000080);
        finishTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object user_ = "";
      /**
       * <code>optional string user = 9;</code>
       */
      public boolean hasUser() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional string user = 9;</code>
       */
      public java.lang.String getUser() {
        java.lang.Object ref = user_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            user_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string user = 9;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getUserBytes() {
        java.lang.Object ref = user_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          user_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string user = 9;</code>
       */
      public Builder setUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000100;
        user_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 9;</code>
       */
      public Builder clearUser() {
        bitField0_ = (bitField0_ & ~0x00000100);
        user_ = getDefaultInstance().getUser();
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 9;</code>
       */
      public Builder setUserBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000100;
        user_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object jobName_ = "";
      /**
       * <code>optional string jobName = 10;</code>
       */
      public boolean hasJobName() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <code>optional string jobName = 10;</code>
       */
      public java.lang.String getJobName() {
        java.lang.Object ref = jobName_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            jobName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string jobName = 10;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getJobNameBytes() {
        java.lang.Object ref = jobName_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          jobName_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string jobName = 10;</code>
       */
      public Builder setJobName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000200;
        jobName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string jobName = 10;</code>
       */
      public Builder clearJobName() {
        bitField0_ = (bitField0_ & ~0x00000200);
        jobName_ = getDefaultInstance().getJobName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string jobName = 10;</code>
       */
      public Builder setJobNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000200;
        jobName_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object trackingUrl_ = "";
      /**
       * <code>optional string trackingUrl = 11;</code>
       */
      public boolean hasTrackingUrl() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional string trackingUrl = 11;</code>
       */
      public java.lang.String getTrackingUrl() {
        java.lang.Object ref = trackingUrl_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            trackingUrl_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string trackingUrl = 11;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getTrackingUrlBytes() {
        java.lang.Object ref = trackingUrl_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          trackingUrl_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string trackingUrl = 11;</code>
       */
      public Builder setTrackingUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        trackingUrl_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string trackingUrl = 11;</code>
       */
      public Builder clearTrackingUrl() {
        bitField0_ = (bitField0_ & ~0x00000400);
        trackingUrl_ = getDefaultInstance().getTrackingUrl();
        onChanged();
        return this;
      }
      /**
       * <code>optional string trackingUrl = 11;</code>
       */
      public Builder setTrackingUrlBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        trackingUrl_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object diagnostics_ = "";
      /**
       * <code>optional string diagnostics = 12;</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000800) != 0);
      }
      /**
       * <code>optional string diagnostics = 12;</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnostics_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 12;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 12;</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 12;</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000800);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 12;</code>
       */
      public Builder setDiagnosticsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000800;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object jobFile_ = "";
      /**
       * <code>optional string jobFile = 13;</code>
       */
      public boolean hasJobFile() {
        return ((bitField0_ & 0x00001000) != 0);
      }
      /**
       * <code>optional string jobFile = 13;</code>
       */
      public java.lang.String getJobFile() {
        java.lang.Object ref = jobFile_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            jobFile_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string jobFile = 13;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getJobFileBytes() {
        java.lang.Object ref = jobFile_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          jobFile_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string jobFile = 13;</code>
       */
      public Builder setJobFile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00001000;
        jobFile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string jobFile = 13;</code>
       */
      public Builder clearJobFile() {
        bitField0_ = (bitField0_ & ~0x00001000);
        jobFile_ = getDefaultInstance().getJobFile();
        onChanged();
        return this;
      }
      /**
       * <code>optional string jobFile = 13;</code>
       */
      public Builder setJobFileBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00001000;
        jobFile_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto> amInfos_ =
        java.util.Collections.emptyList();
      private void ensureAmInfosIsMutable() {
        if (!((bitField0_ & 0x00002000) != 0)) {
          amInfos_ = new java.util.ArrayList<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto>(amInfos_);
          bitField0_ |= 0x00002000;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder> amInfosBuilder_;

      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto> getAmInfosList() {
        if (amInfosBuilder_ == null) {
          return java.util.Collections.unmodifiableList(amInfos_);
        } else {
          return amInfosBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public int getAmInfosCount() {
        if (amInfosBuilder_ == null) {
          return amInfos_.size();
        } else {
          return amInfosBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto getAmInfos(int index) {
        if (amInfosBuilder_ == null) {
          return amInfos_.get(index);
        } else {
          return amInfosBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder setAmInfos(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto value) {
        if (amInfosBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAmInfosIsMutable();
          amInfos_.set(index, value);
          onChanged();
        } else {
          amInfosBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder setAmInfos(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder builderForValue) {
        if (amInfosBuilder_ == null) {
          ensureAmInfosIsMutable();
          amInfos_.set(index, builderForValue.build());
          onChanged();
        } else {
          amInfosBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder addAmInfos(org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto value) {
        if (amInfosBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAmInfosIsMutable();
          amInfos_.add(value);
          onChanged();
        } else {
          amInfosBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder addAmInfos(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto value) {
        if (amInfosBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAmInfosIsMutable();
          amInfos_.add(index, value);
          onChanged();
        } else {
          amInfosBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder addAmInfos(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder builderForValue) {
        if (amInfosBuilder_ == null) {
          ensureAmInfosIsMutable();
          amInfos_.add(builderForValue.build());
          onChanged();
        } else {
          amInfosBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder addAmInfos(
          int index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder builderForValue) {
        if (amInfosBuilder_ == null) {
          ensureAmInfosIsMutable();
          amInfos_.add(index, builderForValue.build());
          onChanged();
        } else {
          amInfosBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder addAllAmInfos(
          java.lang.Iterable<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto> values) {
        if (amInfosBuilder_ == null) {
          ensureAmInfosIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, amInfos_);
          onChanged();
        } else {
          amInfosBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder clearAmInfos() {
        if (amInfosBuilder_ == null) {
          amInfos_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00002000);
          onChanged();
        } else {
          amInfosBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public Builder removeAmInfos(int index) {
        if (amInfosBuilder_ == null) {
          ensureAmInfosIsMutable();
          amInfos_.remove(index);
          onChanged();
        } else {
          amInfosBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder getAmInfosBuilder(
          int index) {
        return getAmInfosFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder getAmInfosOrBuilder(
          int index) {
        if (amInfosBuilder_ == null) {
          return amInfos_.get(index);  } else {
          return amInfosBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public java.util.List<? extends org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder> 
           getAmInfosOrBuilderList() {
        if (amInfosBuilder_ != null) {
          return amInfosBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(amInfos_);
        }
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder addAmInfosBuilder() {
        return getAmInfosFieldBuilder().addBuilder(
            org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder addAmInfosBuilder(
          int index) {
        return getAmInfosFieldBuilder().addBuilder(
            index, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.mapreduce.AMInfoProto am_infos = 14;</code>
       */
      public java.util.List<org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder> 
           getAmInfosBuilderList() {
        return getAmInfosFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder> 
          getAmInfosFieldBuilder() {
        if (amInfosBuilder_ == null) {
          amInfosBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder>(
                  amInfos_,
                  ((bitField0_ & 0x00002000) != 0),
                  getParentForChildren(),
                  isClean());
          amInfos_ = null;
        }
        return amInfosBuilder_;
      }

      private long submitTime_ ;
      /**
       * <code>optional int64 submit_time = 15;</code>
       */
      public boolean hasSubmitTime() {
        return ((bitField0_ & 0x00004000) != 0);
      }
      /**
       * <code>optional int64 submit_time = 15;</code>
       */
      public long getSubmitTime() {
        return submitTime_;
      }
      /**
       * <code>optional int64 submit_time = 15;</code>
       */
      public Builder setSubmitTime(long value) {
        bitField0_ |= 0x00004000;
        submitTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 submit_time = 15;</code>
       */
      public Builder clearSubmitTime() {
        bitField0_ = (bitField0_ & ~0x00004000);
        submitTime_ = 0L;
        onChanged();
        return this;
      }

      private boolean isUber_ ;
      /**
       * <code>optional bool is_uber = 16 [default = false];</code>
       */
      public boolean hasIsUber() {
        return ((bitField0_ & 0x00008000) != 0);
      }
      /**
       * <code>optional bool is_uber = 16 [default = false];</code>
       */
      public boolean getIsUber() {
        return isUber_;
      }
      /**
       * <code>optional bool is_uber = 16 [default = false];</code>
       */
      public Builder setIsUber(boolean value) {
        bitField0_ |= 0x00008000;
        isUber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_uber = 16 [default = false];</code>
       */
      public Builder clearIsUber() {
        bitField0_ = (bitField0_ & ~0x00008000);
        isUber_ = false;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto jobPriority_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> jobPriorityBuilder_;
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public boolean hasJobPriority() {
        return ((bitField0_ & 0x00010000) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getJobPriority() {
        if (jobPriorityBuilder_ == null) {
          return jobPriority_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : jobPriority_;
        } else {
          return jobPriorityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public Builder setJobPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (jobPriorityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          jobPriority_ = value;
          onChanged();
        } else {
          jobPriorityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00010000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public Builder setJobPriority(
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder builderForValue) {
        if (jobPriorityBuilder_ == null) {
          jobPriority_ = builderForValue.build();
          onChanged();
        } else {
          jobPriorityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00010000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public Builder mergeJobPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (jobPriorityBuilder_ == null) {
          if (((bitField0_ & 0x00010000) != 0) &&
              jobPriority_ != null &&
              jobPriority_ != org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance()) {
            jobPriority_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.newBuilder(jobPriority_).mergeFrom(value).buildPartial();
          } else {
            jobPriority_ = value;
          }
          onChanged();
        } else {
          jobPriorityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00010000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public Builder clearJobPriority() {
        if (jobPriorityBuilder_ == null) {
          jobPriority_ = null;
          onChanged();
        } else {
          jobPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00010000);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder getJobPriorityBuilder() {
        bitField0_ |= 0x00010000;
        onChanged();
        return getJobPriorityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getJobPriorityOrBuilder() {
        if (jobPriorityBuilder_ != null) {
          return jobPriorityBuilder_.getMessageOrBuilder();
        } else {
          return jobPriority_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : jobPriority_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto jobPriority = 17;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> 
          getJobPriorityFieldBuilder() {
        if (jobPriorityBuilder_ == null) {
          jobPriorityBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder>(
                  getJobPriority(),
                  getParentForChildren(),
                  isClean());
          jobPriority_ = null;
        }
        return jobPriorityBuilder_;
      }

      private java.lang.Object historyFile_ = "";
      /**
       * <code>optional string historyFile = 18;</code>
       */
      public boolean hasHistoryFile() {
        return ((bitField0_ & 0x00020000) != 0);
      }
      /**
       * <code>optional string historyFile = 18;</code>
       */
      public java.lang.String getHistoryFile() {
        java.lang.Object ref = historyFile_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            historyFile_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string historyFile = 18;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getHistoryFileBytes() {
        java.lang.Object ref = historyFile_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          historyFile_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string historyFile = 18;</code>
       */
      public Builder setHistoryFile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00020000;
        historyFile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string historyFile = 18;</code>
       */
      public Builder clearHistoryFile() {
        bitField0_ = (bitField0_ & ~0x00020000);
        historyFile_ = getDefaultInstance().getHistoryFile();
        onChanged();
        return this;
      }
      /**
       * <code>optional string historyFile = 18;</code>
       */
      public Builder setHistoryFileBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00020000;
        historyFile_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.JobReportProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.JobReportProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<JobReportProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<JobReportProto>() {
      @java.lang.Override
      public JobReportProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new JobReportProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<JobReportProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<JobReportProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.JobReportProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface AMInfoProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.AMInfoProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    boolean hasApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder();

    /**
     * <code>optional int64 start_time = 2;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 2;</code>
     */
    long getStartTime();

    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();

    /**
     * <code>optional string node_manager_host = 4;</code>
     */
    boolean hasNodeManagerHost();
    /**
     * <code>optional string node_manager_host = 4;</code>
     */
    java.lang.String getNodeManagerHost();
    /**
     * <code>optional string node_manager_host = 4;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodeManagerHostBytes();

    /**
     * <code>optional int32 node_manager_port = 5;</code>
     */
    boolean hasNodeManagerPort();
    /**
     * <code>optional int32 node_manager_port = 5;</code>
     */
    int getNodeManagerPort();

    /**
     * <code>optional int32 node_manager_http_port = 6;</code>
     */
    boolean hasNodeManagerHttpPort();
    /**
     * <code>optional int32 node_manager_http_port = 6;</code>
     */
    int getNodeManagerHttpPort();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.AMInfoProto}
   */
  public  static final class AMInfoProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.AMInfoProto)
      AMInfoProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use AMInfoProto.newBuilder() to construct.
    private AMInfoProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private AMInfoProto() {
      nodeManagerHost_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private AMInfoProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = applicationAttemptId_.toBuilder();
              }
              applicationAttemptId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationAttemptId_);
                applicationAttemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              startTime_ = input.readInt64();
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000008;
              nodeManagerHost_ = bs;
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              nodeManagerPort_ = input.readInt32();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              nodeManagerHttpPort_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_AMInfoProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_AMInfoProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder.class);
    }

    private int bitField0_;
    public static final int APPLICATION_ATTEMPT_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public boolean hasApplicationAttemptId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
      return applicationAttemptId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : applicationAttemptId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
      return applicationAttemptId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : applicationAttemptId_;
    }

    public static final int START_TIME_FIELD_NUMBER = 2;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 2;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int64 start_time = 2;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    public static final int CONTAINER_ID_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
    }

    public static final int NODE_MANAGER_HOST_FIELD_NUMBER = 4;
    private volatile java.lang.Object nodeManagerHost_;
    /**
     * <code>optional string node_manager_host = 4;</code>
     */
    public boolean hasNodeManagerHost() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional string node_manager_host = 4;</code>
     */
    public java.lang.String getNodeManagerHost() {
      java.lang.Object ref = nodeManagerHost_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nodeManagerHost_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string node_manager_host = 4;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodeManagerHostBytes() {
      java.lang.Object ref = nodeManagerHost_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nodeManagerHost_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int NODE_MANAGER_PORT_FIELD_NUMBER = 5;
    private int nodeManagerPort_;
    /**
     * <code>optional int32 node_manager_port = 5;</code>
     */
    public boolean hasNodeManagerPort() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int32 node_manager_port = 5;</code>
     */
    public int getNodeManagerPort() {
      return nodeManagerPort_;
    }

    public static final int NODE_MANAGER_HTTP_PORT_FIELD_NUMBER = 6;
    private int nodeManagerHttpPort_;
    /**
     * <code>optional int32 node_manager_http_port = 6;</code>
     */
    public boolean hasNodeManagerHttpPort() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional int32 node_manager_http_port = 6;</code>
     */
    public int getNodeManagerHttpPort() {
      return nodeManagerHttpPort_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getApplicationAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(2, startTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getContainerId());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 4, nodeManagerHost_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt32(5, nodeManagerPort_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeInt32(6, nodeManagerHttpPort_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getApplicationAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(2, startTime_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(3, getContainerId());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(4, nodeManagerHost_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(5, nodeManagerPort_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(6, nodeManagerHttpPort_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto) obj;

      if (hasApplicationAttemptId() != other.hasApplicationAttemptId()) return false;
      if (hasApplicationAttemptId()) {
        if (!getApplicationAttemptId()
            .equals(other.getApplicationAttemptId())) return false;
      }
      if (hasStartTime() != other.hasStartTime()) return false;
      if (hasStartTime()) {
        if (getStartTime()
            != other.getStartTime()) return false;
      }
      if (hasContainerId() != other.hasContainerId()) return false;
      if (hasContainerId()) {
        if (!getContainerId()
            .equals(other.getContainerId())) return false;
      }
      if (hasNodeManagerHost() != other.hasNodeManagerHost()) return false;
      if (hasNodeManagerHost()) {
        if (!getNodeManagerHost()
            .equals(other.getNodeManagerHost())) return false;
      }
      if (hasNodeManagerPort() != other.hasNodeManagerPort()) return false;
      if (hasNodeManagerPort()) {
        if (getNodeManagerPort()
            != other.getNodeManagerPort()) return false;
      }
      if (hasNodeManagerHttpPort() != other.hasNodeManagerHttpPort()) return false;
      if (hasNodeManagerHttpPort()) {
        if (getNodeManagerHttpPort()
            != other.getNodeManagerHttpPort()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasApplicationAttemptId()) {
        hash = (37 * hash) + APPLICATION_ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationAttemptId().hashCode();
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getStartTime());
      }
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      if (hasNodeManagerHost()) {
        hash = (37 * hash) + NODE_MANAGER_HOST_FIELD_NUMBER;
        hash = (53 * hash) + getNodeManagerHost().hashCode();
      }
      if (hasNodeManagerPort()) {
        hash = (37 * hash) + NODE_MANAGER_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getNodeManagerPort();
      }
      if (hasNodeManagerHttpPort()) {
        hash = (37 * hash) + NODE_MANAGER_HTTP_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getNodeManagerHttpPort();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.AMInfoProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.AMInfoProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_AMInfoProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_AMInfoProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getApplicationAttemptIdFieldBuilder();
          getContainerIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = null;
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (containerIdBuilder_ == null) {
          containerId_ = null;
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        nodeManagerHost_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        nodeManagerPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        nodeManagerHttpPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_AMInfoProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (applicationAttemptIdBuilder_ == null) {
            result.applicationAttemptId_ = applicationAttemptId_;
          } else {
            result.applicationAttemptId_ = applicationAttemptIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.startTime_ = startTime_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (containerIdBuilder_ == null) {
            result.containerId_ = containerId_;
          } else {
            result.containerId_ = containerIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.nodeManagerHost_ = nodeManagerHost_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.nodeManagerPort_ = nodeManagerPort_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.nodeManagerHttpPort_ = nodeManagerHttpPort_;
          to_bitField0_ |= 0x00000020;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto.getDefaultInstance()) return this;
        if (other.hasApplicationAttemptId()) {
          mergeApplicationAttemptId(other.getApplicationAttemptId());
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        if (other.hasNodeManagerHost()) {
          bitField0_ |= 0x00000008;
          nodeManagerHost_ = other.nodeManagerHost_;
          onChanged();
        }
        if (other.hasNodeManagerPort()) {
          setNodeManagerPort(other.getNodeManagerPort());
        }
        if (other.hasNodeManagerHttpPort()) {
          setNodeManagerHttpPort(other.getNodeManagerHttpPort());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> applicationAttemptIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public boolean hasApplicationAttemptId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          return applicationAttemptId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : applicationAttemptId_;
        } else {
          return applicationAttemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationAttemptId_ = value;
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder builderForValue) {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = builderForValue.build();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder mergeApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              applicationAttemptId_ != null &&
              applicationAttemptId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance()) {
            applicationAttemptId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.newBuilder(applicationAttemptId_).mergeFrom(value).buildPartial();
          } else {
            applicationAttemptId_ = value;
          }
          onChanged();
        } else {
          applicationAttemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder clearApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = null;
          onChanged();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder getApplicationAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
        if (applicationAttemptIdBuilder_ != null) {
          return applicationAttemptIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationAttemptId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : applicationAttemptId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> 
          getApplicationAttemptIdFieldBuilder() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder>(
                  getApplicationAttemptId(),
                  getParentForChildren(),
                  isClean());
          applicationAttemptId_ = null;
        }
        return applicationAttemptIdBuilder_;
      }

      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 2;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int64 start_time = 2;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 2;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000002;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 2;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000002);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              containerId_ != null &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = null;
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 3;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  getContainerId(),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      private java.lang.Object nodeManagerHost_ = "";
      /**
       * <code>optional string node_manager_host = 4;</code>
       */
      public boolean hasNodeManagerHost() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional string node_manager_host = 4;</code>
       */
      public java.lang.String getNodeManagerHost() {
        java.lang.Object ref = nodeManagerHost_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            nodeManagerHost_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string node_manager_host = 4;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNodeManagerHostBytes() {
        java.lang.Object ref = nodeManagerHost_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nodeManagerHost_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string node_manager_host = 4;</code>
       */
      public Builder setNodeManagerHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        nodeManagerHost_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string node_manager_host = 4;</code>
       */
      public Builder clearNodeManagerHost() {
        bitField0_ = (bitField0_ & ~0x00000008);
        nodeManagerHost_ = getDefaultInstance().getNodeManagerHost();
        onChanged();
        return this;
      }
      /**
       * <code>optional string node_manager_host = 4;</code>
       */
      public Builder setNodeManagerHostBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        nodeManagerHost_ = value;
        onChanged();
        return this;
      }

      private int nodeManagerPort_ ;
      /**
       * <code>optional int32 node_manager_port = 5;</code>
       */
      public boolean hasNodeManagerPort() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional int32 node_manager_port = 5;</code>
       */
      public int getNodeManagerPort() {
        return nodeManagerPort_;
      }
      /**
       * <code>optional int32 node_manager_port = 5;</code>
       */
      public Builder setNodeManagerPort(int value) {
        bitField0_ |= 0x00000010;
        nodeManagerPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 node_manager_port = 5;</code>
       */
      public Builder clearNodeManagerPort() {
        bitField0_ = (bitField0_ & ~0x00000010);
        nodeManagerPort_ = 0;
        onChanged();
        return this;
      }

      private int nodeManagerHttpPort_ ;
      /**
       * <code>optional int32 node_manager_http_port = 6;</code>
       */
      public boolean hasNodeManagerHttpPort() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional int32 node_manager_http_port = 6;</code>
       */
      public int getNodeManagerHttpPort() {
        return nodeManagerHttpPort_;
      }
      /**
       * <code>optional int32 node_manager_http_port = 6;</code>
       */
      public Builder setNodeManagerHttpPort(int value) {
        bitField0_ |= 0x00000020;
        nodeManagerHttpPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 node_manager_http_port = 6;</code>
       */
      public Builder clearNodeManagerHttpPort() {
        bitField0_ = (bitField0_ & ~0x00000020);
        nodeManagerHttpPort_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.AMInfoProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.AMInfoProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<AMInfoProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<AMInfoProto>() {
      @java.lang.Override
      public AMInfoProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new AMInfoProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<AMInfoProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<AMInfoProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.AMInfoProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TaskAttemptCompletionEventProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.TaskAttemptCompletionEventProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
     */
    boolean hasAttemptId();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getAttemptId();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getAttemptIdOrBuilder();

    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto getStatus();

    /**
     * <code>optional string map_output_server_address = 3;</code>
     */
    boolean hasMapOutputServerAddress();
    /**
     * <code>optional string map_output_server_address = 3;</code>
     */
    java.lang.String getMapOutputServerAddress();
    /**
     * <code>optional string map_output_server_address = 3;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getMapOutputServerAddressBytes();

    /**
     * <code>optional int32 attempt_run_time = 4;</code>
     */
    boolean hasAttemptRunTime();
    /**
     * <code>optional int32 attempt_run_time = 4;</code>
     */
    int getAttemptRunTime();

    /**
     * <code>optional int32 event_id = 5;</code>
     */
    boolean hasEventId();
    /**
     * <code>optional int32 event_id = 5;</code>
     */
    int getEventId();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.TaskAttemptCompletionEventProto}
   */
  public  static final class TaskAttemptCompletionEventProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.TaskAttemptCompletionEventProto)
      TaskAttemptCompletionEventProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TaskAttemptCompletionEventProto.newBuilder() to construct.
    private TaskAttemptCompletionEventProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TaskAttemptCompletionEventProto() {
      status_ = 1;
      mapOutputServerAddress_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TaskAttemptCompletionEventProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = attemptId_.toBuilder();
              }
              attemptId_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(attemptId_);
                attemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto value = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                status_ = rawValue;
              }
              break;
            }
            case 26: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              mapOutputServerAddress_ = bs;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              attemptRunTime_ = input.readInt32();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              eventId_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.Builder.class);
    }

    private int bitField0_;
    public static final int ATTEMPT_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto attemptId_;
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
     */
    public boolean hasAttemptId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getAttemptId() {
      return attemptId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : attemptId_;
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getAttemptIdOrBuilder() {
      return attemptId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : attemptId_;
    }

    public static final int STATUS_FIELD_NUMBER = 2;
    private int status_;
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto getStatus() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto.valueOf(status_);
      return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto.TACE_FAILED : result;
    }

    public static final int MAP_OUTPUT_SERVER_ADDRESS_FIELD_NUMBER = 3;
    private volatile java.lang.Object mapOutputServerAddress_;
    /**
     * <code>optional string map_output_server_address = 3;</code>
     */
    public boolean hasMapOutputServerAddress() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional string map_output_server_address = 3;</code>
     */
    public java.lang.String getMapOutputServerAddress() {
      java.lang.Object ref = mapOutputServerAddress_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          mapOutputServerAddress_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string map_output_server_address = 3;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getMapOutputServerAddressBytes() {
      java.lang.Object ref = mapOutputServerAddress_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mapOutputServerAddress_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int ATTEMPT_RUN_TIME_FIELD_NUMBER = 4;
    private int attemptRunTime_;
    /**
     * <code>optional int32 attempt_run_time = 4;</code>
     */
    public boolean hasAttemptRunTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int32 attempt_run_time = 4;</code>
     */
    public int getAttemptRunTime() {
      return attemptRunTime_;
    }

    public static final int EVENT_ID_FIELD_NUMBER = 5;
    private int eventId_;
    /**
     * <code>optional int32 event_id = 5;</code>
     */
    public boolean hasEventId() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int32 event_id = 5;</code>
     */
    public int getEventId() {
      return eventId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, status_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 3, mapOutputServerAddress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt32(4, attemptRunTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt32(5, eventId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, status_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(3, mapOutputServerAddress_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(4, attemptRunTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(5, eventId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto) obj;

      if (hasAttemptId() != other.hasAttemptId()) return false;
      if (hasAttemptId()) {
        if (!getAttemptId()
            .equals(other.getAttemptId())) return false;
      }
      if (hasStatus() != other.hasStatus()) return false;
      if (hasStatus()) {
        if (status_ != other.status_) return false;
      }
      if (hasMapOutputServerAddress() != other.hasMapOutputServerAddress()) return false;
      if (hasMapOutputServerAddress()) {
        if (!getMapOutputServerAddress()
            .equals(other.getMapOutputServerAddress())) return false;
      }
      if (hasAttemptRunTime() != other.hasAttemptRunTime()) return false;
      if (hasAttemptRunTime()) {
        if (getAttemptRunTime()
            != other.getAttemptRunTime()) return false;
      }
      if (hasEventId() != other.hasEventId()) return false;
      if (hasEventId()) {
        if (getEventId()
            != other.getEventId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAttemptId()) {
        hash = (37 * hash) + ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getAttemptId().hashCode();
      }
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + status_;
      }
      if (hasMapOutputServerAddress()) {
        hash = (37 * hash) + MAP_OUTPUT_SERVER_ADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getMapOutputServerAddress().hashCode();
      }
      if (hasAttemptRunTime()) {
        hash = (37 * hash) + ATTEMPT_RUN_TIME_FIELD_NUMBER;
        hash = (53 * hash) + getAttemptRunTime();
      }
      if (hasEventId()) {
        hash = (37 * hash) + EVENT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getEventId();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.TaskAttemptCompletionEventProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.TaskAttemptCompletionEventProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAttemptIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (attemptIdBuilder_ == null) {
          attemptId_ = null;
        } else {
          attemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        status_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        mapOutputServerAddress_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        attemptRunTime_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        eventId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (attemptIdBuilder_ == null) {
            result.attemptId_ = attemptId_;
          } else {
            result.attemptId_ = attemptIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.status_ = status_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.mapOutputServerAddress_ = mapOutputServerAddress_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.attemptRunTime_ = attemptRunTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.eventId_ = eventId_;
          to_bitField0_ |= 0x00000010;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto.getDefaultInstance()) return this;
        if (other.hasAttemptId()) {
          mergeAttemptId(other.getAttemptId());
        }
        if (other.hasStatus()) {
          setStatus(other.getStatus());
        }
        if (other.hasMapOutputServerAddress()) {
          bitField0_ |= 0x00000004;
          mapOutputServerAddress_ = other.mapOutputServerAddress_;
          onChanged();
        }
        if (other.hasAttemptRunTime()) {
          setAttemptRunTime(other.getAttemptRunTime());
        }
        if (other.hasEventId()) {
          setEventId(other.getEventId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto attemptId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> attemptIdBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public boolean hasAttemptId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto getAttemptId() {
        if (attemptIdBuilder_ == null) {
          return attemptId_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : attemptId_;
        } else {
          return attemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public Builder setAttemptId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (attemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          attemptId_ = value;
          onChanged();
        } else {
          attemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public Builder setAttemptId(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder builderForValue) {
        if (attemptIdBuilder_ == null) {
          attemptId_ = builderForValue.build();
          onChanged();
        } else {
          attemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public Builder mergeAttemptId(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto value) {
        if (attemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              attemptId_ != null &&
              attemptId_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance()) {
            attemptId_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.newBuilder(attemptId_).mergeFrom(value).buildPartial();
          } else {
            attemptId_ = value;
          }
          onChanged();
        } else {
          attemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public Builder clearAttemptId() {
        if (attemptIdBuilder_ == null) {
          attemptId_ = null;
          onChanged();
        } else {
          attemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder getAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder getAttemptIdOrBuilder() {
        if (attemptIdBuilder_ != null) {
          return attemptIdBuilder_.getMessageOrBuilder();
        } else {
          return attemptId_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.getDefaultInstance() : attemptId_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptIdProto attempt_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder> 
          getAttemptIdFieldBuilder() {
        if (attemptIdBuilder_ == null) {
          attemptIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptIdProtoOrBuilder>(
                  getAttemptId(),
                  getParentForChildren(),
                  isClean());
          attemptId_ = null;
        }
        return attemptIdBuilder_;
      }

      private int status_ = 1;
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto getStatus() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto result = org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto.valueOf(status_);
        return result == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto.TACE_FAILED : result;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
       */
      public Builder setStatus(org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        status_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.TaskAttemptCompletionEventStatusProto status = 2;</code>
       */
      public Builder clearStatus() {
        bitField0_ = (bitField0_ & ~0x00000002);
        status_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object mapOutputServerAddress_ = "";
      /**
       * <code>optional string map_output_server_address = 3;</code>
       */
      public boolean hasMapOutputServerAddress() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional string map_output_server_address = 3;</code>
       */
      public java.lang.String getMapOutputServerAddress() {
        java.lang.Object ref = mapOutputServerAddress_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            mapOutputServerAddress_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string map_output_server_address = 3;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getMapOutputServerAddressBytes() {
        java.lang.Object ref = mapOutputServerAddress_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mapOutputServerAddress_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string map_output_server_address = 3;</code>
       */
      public Builder setMapOutputServerAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        mapOutputServerAddress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string map_output_server_address = 3;</code>
       */
      public Builder clearMapOutputServerAddress() {
        bitField0_ = (bitField0_ & ~0x00000004);
        mapOutputServerAddress_ = getDefaultInstance().getMapOutputServerAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string map_output_server_address = 3;</code>
       */
      public Builder setMapOutputServerAddressBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        mapOutputServerAddress_ = value;
        onChanged();
        return this;
      }

      private int attemptRunTime_ ;
      /**
       * <code>optional int32 attempt_run_time = 4;</code>
       */
      public boolean hasAttemptRunTime() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int32 attempt_run_time = 4;</code>
       */
      public int getAttemptRunTime() {
        return attemptRunTime_;
      }
      /**
       * <code>optional int32 attempt_run_time = 4;</code>
       */
      public Builder setAttemptRunTime(int value) {
        bitField0_ |= 0x00000008;
        attemptRunTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 attempt_run_time = 4;</code>
       */
      public Builder clearAttemptRunTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        attemptRunTime_ = 0;
        onChanged();
        return this;
      }

      private int eventId_ ;
      /**
       * <code>optional int32 event_id = 5;</code>
       */
      public boolean hasEventId() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional int32 event_id = 5;</code>
       */
      public int getEventId() {
        return eventId_;
      }
      /**
       * <code>optional int32 event_id = 5;</code>
       */
      public Builder setEventId(int value) {
        bitField0_ |= 0x00000010;
        eventId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 event_id = 5;</code>
       */
      public Builder clearEventId() {
        bitField0_ = (bitField0_ & ~0x00000010);
        eventId_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.TaskAttemptCompletionEventProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.TaskAttemptCompletionEventProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptCompletionEventProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<TaskAttemptCompletionEventProto>() {
      @java.lang.Override
      public TaskAttemptCompletionEventProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new TaskAttemptCompletionEventProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptCompletionEventProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<TaskAttemptCompletionEventProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.TaskAttemptCompletionEventProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StringCounterMapProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.StringCounterMapProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string key = 1;</code>
     */
    boolean hasKey();
    /**
     * <code>optional string key = 1;</code>
     */
    java.lang.String getKey();
    /**
     * <code>optional string key = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getKeyBytes();

    /**
     * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
     */
    boolean hasValue();
    /**
     * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto getValue();
    /**
     * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder getValueOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.StringCounterMapProto}
   */
  public  static final class StringCounterMapProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.StringCounterMapProto)
      StringCounterMapProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StringCounterMapProto.newBuilder() to construct.
    private StringCounterMapProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StringCounterMapProto() {
      key_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StringCounterMapProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              key_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = value_.toBuilder();
              }
              value_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(value_);
                value_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterMapProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterMapProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder.class);
    }

    private int bitField0_;
    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>optional string key = 1;</code>
     */
    public boolean hasKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string key = 1;</code>
     */
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          key_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string key = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 2;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto value_;
    /**
     * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
     */
    public boolean hasValue() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto getValue() {
      return value_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance() : value_;
    }
    /**
     * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder getValueOrBuilder() {
      return value_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance() : value_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getValue());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getValue());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto) obj;

      if (hasKey() != other.hasKey()) return false;
      if (hasKey()) {
        if (!getKey()
            .equals(other.getKey())) return false;
      }
      if (hasValue() != other.hasValue()) return false;
      if (hasValue()) {
        if (!getValue()
            .equals(other.getValue())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasKey()) {
        hash = (37 * hash) + KEY_FIELD_NUMBER;
        hash = (53 * hash) + getKey().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + getValue().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.StringCounterMapProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.StringCounterMapProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterMapProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterMapProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getValueFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (valueBuilder_ == null) {
          value_ = null;
        } else {
          valueBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterMapProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.key_ = key_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (valueBuilder_ == null) {
            result.value_ = value_;
          } else {
            result.value_ = valueBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto.getDefaultInstance()) return this;
        if (other.hasKey()) {
          bitField0_ |= 0x00000001;
          key_ = other.key_;
          onChanged();
        }
        if (other.hasValue()) {
          mergeValue(other.getValue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object key_ = "";
      /**
       * <code>optional string key = 1;</code>
       */
      public boolean hasKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            key_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder clearKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder setKeyBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto value_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder> valueBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public boolean hasValue() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto getValue() {
        if (valueBuilder_ == null) {
          return value_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance() : value_;
        } else {
          return valueBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public Builder setValue(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto value) {
        if (valueBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          value_ = value;
          onChanged();
        } else {
          valueBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public Builder setValue(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder builderForValue) {
        if (valueBuilder_ == null) {
          value_ = builderForValue.build();
          onChanged();
        } else {
          valueBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public Builder mergeValue(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto value) {
        if (valueBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              value_ != null &&
              value_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance()) {
            value_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.newBuilder(value_).mergeFrom(value).buildPartial();
          } else {
            value_ = value;
          }
          onChanged();
        } else {
          valueBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public Builder clearValue() {
        if (valueBuilder_ == null) {
          value_ = null;
          onChanged();
        } else {
          valueBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder getValueBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getValueFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder getValueOrBuilder() {
        if (valueBuilder_ != null) {
          return valueBuilder_.getMessageOrBuilder();
        } else {
          return value_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.getDefaultInstance() : value_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterProto value = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder> 
          getValueFieldBuilder() {
        if (valueBuilder_ == null) {
          valueBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterProtoOrBuilder>(
                  getValue(),
                  getParentForChildren(),
                  isClean());
          value_ = null;
        }
        return valueBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.StringCounterMapProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.StringCounterMapProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<StringCounterMapProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<StringCounterMapProto>() {
      @java.lang.Override
      public StringCounterMapProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new StringCounterMapProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<StringCounterMapProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<StringCounterMapProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterMapProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StringCounterGroupMapProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.mapreduce.StringCounterGroupMapProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string key = 1;</code>
     */
    boolean hasKey();
    /**
     * <code>optional string key = 1;</code>
     */
    java.lang.String getKey();
    /**
     * <code>optional string key = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getKeyBytes();

    /**
     * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
     */
    boolean hasValue();
    /**
     * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto getValue();
    /**
     * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
     */
    org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder getValueOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.mapreduce.StringCounterGroupMapProto}
   */
  public  static final class StringCounterGroupMapProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.mapreduce.StringCounterGroupMapProto)
      StringCounterGroupMapProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StringCounterGroupMapProto.newBuilder() to construct.
    private StringCounterGroupMapProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StringCounterGroupMapProto() {
      key_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StringCounterGroupMapProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              key_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = value_.toBuilder();
              }
              value_ = input.readMessage(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(value_);
                value_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterGroupMapProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterGroupMapProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder.class);
    }

    private int bitField0_;
    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>optional string key = 1;</code>
     */
    public boolean hasKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string key = 1;</code>
     */
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          key_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string key = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 2;
    private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto value_;
    /**
     * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
     */
    public boolean hasValue() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto getValue() {
      return value_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance() : value_;
    }
    /**
     * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
     */
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder getValueOrBuilder() {
      return value_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance() : value_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getValue());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getValue());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto other = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto) obj;

      if (hasKey() != other.hasKey()) return false;
      if (hasKey()) {
        if (!getKey()
            .equals(other.getKey())) return false;
      }
      if (hasValue() != other.hasValue()) return false;
      if (hasValue()) {
        if (!getValue()
            .equals(other.getValue())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasKey()) {
        hash = (37 * hash) + KEY_FIELD_NUMBER;
        hash = (53 * hash) + getKey().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + getValue().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.mapreduce.StringCounterGroupMapProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.mapreduce.StringCounterGroupMapProto)
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterGroupMapProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterGroupMapProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.class, org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.Builder.class);
      }

      // Construct using org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getValueFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (valueBuilder_ == null) {
          value_ = null;
        } else {
          valueBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.internal_static_hadoop_mapreduce_StringCounterGroupMapProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto getDefaultInstanceForType() {
        return org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto build() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto buildPartial() {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto result = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.key_ = key_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (valueBuilder_ == null) {
            result.value_ = value_;
          } else {
            result.value_ = valueBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto) {
          return mergeFrom((org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto other) {
        if (other == org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto.getDefaultInstance()) return this;
        if (other.hasKey()) {
          bitField0_ |= 0x00000001;
          key_ = other.key_;
          onChanged();
        }
        if (other.hasValue()) {
          mergeValue(other.getValue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object key_ = "";
      /**
       * <code>optional string key = 1;</code>
       */
      public boolean hasKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            key_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder clearKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       */
      public Builder setKeyBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto value_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder> valueBuilder_;
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public boolean hasValue() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto getValue() {
        if (valueBuilder_ == null) {
          return value_ == null ? org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance() : value_;
        } else {
          return valueBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public Builder setValue(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto value) {
        if (valueBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          value_ = value;
          onChanged();
        } else {
          valueBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public Builder setValue(
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder builderForValue) {
        if (valueBuilder_ == null) {
          value_ = builderForValue.build();
          onChanged();
        } else {
          valueBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public Builder mergeValue(org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto value) {
        if (valueBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              value_ != null &&
              value_ != org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance()) {
            value_ =
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.newBuilder(value_).mergeFrom(value).buildPartial();
          } else {
            value_ = value;
          }
          onChanged();
        } else {
          valueBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public Builder clearValue() {
        if (valueBuilder_ == null) {
          value_ = null;
          onChanged();
        } else {
          valueBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder getValueBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getValueFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      public org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder getValueOrBuilder() {
        if (valueBuilder_ != null) {
          return valueBuilder_.getMessageOrBuilder();
        } else {
          return value_ == null ?
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.getDefaultInstance() : value_;
        }
      }
      /**
       * <code>optional .hadoop.mapreduce.CounterGroupProto value = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder> 
          getValueFieldBuilder() {
        if (valueBuilder_ == null) {
          valueBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProto.Builder, org.apache.hadoop.mapreduce.v2.proto.MRProtos.CounterGroupProtoOrBuilder>(
                  getValue(),
                  getParentForChildren(),
                  isClean());
          value_ = null;
        }
        return valueBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.mapreduce.StringCounterGroupMapProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.mapreduce.StringCounterGroupMapProto)
    private static final org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto();
    }

    public static org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<StringCounterGroupMapProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<StringCounterGroupMapProto>() {
      @java.lang.Override
      public StringCounterGroupMapProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new StringCounterGroupMapProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<StringCounterGroupMapProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<StringCounterGroupMapProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.mapreduce.v2.proto.MRProtos.StringCounterGroupMapProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_JobIdProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_JobIdProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_TaskIdProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_TaskIdProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_TaskAttemptIdProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_TaskAttemptIdProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_CounterProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_CounterProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_CounterGroupProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_CounterGroupProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_CountersProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_CountersProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_TaskReportProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_TaskReportProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_TaskAttemptReportProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_TaskAttemptReportProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_JobReportProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_JobReportProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_AMInfoProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_AMInfoProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_StringCounterMapProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_StringCounterMapProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_mapreduce_StringCounterGroupMapProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_mapreduce_StringCounterGroupMapProto_fieldAccessorTable;

  public static org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\017mr_protos.proto\022\020hadoop.mapreduce\032\021yar" +
      "n_protos.proto\"I\n\nJobIdProto\022/\n\006app_id\030\001" +
      " \001(\0132\037.hadoop.yarn.ApplicationIdProto\022\n\n" +
      "\002id\030\002 \001(\005\"{\n\013TaskIdProto\022,\n\006job_id\030\001 \001(\013" +
      "2\034.hadoop.mapreduce.JobIdProto\0222\n\ttask_t" +
      "ype\030\002 \001(\0162\037.hadoop.mapreduce.TaskTypePro" +
      "to\022\n\n\002id\030\003 \001(\005\"P\n\022TaskAttemptIdProto\022.\n\007" +
      "task_id\030\001 \001(\0132\035.hadoop.mapreduce.TaskIdP" +
      "roto\022\n\n\002id\030\002 \001(\005\"A\n\014CounterProto\022\014\n\004name" +
      "\030\001 \001(\t\022\024\n\014display_name\030\002 \001(\t\022\r\n\005value\030\003 " +
      "\001(\003\"r\n\021CounterGroupProto\022\014\n\004name\030\001 \001(\t\022\024" +
      "\n\014display_name\030\002 \001(\t\0229\n\010counters\030\003 \003(\0132\'" +
      ".hadoop.mapreduce.StringCounterMapProto\"" +
      "U\n\rCountersProto\022D\n\016counter_groups\030\001 \003(\013" +
      "2,.hadoop.mapreduce.StringCounterGroupMa" +
      "pProto\"\374\002\n\017TaskReportProto\022.\n\007task_id\030\001 " +
      "\001(\0132\035.hadoop.mapreduce.TaskIdProto\0224\n\nta" +
      "sk_state\030\002 \001(\0162 .hadoop.mapreduce.TaskSt" +
      "ateProto\022\020\n\010progress\030\003 \001(\002\022\022\n\nstart_time" +
      "\030\004 \001(\003\022\023\n\013finish_time\030\005 \001(\003\0221\n\010counters\030" +
      "\006 \001(\0132\037.hadoop.mapreduce.CountersProto\022>" +
      "\n\020running_attempts\030\007 \003(\0132$.hadoop.mapred" +
      "uce.TaskAttemptIdProto\022@\n\022successful_att" +
      "empt\030\010 \001(\0132$.hadoop.mapreduce.TaskAttemp" +
      "tIdProto\022\023\n\013diagnostics\030\t \003(\t\"\250\004\n\026TaskAt" +
      "temptReportProto\022=\n\017task_attempt_id\030\001 \001(" +
      "\0132$.hadoop.mapreduce.TaskAttemptIdProto\022" +
      "C\n\022task_attempt_state\030\002 \001(\0162\'.hadoop.map" +
      "reduce.TaskAttemptStateProto\022\020\n\010progress" +
      "\030\003 \001(\002\022\022\n\nstart_time\030\004 \001(\003\022\023\n\013finish_tim" +
      "e\030\005 \001(\003\0221\n\010counters\030\006 \001(\0132\037.hadoop.mapre" +
      "duce.CountersProto\022\027\n\017diagnostic_info\030\007 " +
      "\001(\t\022\024\n\014state_string\030\010 \001(\t\022+\n\005phase\030\t \001(\016" +
      "2\034.hadoop.mapreduce.PhaseProto\022\033\n\023shuffl" +
      "e_finish_time\030\n \001(\003\022\030\n\020sort_finish_time\030" +
      "\013 \001(\003\022\031\n\021node_manager_host\030\014 \001(\t\022\031\n\021node" +
      "_manager_port\030\r \001(\005\022\036\n\026node_manager_http" +
      "_port\030\016 \001(\005\0223\n\014container_id\030\017 \001(\0132\035.hado" +
      "op.yarn.ContainerIdProto\"\372\003\n\016JobReportPr" +
      "oto\022,\n\006job_id\030\001 \001(\0132\034.hadoop.mapreduce.J" +
      "obIdProto\0222\n\tjob_state\030\002 \001(\0162\037.hadoop.ma" +
      "preduce.JobStateProto\022\024\n\014map_progress\030\003 " +
      "\001(\002\022\027\n\017reduce_progress\030\004 \001(\002\022\030\n\020cleanup_" +
      "progress\030\005 \001(\002\022\026\n\016setup_progress\030\006 \001(\002\022\022" +
      "\n\nstart_time\030\007 \001(\003\022\023\n\013finish_time\030\010 \001(\003\022" +
      "\014\n\004user\030\t \001(\t\022\017\n\007jobName\030\n \001(\t\022\023\n\013tracki" +
      "ngUrl\030\013 \001(\t\022\023\n\013diagnostics\030\014 \001(\t\022\017\n\007jobF" +
      "ile\030\r \001(\t\022/\n\010am_infos\030\016 \003(\0132\035.hadoop.map" +
      "reduce.AMInfoProto\022\023\n\013submit_time\030\017 \001(\003\022" +
      "\026\n\007is_uber\030\020 \001(\010:\005false\022/\n\013jobPriority\030\021" +
      " \001(\0132\032.hadoop.yarn.PriorityProto\022\023\n\013hist" +
      "oryFile\030\022 \001(\t\"\364\001\n\013AMInfoProto\022F\n\026applica" +
      "tion_attempt_id\030\001 \001(\0132&.hadoop.yarn.Appl" +
      "icationAttemptIdProto\022\022\n\nstart_time\030\002 \001(" +
      "\003\0223\n\014container_id\030\003 \001(\0132\035.hadoop.yarn.Co" +
      "ntainerIdProto\022\031\n\021node_manager_host\030\004 \001(" +
      "\t\022\031\n\021node_manager_port\030\005 \001(\005\022\036\n\026node_man" +
      "ager_http_port\030\006 \001(\005\"\363\001\n\037TaskAttemptComp" +
      "letionEventProto\0228\n\nattempt_id\030\001 \001(\0132$.h" +
      "adoop.mapreduce.TaskAttemptIdProto\022G\n\006st" +
      "atus\030\002 \001(\01627.hadoop.mapreduce.TaskAttemp" +
      "tCompletionEventStatusProto\022!\n\031map_outpu" +
      "t_server_address\030\003 \001(\t\022\030\n\020attempt_run_ti" +
      "me\030\004 \001(\005\022\020\n\010event_id\030\005 \001(\005\"S\n\025StringCoun" +
      "terMapProto\022\013\n\003key\030\001 \001(\t\022-\n\005value\030\002 \001(\0132" +
      "\036.hadoop.mapreduce.CounterProto\"]\n\032Strin" +
      "gCounterGroupMapProto\022\013\n\003key\030\001 \001(\t\0222\n\005va" +
      "lue\030\002 \001(\0132#.hadoop.mapreduce.CounterGrou" +
      "pProto*$\n\rTaskTypeProto\022\007\n\003MAP\020\001\022\n\n\006REDU" +
      "CE\020\002*n\n\016TaskStateProto\022\n\n\006TS_NEW\020\001\022\020\n\014TS" +
      "_SCHEDULED\020\002\022\016\n\nTS_RUNNING\020\003\022\020\n\014TS_SUCCE" +
      "EDED\020\004\022\r\n\tTS_FAILED\020\005\022\r\n\tTS_KILLED\020\006*_\n\n" +
      "PhaseProto\022\016\n\nP_STARTING\020\001\022\t\n\005P_MAP\020\002\022\r\n" +
      "\tP_SHUFFLE\020\003\022\n\n\006P_SORT\020\004\022\014\n\010P_REDUCE\020\005\022\r" +
      "\n\tP_CLEANUP\020\006*\213\001\n\025TaskAttemptStateProto\022" +
      "\n\n\006TA_NEW\020\001\022\017\n\013TA_STARTING\020\002\022\016\n\nTA_RUNNI" +
      "NG\020\003\022\025\n\021TA_COMMIT_PENDING\020\004\022\020\n\014TA_SUCCEE" +
      "DED\020\005\022\r\n\tTA_FAILED\020\006\022\r\n\tTA_KILLED\020\007*q\n\rJ" +
      "obStateProto\022\t\n\005J_NEW\020\001\022\014\n\010J_INITED\020\002\022\r\n" +
      "\tJ_RUNNING\020\003\022\017\n\013J_SUCCEEDED\020\004\022\014\n\010J_FAILE" +
      "D\020\005\022\014\n\010J_KILLED\020\006\022\013\n\007J_ERROR\020\007*\204\001\n%TaskA" +
      "ttemptCompletionEventStatusProto\022\017\n\013TACE" +
      "_FAILED\020\001\022\017\n\013TACE_KILLED\020\002\022\022\n\016TACE_SUCCE" +
      "EDED\020\003\022\021\n\rTACE_OBSOLETE\020\004\022\022\n\016TACE_TIPFAI" +
      "LED\020\005B6\n$org.apache.hadoop.mapreduce.v2." +
      "protoB\010MRProtos\210\001\001\240\001\001"
    };
    org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
          public org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry assignDescriptors(
              org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor root) {
            descriptor = root;
            return null;
          }
        };
    org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor(),
        }, assigner);
    internal_static_hadoop_mapreduce_JobIdProto_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_hadoop_mapreduce_JobIdProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_JobIdProto_descriptor,
        new java.lang.String[] { "AppId", "Id", });
    internal_static_hadoop_mapreduce_TaskIdProto_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_hadoop_mapreduce_TaskIdProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_TaskIdProto_descriptor,
        new java.lang.String[] { "JobId", "TaskType", "Id", });
    internal_static_hadoop_mapreduce_TaskAttemptIdProto_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_hadoop_mapreduce_TaskAttemptIdProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_TaskAttemptIdProto_descriptor,
        new java.lang.String[] { "TaskId", "Id", });
    internal_static_hadoop_mapreduce_CounterProto_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_hadoop_mapreduce_CounterProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_CounterProto_descriptor,
        new java.lang.String[] { "Name", "DisplayName", "Value", });
    internal_static_hadoop_mapreduce_CounterGroupProto_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_hadoop_mapreduce_CounterGroupProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_CounterGroupProto_descriptor,
        new java.lang.String[] { "Name", "DisplayName", "Counters", });
    internal_static_hadoop_mapreduce_CountersProto_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_hadoop_mapreduce_CountersProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_CountersProto_descriptor,
        new java.lang.String[] { "CounterGroups", });
    internal_static_hadoop_mapreduce_TaskReportProto_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_hadoop_mapreduce_TaskReportProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_TaskReportProto_descriptor,
        new java.lang.String[] { "TaskId", "TaskState", "Progress", "StartTime", "FinishTime", "Counters", "RunningAttempts", "SuccessfulAttempt", "Diagnostics", });
    internal_static_hadoop_mapreduce_TaskAttemptReportProto_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_hadoop_mapreduce_TaskAttemptReportProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_TaskAttemptReportProto_descriptor,
        new java.lang.String[] { "TaskAttemptId", "TaskAttemptState", "Progress", "StartTime", "FinishTime", "Counters", "DiagnosticInfo", "StateString", "Phase", "ShuffleFinishTime", "SortFinishTime", "NodeManagerHost", "NodeManagerPort", "NodeManagerHttpPort", "ContainerId", });
    internal_static_hadoop_mapreduce_JobReportProto_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_hadoop_mapreduce_JobReportProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_JobReportProto_descriptor,
        new java.lang.String[] { "JobId", "JobState", "MapProgress", "ReduceProgress", "CleanupProgress", "SetupProgress", "StartTime", "FinishTime", "User", "JobName", "TrackingUrl", "Diagnostics", "JobFile", "AmInfos", "SubmitTime", "IsUber", "JobPriority", "HistoryFile", });
    internal_static_hadoop_mapreduce_AMInfoProto_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_hadoop_mapreduce_AMInfoProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_AMInfoProto_descriptor,
        new java.lang.String[] { "ApplicationAttemptId", "StartTime", "ContainerId", "NodeManagerHost", "NodeManagerPort", "NodeManagerHttpPort", });
    internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_TaskAttemptCompletionEventProto_descriptor,
        new java.lang.String[] { "AttemptId", "Status", "MapOutputServerAddress", "AttemptRunTime", "EventId", });
    internal_static_hadoop_mapreduce_StringCounterMapProto_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_hadoop_mapreduce_StringCounterMapProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_StringCounterMapProto_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_hadoop_mapreduce_StringCounterGroupMapProto_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_hadoop_mapreduce_StringCounterGroupMapProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_mapreduce_StringCounterGroupMapProto_descriptor,
        new java.lang.String[] { "Key", "Value", });
    org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}
