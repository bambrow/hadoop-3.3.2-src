// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: yarn_server_common_service_protos.proto

package org.apache.hadoop.yarn.proto;

public final class YarnServerCommonServiceProtos {
  private YarnServerCommonServiceProtos() {}
  public static void registerAllExtensions(
      org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite) registry);
  }
  public interface RemoteNodeProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.RemoteNodeProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    boolean hasNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder();

    /**
     * <code>optional string http_address = 2;</code>
     */
    boolean hasHttpAddress();
    /**
     * <code>optional string http_address = 2;</code>
     */
    java.lang.String getHttpAddress();
    /**
     * <code>optional string http_address = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getHttpAddressBytes();

    /**
     * <code>optional string rack_name = 3;</code>
     */
    boolean hasRackName();
    /**
     * <code>optional string rack_name = 3;</code>
     */
    java.lang.String getRackName();
    /**
     * <code>optional string rack_name = 3;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getRackNameBytes();

    /**
     * <code>optional string node_partition = 4;</code>
     */
    boolean hasNodePartition();
    /**
     * <code>optional string node_partition = 4;</code>
     */
    java.lang.String getNodePartition();
    /**
     * <code>optional string node_partition = 4;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodePartitionBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RemoteNodeProto}
   */
  public  static final class RemoteNodeProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.RemoteNodeProto)
      RemoteNodeProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RemoteNodeProto.newBuilder() to construct.
    private RemoteNodeProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RemoteNodeProto() {
      httpAddress_ = "";
      rackName_ = "";
      nodePartition_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RemoteNodeProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = nodeId_.toBuilder();
              }
              nodeId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeId_);
                nodeId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              httpAddress_ = bs;
              break;
            }
            case 26: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              rackName_ = bs;
              break;
            }
            case 34: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000008;
              nodePartition_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RemoteNodeProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RemoteNodeProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder.class);
    }

    private int bitField0_;
    public static final int NODE_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public boolean hasNodeId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
      return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
      return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
    }

    public static final int HTTP_ADDRESS_FIELD_NUMBER = 2;
    private volatile java.lang.Object httpAddress_;
    /**
     * <code>optional string http_address = 2;</code>
     */
    public boolean hasHttpAddress() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string http_address = 2;</code>
     */
    public java.lang.String getHttpAddress() {
      java.lang.Object ref = httpAddress_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          httpAddress_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string http_address = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getHttpAddressBytes() {
      java.lang.Object ref = httpAddress_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        httpAddress_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int RACK_NAME_FIELD_NUMBER = 3;
    private volatile java.lang.Object rackName_;
    /**
     * <code>optional string rack_name = 3;</code>
     */
    public boolean hasRackName() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional string rack_name = 3;</code>
     */
    public java.lang.String getRackName() {
      java.lang.Object ref = rackName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          rackName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string rack_name = 3;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getRackNameBytes() {
      java.lang.Object ref = rackName_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        rackName_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int NODE_PARTITION_FIELD_NUMBER = 4;
    private volatile java.lang.Object nodePartition_;
    /**
     * <code>optional string node_partition = 4;</code>
     */
    public boolean hasNodePartition() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional string node_partition = 4;</code>
     */
    public java.lang.String getNodePartition() {
      java.lang.Object ref = nodePartition_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nodePartition_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string node_partition = 4;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodePartitionBytes() {
      java.lang.Object ref = nodePartition_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nodePartition_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getNodeId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 2, httpAddress_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 3, rackName_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 4, nodePartition_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getNodeId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(2, httpAddress_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(3, rackName_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(4, nodePartition_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto) obj;

      if (hasNodeId() != other.hasNodeId()) return false;
      if (hasNodeId()) {
        if (!getNodeId()
            .equals(other.getNodeId())) return false;
      }
      if (hasHttpAddress() != other.hasHttpAddress()) return false;
      if (hasHttpAddress()) {
        if (!getHttpAddress()
            .equals(other.getHttpAddress())) return false;
      }
      if (hasRackName() != other.hasRackName()) return false;
      if (hasRackName()) {
        if (!getRackName()
            .equals(other.getRackName())) return false;
      }
      if (hasNodePartition() != other.hasNodePartition()) return false;
      if (hasNodePartition()) {
        if (!getNodePartition()
            .equals(other.getNodePartition())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasNodeId()) {
        hash = (37 * hash) + NODE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getNodeId().hashCode();
      }
      if (hasHttpAddress()) {
        hash = (37 * hash) + HTTP_ADDRESS_FIELD_NUMBER;
        hash = (53 * hash) + getHttpAddress().hashCode();
      }
      if (hasRackName()) {
        hash = (37 * hash) + RACK_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getRackName().hashCode();
      }
      if (hasNodePartition()) {
        hash = (37 * hash) + NODE_PARTITION_FIELD_NUMBER;
        hash = (53 * hash) + getNodePartition().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RemoteNodeProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.RemoteNodeProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RemoteNodeProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RemoteNodeProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (nodeIdBuilder_ == null) {
          nodeId_ = null;
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        httpAddress_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        rackName_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        nodePartition_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RemoteNodeProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (nodeIdBuilder_ == null) {
            result.nodeId_ = nodeId_;
          } else {
            result.nodeId_ = nodeIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.httpAddress_ = httpAddress_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.rackName_ = rackName_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.nodePartition_ = nodePartition_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.getDefaultInstance()) return this;
        if (other.hasNodeId()) {
          mergeNodeId(other.getNodeId());
        }
        if (other.hasHttpAddress()) {
          bitField0_ |= 0x00000002;
          httpAddress_ = other.httpAddress_;
          onChanged();
        }
        if (other.hasRackName()) {
          bitField0_ |= 0x00000004;
          rackName_ = other.rackName_;
          onChanged();
        }
        if (other.hasNodePartition()) {
          bitField0_ |= 0x00000008;
          nodePartition_ = other.nodePartition_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> nodeIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public boolean hasNodeId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
        if (nodeIdBuilder_ == null) {
          return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
        } else {
          return nodeIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder setNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeId_ = value;
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder setNodeId(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder builderForValue) {
        if (nodeIdBuilder_ == null) {
          nodeId_ = builderForValue.build();
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder mergeNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              nodeId_ != null &&
              nodeId_ != org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance()) {
            nodeId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.newBuilder(nodeId_).mergeFrom(value).buildPartial();
          } else {
            nodeId_ = value;
          }
          onChanged();
        } else {
          nodeIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder clearNodeId() {
        if (nodeIdBuilder_ == null) {
          nodeId_ = null;
          onChanged();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder getNodeIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNodeIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
        if (nodeIdBuilder_ != null) {
          return nodeIdBuilder_.getMessageOrBuilder();
        } else {
          return nodeId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> 
          getNodeIdFieldBuilder() {
        if (nodeIdBuilder_ == null) {
          nodeIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder>(
                  getNodeId(),
                  getParentForChildren(),
                  isClean());
          nodeId_ = null;
        }
        return nodeIdBuilder_;
      }

      private java.lang.Object httpAddress_ = "";
      /**
       * <code>optional string http_address = 2;</code>
       */
      public boolean hasHttpAddress() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string http_address = 2;</code>
       */
      public java.lang.String getHttpAddress() {
        java.lang.Object ref = httpAddress_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            httpAddress_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string http_address = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getHttpAddressBytes() {
        java.lang.Object ref = httpAddress_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          httpAddress_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string http_address = 2;</code>
       */
      public Builder setHttpAddress(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        httpAddress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string http_address = 2;</code>
       */
      public Builder clearHttpAddress() {
        bitField0_ = (bitField0_ & ~0x00000002);
        httpAddress_ = getDefaultInstance().getHttpAddress();
        onChanged();
        return this;
      }
      /**
       * <code>optional string http_address = 2;</code>
       */
      public Builder setHttpAddressBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        httpAddress_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object rackName_ = "";
      /**
       * <code>optional string rack_name = 3;</code>
       */
      public boolean hasRackName() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional string rack_name = 3;</code>
       */
      public java.lang.String getRackName() {
        java.lang.Object ref = rackName_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            rackName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string rack_name = 3;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getRackNameBytes() {
        java.lang.Object ref = rackName_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          rackName_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string rack_name = 3;</code>
       */
      public Builder setRackName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        rackName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string rack_name = 3;</code>
       */
      public Builder clearRackName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        rackName_ = getDefaultInstance().getRackName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string rack_name = 3;</code>
       */
      public Builder setRackNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        rackName_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object nodePartition_ = "";
      /**
       * <code>optional string node_partition = 4;</code>
       */
      public boolean hasNodePartition() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional string node_partition = 4;</code>
       */
      public java.lang.String getNodePartition() {
        java.lang.Object ref = nodePartition_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            nodePartition_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string node_partition = 4;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNodePartitionBytes() {
        java.lang.Object ref = nodePartition_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nodePartition_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string node_partition = 4;</code>
       */
      public Builder setNodePartition(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        nodePartition_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string node_partition = 4;</code>
       */
      public Builder clearNodePartition() {
        bitField0_ = (bitField0_ & ~0x00000008);
        nodePartition_ = getDefaultInstance().getNodePartition();
        onChanged();
        return this;
      }
      /**
       * <code>optional string node_partition = 4;</code>
       */
      public Builder setNodePartitionBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        nodePartition_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RemoteNodeProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RemoteNodeProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<RemoteNodeProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<RemoteNodeProto>() {
      @java.lang.Override
      public RemoteNodeProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new RemoteNodeProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<RemoteNodeProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<RemoteNodeProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterDistributedSchedulingAMResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.RegisterDistributedSchedulingAMResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
     */
    boolean hasRegisterResponse();
    /**
     * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto getRegisterResponse();
    /**
     * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder getRegisterResponseOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
     */
    boolean hasMaxContainerResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaxContainerResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaxContainerResourceOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
     */
    boolean hasMinContainerResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMinContainerResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMinContainerResourceOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
     */
    boolean hasIncrContainerResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getIncrContainerResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getIncrContainerResourceOrBuilder();

    /**
     * <code>optional int32 container_token_expiry_interval = 5;</code>
     */
    boolean hasContainerTokenExpiryInterval();
    /**
     * <code>optional int32 container_token_expiry_interval = 5;</code>
     */
    int getContainerTokenExpiryInterval();

    /**
     * <code>optional int64 container_id_start = 6;</code>
     */
    boolean hasContainerIdStart();
    /**
     * <code>optional int64 container_id_start = 6;</code>
     */
    long getContainerIdStart();

    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> 
        getNodesForSchedulingList();
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getNodesForScheduling(int index);
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    int getNodesForSchedulingCount();
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
        getNodesForSchedulingOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder getNodesForSchedulingOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterDistributedSchedulingAMResponseProto}
   */
  public  static final class RegisterDistributedSchedulingAMResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.RegisterDistributedSchedulingAMResponseProto)
      RegisterDistributedSchedulingAMResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterDistributedSchedulingAMResponseProto.newBuilder() to construct.
    private RegisterDistributedSchedulingAMResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterDistributedSchedulingAMResponseProto() {
      nodesForScheduling_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterDistributedSchedulingAMResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = registerResponse_.toBuilder();
              }
              registerResponse_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(registerResponse_);
                registerResponse_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = maxContainerResource_.toBuilder();
              }
              maxContainerResource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(maxContainerResource_);
                maxContainerResource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = minContainerResource_.toBuilder();
              }
              minContainerResource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(minContainerResource_);
                minContainerResource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000008) != 0)) {
                subBuilder = incrContainerResource_.toBuilder();
              }
              incrContainerResource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(incrContainerResource_);
                incrContainerResource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000008;
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              containerTokenExpiryInterval_ = input.readInt32();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              containerIdStart_ = input.readInt64();
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                nodesForScheduling_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto>();
                mutable_bitField0_ |= 0x00000040;
              }
              nodesForScheduling_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000040) != 0)) {
          nodesForScheduling_ = java.util.Collections.unmodifiableList(nodesForScheduling_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int REGISTER_RESPONSE_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto registerResponse_;
    /**
     * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
     */
    public boolean hasRegisterResponse() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto getRegisterResponse() {
      return registerResponse_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance() : registerResponse_;
    }
    /**
     * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder getRegisterResponseOrBuilder() {
      return registerResponse_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance() : registerResponse_;
    }

    public static final int MAX_CONTAINER_RESOURCE_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maxContainerResource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
     */
    public boolean hasMaxContainerResource() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaxContainerResource() {
      return maxContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : maxContainerResource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaxContainerResourceOrBuilder() {
      return maxContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : maxContainerResource_;
    }

    public static final int MIN_CONTAINER_RESOURCE_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto minContainerResource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
     */
    public boolean hasMinContainerResource() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMinContainerResource() {
      return minContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : minContainerResource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMinContainerResourceOrBuilder() {
      return minContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : minContainerResource_;
    }

    public static final int INCR_CONTAINER_RESOURCE_FIELD_NUMBER = 4;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto incrContainerResource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
     */
    public boolean hasIncrContainerResource() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getIncrContainerResource() {
      return incrContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : incrContainerResource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getIncrContainerResourceOrBuilder() {
      return incrContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : incrContainerResource_;
    }

    public static final int CONTAINER_TOKEN_EXPIRY_INTERVAL_FIELD_NUMBER = 5;
    private int containerTokenExpiryInterval_;
    /**
     * <code>optional int32 container_token_expiry_interval = 5;</code>
     */
    public boolean hasContainerTokenExpiryInterval() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int32 container_token_expiry_interval = 5;</code>
     */
    public int getContainerTokenExpiryInterval() {
      return containerTokenExpiryInterval_;
    }

    public static final int CONTAINER_ID_START_FIELD_NUMBER = 6;
    private long containerIdStart_;
    /**
     * <code>optional int64 container_id_start = 6;</code>
     */
    public boolean hasContainerIdStart() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional int64 container_id_start = 6;</code>
     */
    public long getContainerIdStart() {
      return containerIdStart_;
    }

    public static final int NODES_FOR_SCHEDULING_FIELD_NUMBER = 7;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> nodesForScheduling_;
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> getNodesForSchedulingList() {
      return nodesForScheduling_;
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
        getNodesForSchedulingOrBuilderList() {
      return nodesForScheduling_;
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    public int getNodesForSchedulingCount() {
      return nodesForScheduling_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getNodesForScheduling(int index) {
      return nodesForScheduling_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder getNodesForSchedulingOrBuilder(
        int index) {
      return nodesForScheduling_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasRegisterResponse()) {
        if (!getRegisterResponse().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasMaxContainerResource()) {
        if (!getMaxContainerResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasMinContainerResource()) {
        if (!getMinContainerResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasIncrContainerResource()) {
        if (!getIncrContainerResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getRegisterResponse());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getMaxContainerResource());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getMinContainerResource());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeMessage(4, getIncrContainerResource());
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt32(5, containerTokenExpiryInterval_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeInt64(6, containerIdStart_);
      }
      for (int i = 0; i < nodesForScheduling_.size(); i++) {
        output.writeMessage(7, nodesForScheduling_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getRegisterResponse());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getMaxContainerResource());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(3, getMinContainerResource());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(4, getIncrContainerResource());
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(5, containerTokenExpiryInterval_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(6, containerIdStart_);
      }
      for (int i = 0; i < nodesForScheduling_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(7, nodesForScheduling_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto) obj;

      if (hasRegisterResponse() != other.hasRegisterResponse()) return false;
      if (hasRegisterResponse()) {
        if (!getRegisterResponse()
            .equals(other.getRegisterResponse())) return false;
      }
      if (hasMaxContainerResource() != other.hasMaxContainerResource()) return false;
      if (hasMaxContainerResource()) {
        if (!getMaxContainerResource()
            .equals(other.getMaxContainerResource())) return false;
      }
      if (hasMinContainerResource() != other.hasMinContainerResource()) return false;
      if (hasMinContainerResource()) {
        if (!getMinContainerResource()
            .equals(other.getMinContainerResource())) return false;
      }
      if (hasIncrContainerResource() != other.hasIncrContainerResource()) return false;
      if (hasIncrContainerResource()) {
        if (!getIncrContainerResource()
            .equals(other.getIncrContainerResource())) return false;
      }
      if (hasContainerTokenExpiryInterval() != other.hasContainerTokenExpiryInterval()) return false;
      if (hasContainerTokenExpiryInterval()) {
        if (getContainerTokenExpiryInterval()
            != other.getContainerTokenExpiryInterval()) return false;
      }
      if (hasContainerIdStart() != other.hasContainerIdStart()) return false;
      if (hasContainerIdStart()) {
        if (getContainerIdStart()
            != other.getContainerIdStart()) return false;
      }
      if (!getNodesForSchedulingList()
          .equals(other.getNodesForSchedulingList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRegisterResponse()) {
        hash = (37 * hash) + REGISTER_RESPONSE_FIELD_NUMBER;
        hash = (53 * hash) + getRegisterResponse().hashCode();
      }
      if (hasMaxContainerResource()) {
        hash = (37 * hash) + MAX_CONTAINER_RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getMaxContainerResource().hashCode();
      }
      if (hasMinContainerResource()) {
        hash = (37 * hash) + MIN_CONTAINER_RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getMinContainerResource().hashCode();
      }
      if (hasIncrContainerResource()) {
        hash = (37 * hash) + INCR_CONTAINER_RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getIncrContainerResource().hashCode();
      }
      if (hasContainerTokenExpiryInterval()) {
        hash = (37 * hash) + CONTAINER_TOKEN_EXPIRY_INTERVAL_FIELD_NUMBER;
        hash = (53 * hash) + getContainerTokenExpiryInterval();
      }
      if (hasContainerIdStart()) {
        hash = (37 * hash) + CONTAINER_ID_START_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getContainerIdStart());
      }
      if (getNodesForSchedulingCount() > 0) {
        hash = (37 * hash) + NODES_FOR_SCHEDULING_FIELD_NUMBER;
        hash = (53 * hash) + getNodesForSchedulingList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterDistributedSchedulingAMResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.RegisterDistributedSchedulingAMResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRegisterResponseFieldBuilder();
          getMaxContainerResourceFieldBuilder();
          getMinContainerResourceFieldBuilder();
          getIncrContainerResourceFieldBuilder();
          getNodesForSchedulingFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (registerResponseBuilder_ == null) {
          registerResponse_ = null;
        } else {
          registerResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (maxContainerResourceBuilder_ == null) {
          maxContainerResource_ = null;
        } else {
          maxContainerResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (minContainerResourceBuilder_ == null) {
          minContainerResource_ = null;
        } else {
          minContainerResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        if (incrContainerResourceBuilder_ == null) {
          incrContainerResource_ = null;
        } else {
          incrContainerResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        containerTokenExpiryInterval_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        containerIdStart_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        if (nodesForSchedulingBuilder_ == null) {
          nodesForScheduling_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
        } else {
          nodesForSchedulingBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (registerResponseBuilder_ == null) {
            result.registerResponse_ = registerResponse_;
          } else {
            result.registerResponse_ = registerResponseBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (maxContainerResourceBuilder_ == null) {
            result.maxContainerResource_ = maxContainerResource_;
          } else {
            result.maxContainerResource_ = maxContainerResourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (minContainerResourceBuilder_ == null) {
            result.minContainerResource_ = minContainerResource_;
          } else {
            result.minContainerResource_ = minContainerResourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          if (incrContainerResourceBuilder_ == null) {
            result.incrContainerResource_ = incrContainerResource_;
          } else {
            result.incrContainerResource_ = incrContainerResourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.containerTokenExpiryInterval_ = containerTokenExpiryInterval_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.containerIdStart_ = containerIdStart_;
          to_bitField0_ |= 0x00000020;
        }
        if (nodesForSchedulingBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0)) {
            nodesForScheduling_ = java.util.Collections.unmodifiableList(nodesForScheduling_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.nodesForScheduling_ = nodesForScheduling_;
        } else {
          result.nodesForScheduling_ = nodesForSchedulingBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto.getDefaultInstance()) return this;
        if (other.hasRegisterResponse()) {
          mergeRegisterResponse(other.getRegisterResponse());
        }
        if (other.hasMaxContainerResource()) {
          mergeMaxContainerResource(other.getMaxContainerResource());
        }
        if (other.hasMinContainerResource()) {
          mergeMinContainerResource(other.getMinContainerResource());
        }
        if (other.hasIncrContainerResource()) {
          mergeIncrContainerResource(other.getIncrContainerResource());
        }
        if (other.hasContainerTokenExpiryInterval()) {
          setContainerTokenExpiryInterval(other.getContainerTokenExpiryInterval());
        }
        if (other.hasContainerIdStart()) {
          setContainerIdStart(other.getContainerIdStart());
        }
        if (nodesForSchedulingBuilder_ == null) {
          if (!other.nodesForScheduling_.isEmpty()) {
            if (nodesForScheduling_.isEmpty()) {
              nodesForScheduling_ = other.nodesForScheduling_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureNodesForSchedulingIsMutable();
              nodesForScheduling_.addAll(other.nodesForScheduling_);
            }
            onChanged();
          }
        } else {
          if (!other.nodesForScheduling_.isEmpty()) {
            if (nodesForSchedulingBuilder_.isEmpty()) {
              nodesForSchedulingBuilder_.dispose();
              nodesForSchedulingBuilder_ = null;
              nodesForScheduling_ = other.nodesForScheduling_;
              bitField0_ = (bitField0_ & ~0x00000040);
              nodesForSchedulingBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodesForSchedulingFieldBuilder() : null;
            } else {
              nodesForSchedulingBuilder_.addAllMessages(other.nodesForScheduling_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasRegisterResponse()) {
          if (!getRegisterResponse().isInitialized()) {
            return false;
          }
        }
        if (hasMaxContainerResource()) {
          if (!getMaxContainerResource().isInitialized()) {
            return false;
          }
        }
        if (hasMinContainerResource()) {
          if (!getMinContainerResource().isInitialized()) {
            return false;
          }
        }
        if (hasIncrContainerResource()) {
          if (!getIncrContainerResource().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto registerResponse_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder> registerResponseBuilder_;
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public boolean hasRegisterResponse() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto getRegisterResponse() {
        if (registerResponseBuilder_ == null) {
          return registerResponse_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance() : registerResponse_;
        } else {
          return registerResponseBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public Builder setRegisterResponse(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto value) {
        if (registerResponseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          registerResponse_ = value;
          onChanged();
        } else {
          registerResponseBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public Builder setRegisterResponse(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder builderForValue) {
        if (registerResponseBuilder_ == null) {
          registerResponse_ = builderForValue.build();
          onChanged();
        } else {
          registerResponseBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public Builder mergeRegisterResponse(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto value) {
        if (registerResponseBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              registerResponse_ != null &&
              registerResponse_ != org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance()) {
            registerResponse_ =
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.newBuilder(registerResponse_).mergeFrom(value).buildPartial();
          } else {
            registerResponse_ = value;
          }
          onChanged();
        } else {
          registerResponseBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public Builder clearRegisterResponse() {
        if (registerResponseBuilder_ == null) {
          registerResponse_ = null;
          onChanged();
        } else {
          registerResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder getRegisterResponseBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRegisterResponseFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder getRegisterResponseOrBuilder() {
        if (registerResponseBuilder_ != null) {
          return registerResponseBuilder_.getMessageOrBuilder();
        } else {
          return registerResponse_ == null ?
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance() : registerResponse_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.RegisterApplicationMasterResponseProto register_response = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder> 
          getRegisterResponseFieldBuilder() {
        if (registerResponseBuilder_ == null) {
          registerResponseBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder>(
                  getRegisterResponse(),
                  getParentForChildren(),
                  isClean());
          registerResponse_ = null;
        }
        return registerResponseBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maxContainerResource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> maxContainerResourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public boolean hasMaxContainerResource() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaxContainerResource() {
        if (maxContainerResourceBuilder_ == null) {
          return maxContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : maxContainerResource_;
        } else {
          return maxContainerResourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public Builder setMaxContainerResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maxContainerResourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          maxContainerResource_ = value;
          onChanged();
        } else {
          maxContainerResourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public Builder setMaxContainerResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (maxContainerResourceBuilder_ == null) {
          maxContainerResource_ = builderForValue.build();
          onChanged();
        } else {
          maxContainerResourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public Builder mergeMaxContainerResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maxContainerResourceBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              maxContainerResource_ != null &&
              maxContainerResource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            maxContainerResource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(maxContainerResource_).mergeFrom(value).buildPartial();
          } else {
            maxContainerResource_ = value;
          }
          onChanged();
        } else {
          maxContainerResourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public Builder clearMaxContainerResource() {
        if (maxContainerResourceBuilder_ == null) {
          maxContainerResource_ = null;
          onChanged();
        } else {
          maxContainerResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getMaxContainerResourceBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getMaxContainerResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaxContainerResourceOrBuilder() {
        if (maxContainerResourceBuilder_ != null) {
          return maxContainerResourceBuilder_.getMessageOrBuilder();
        } else {
          return maxContainerResource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : maxContainerResource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto max_container_resource = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getMaxContainerResourceFieldBuilder() {
        if (maxContainerResourceBuilder_ == null) {
          maxContainerResourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getMaxContainerResource(),
                  getParentForChildren(),
                  isClean());
          maxContainerResource_ = null;
        }
        return maxContainerResourceBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto minContainerResource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> minContainerResourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public boolean hasMinContainerResource() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMinContainerResource() {
        if (minContainerResourceBuilder_ == null) {
          return minContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : minContainerResource_;
        } else {
          return minContainerResourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public Builder setMinContainerResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (minContainerResourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          minContainerResource_ = value;
          onChanged();
        } else {
          minContainerResourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public Builder setMinContainerResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (minContainerResourceBuilder_ == null) {
          minContainerResource_ = builderForValue.build();
          onChanged();
        } else {
          minContainerResourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public Builder mergeMinContainerResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (minContainerResourceBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              minContainerResource_ != null &&
              minContainerResource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            minContainerResource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(minContainerResource_).mergeFrom(value).buildPartial();
          } else {
            minContainerResource_ = value;
          }
          onChanged();
        } else {
          minContainerResourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public Builder clearMinContainerResource() {
        if (minContainerResourceBuilder_ == null) {
          minContainerResource_ = null;
          onChanged();
        } else {
          minContainerResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getMinContainerResourceBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getMinContainerResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMinContainerResourceOrBuilder() {
        if (minContainerResourceBuilder_ != null) {
          return minContainerResourceBuilder_.getMessageOrBuilder();
        } else {
          return minContainerResource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : minContainerResource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto min_container_resource = 3;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getMinContainerResourceFieldBuilder() {
        if (minContainerResourceBuilder_ == null) {
          minContainerResourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getMinContainerResource(),
                  getParentForChildren(),
                  isClean());
          minContainerResource_ = null;
        }
        return minContainerResourceBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto incrContainerResource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> incrContainerResourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public boolean hasIncrContainerResource() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getIncrContainerResource() {
        if (incrContainerResourceBuilder_ == null) {
          return incrContainerResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : incrContainerResource_;
        } else {
          return incrContainerResourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public Builder setIncrContainerResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (incrContainerResourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          incrContainerResource_ = value;
          onChanged();
        } else {
          incrContainerResourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public Builder setIncrContainerResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (incrContainerResourceBuilder_ == null) {
          incrContainerResource_ = builderForValue.build();
          onChanged();
        } else {
          incrContainerResourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public Builder mergeIncrContainerResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (incrContainerResourceBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0) &&
              incrContainerResource_ != null &&
              incrContainerResource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            incrContainerResource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(incrContainerResource_).mergeFrom(value).buildPartial();
          } else {
            incrContainerResource_ = value;
          }
          onChanged();
        } else {
          incrContainerResourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public Builder clearIncrContainerResource() {
        if (incrContainerResourceBuilder_ == null) {
          incrContainerResource_ = null;
          onChanged();
        } else {
          incrContainerResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getIncrContainerResourceBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getIncrContainerResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getIncrContainerResourceOrBuilder() {
        if (incrContainerResourceBuilder_ != null) {
          return incrContainerResourceBuilder_.getMessageOrBuilder();
        } else {
          return incrContainerResource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : incrContainerResource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto incr_container_resource = 4;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getIncrContainerResourceFieldBuilder() {
        if (incrContainerResourceBuilder_ == null) {
          incrContainerResourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getIncrContainerResource(),
                  getParentForChildren(),
                  isClean());
          incrContainerResource_ = null;
        }
        return incrContainerResourceBuilder_;
      }

      private int containerTokenExpiryInterval_ ;
      /**
       * <code>optional int32 container_token_expiry_interval = 5;</code>
       */
      public boolean hasContainerTokenExpiryInterval() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional int32 container_token_expiry_interval = 5;</code>
       */
      public int getContainerTokenExpiryInterval() {
        return containerTokenExpiryInterval_;
      }
      /**
       * <code>optional int32 container_token_expiry_interval = 5;</code>
       */
      public Builder setContainerTokenExpiryInterval(int value) {
        bitField0_ |= 0x00000010;
        containerTokenExpiryInterval_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 container_token_expiry_interval = 5;</code>
       */
      public Builder clearContainerTokenExpiryInterval() {
        bitField0_ = (bitField0_ & ~0x00000010);
        containerTokenExpiryInterval_ = 0;
        onChanged();
        return this;
      }

      private long containerIdStart_ ;
      /**
       * <code>optional int64 container_id_start = 6;</code>
       */
      public boolean hasContainerIdStart() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional int64 container_id_start = 6;</code>
       */
      public long getContainerIdStart() {
        return containerIdStart_;
      }
      /**
       * <code>optional int64 container_id_start = 6;</code>
       */
      public Builder setContainerIdStart(long value) {
        bitField0_ |= 0x00000020;
        containerIdStart_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 container_id_start = 6;</code>
       */
      public Builder clearContainerIdStart() {
        bitField0_ = (bitField0_ & ~0x00000020);
        containerIdStart_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> nodesForScheduling_ =
        java.util.Collections.emptyList();
      private void ensureNodesForSchedulingIsMutable() {
        if (!((bitField0_ & 0x00000040) != 0)) {
          nodesForScheduling_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto>(nodesForScheduling_);
          bitField0_ |= 0x00000040;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> nodesForSchedulingBuilder_;

      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> getNodesForSchedulingList() {
        if (nodesForSchedulingBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodesForScheduling_);
        } else {
          return nodesForSchedulingBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public int getNodesForSchedulingCount() {
        if (nodesForSchedulingBuilder_ == null) {
          return nodesForScheduling_.size();
        } else {
          return nodesForSchedulingBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getNodesForScheduling(int index) {
        if (nodesForSchedulingBuilder_ == null) {
          return nodesForScheduling_.get(index);
        } else {
          return nodesForSchedulingBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder setNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto value) {
        if (nodesForSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.set(index, value);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder setNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder builderForValue) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodesForSchedulingBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder addNodesForScheduling(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto value) {
        if (nodesForSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(value);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder addNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto value) {
        if (nodesForSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(index, value);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder addNodesForScheduling(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder builderForValue) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(builderForValue.build());
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder addNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder builderForValue) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder addAllNodesForScheduling(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> values) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, nodesForScheduling_);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder clearNodesForScheduling() {
        if (nodesForSchedulingBuilder_ == null) {
          nodesForScheduling_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public Builder removeNodesForScheduling(int index) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.remove(index);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder getNodesForSchedulingBuilder(
          int index) {
        return getNodesForSchedulingFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder getNodesForSchedulingOrBuilder(
          int index) {
        if (nodesForSchedulingBuilder_ == null) {
          return nodesForScheduling_.get(index);  } else {
          return nodesForSchedulingBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
           getNodesForSchedulingOrBuilderList() {
        if (nodesForSchedulingBuilder_ != null) {
          return nodesForSchedulingBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodesForScheduling_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder addNodesForSchedulingBuilder() {
        return getNodesForSchedulingFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder addNodesForSchedulingBuilder(
          int index) {
        return getNodesForSchedulingFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder> 
           getNodesForSchedulingBuilderList() {
        return getNodesForSchedulingFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
          getNodesForSchedulingFieldBuilder() {
        if (nodesForSchedulingBuilder_ == null) {
          nodesForSchedulingBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder>(
                  nodesForScheduling_,
                  ((bitField0_ & 0x00000040) != 0),
                  getParentForChildren(),
                  isClean());
          nodesForScheduling_ = null;
        }
        return nodesForSchedulingBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterDistributedSchedulingAMResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterDistributedSchedulingAMResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<RegisterDistributedSchedulingAMResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<RegisterDistributedSchedulingAMResponseProto>() {
      @java.lang.Override
      public RegisterDistributedSchedulingAMResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new RegisterDistributedSchedulingAMResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<RegisterDistributedSchedulingAMResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<RegisterDistributedSchedulingAMResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterDistributedSchedulingAMResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface DistributedSchedulingAllocateResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.DistributedSchedulingAllocateResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
     */
    boolean hasAllocateResponse();
    /**
     * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto getAllocateResponse();
    /**
     * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder getAllocateResponseOrBuilder();

    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> 
        getNodesForSchedulingList();
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getNodesForScheduling(int index);
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    int getNodesForSchedulingCount();
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
        getNodesForSchedulingOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder getNodesForSchedulingOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.DistributedSchedulingAllocateResponseProto}
   */
  public  static final class DistributedSchedulingAllocateResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.DistributedSchedulingAllocateResponseProto)
      DistributedSchedulingAllocateResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use DistributedSchedulingAllocateResponseProto.newBuilder() to construct.
    private DistributedSchedulingAllocateResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private DistributedSchedulingAllocateResponseProto() {
      nodesForScheduling_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private DistributedSchedulingAllocateResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = allocateResponse_.toBuilder();
              }
              allocateResponse_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(allocateResponse_);
                allocateResponse_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                nodesForScheduling_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              nodesForScheduling_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          nodesForScheduling_ = java.util.Collections.unmodifiableList(nodesForScheduling_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int ALLOCATE_RESPONSE_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto allocateResponse_;
    /**
     * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
     */
    public boolean hasAllocateResponse() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto getAllocateResponse() {
      return allocateResponse_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance() : allocateResponse_;
    }
    /**
     * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder getAllocateResponseOrBuilder() {
      return allocateResponse_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance() : allocateResponse_;
    }

    public static final int NODES_FOR_SCHEDULING_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> nodesForScheduling_;
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> getNodesForSchedulingList() {
      return nodesForScheduling_;
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
        getNodesForSchedulingOrBuilderList() {
      return nodesForScheduling_;
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    public int getNodesForSchedulingCount() {
      return nodesForScheduling_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getNodesForScheduling(int index) {
      return nodesForScheduling_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder getNodesForSchedulingOrBuilder(
        int index) {
      return nodesForScheduling_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasAllocateResponse()) {
        if (!getAllocateResponse().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAllocateResponse());
      }
      for (int i = 0; i < nodesForScheduling_.size(); i++) {
        output.writeMessage(2, nodesForScheduling_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAllocateResponse());
      }
      for (int i = 0; i < nodesForScheduling_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, nodesForScheduling_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto) obj;

      if (hasAllocateResponse() != other.hasAllocateResponse()) return false;
      if (hasAllocateResponse()) {
        if (!getAllocateResponse()
            .equals(other.getAllocateResponse())) return false;
      }
      if (!getNodesForSchedulingList()
          .equals(other.getNodesForSchedulingList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAllocateResponse()) {
        hash = (37 * hash) + ALLOCATE_RESPONSE_FIELD_NUMBER;
        hash = (53 * hash) + getAllocateResponse().hashCode();
      }
      if (getNodesForSchedulingCount() > 0) {
        hash = (37 * hash) + NODES_FOR_SCHEDULING_FIELD_NUMBER;
        hash = (53 * hash) + getNodesForSchedulingList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.DistributedSchedulingAllocateResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.DistributedSchedulingAllocateResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAllocateResponseFieldBuilder();
          getNodesForSchedulingFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (allocateResponseBuilder_ == null) {
          allocateResponse_ = null;
        } else {
          allocateResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (nodesForSchedulingBuilder_ == null) {
          nodesForScheduling_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          nodesForSchedulingBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (allocateResponseBuilder_ == null) {
            result.allocateResponse_ = allocateResponse_;
          } else {
            result.allocateResponse_ = allocateResponseBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (nodesForSchedulingBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            nodesForScheduling_ = java.util.Collections.unmodifiableList(nodesForScheduling_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.nodesForScheduling_ = nodesForScheduling_;
        } else {
          result.nodesForScheduling_ = nodesForSchedulingBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto.getDefaultInstance()) return this;
        if (other.hasAllocateResponse()) {
          mergeAllocateResponse(other.getAllocateResponse());
        }
        if (nodesForSchedulingBuilder_ == null) {
          if (!other.nodesForScheduling_.isEmpty()) {
            if (nodesForScheduling_.isEmpty()) {
              nodesForScheduling_ = other.nodesForScheduling_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureNodesForSchedulingIsMutable();
              nodesForScheduling_.addAll(other.nodesForScheduling_);
            }
            onChanged();
          }
        } else {
          if (!other.nodesForScheduling_.isEmpty()) {
            if (nodesForSchedulingBuilder_.isEmpty()) {
              nodesForSchedulingBuilder_.dispose();
              nodesForSchedulingBuilder_ = null;
              nodesForScheduling_ = other.nodesForScheduling_;
              bitField0_ = (bitField0_ & ~0x00000002);
              nodesForSchedulingBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodesForSchedulingFieldBuilder() : null;
            } else {
              nodesForSchedulingBuilder_.addAllMessages(other.nodesForScheduling_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasAllocateResponse()) {
          if (!getAllocateResponse().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto allocateResponse_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder> allocateResponseBuilder_;
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public boolean hasAllocateResponse() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto getAllocateResponse() {
        if (allocateResponseBuilder_ == null) {
          return allocateResponse_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance() : allocateResponse_;
        } else {
          return allocateResponseBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public Builder setAllocateResponse(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto value) {
        if (allocateResponseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          allocateResponse_ = value;
          onChanged();
        } else {
          allocateResponseBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public Builder setAllocateResponse(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder builderForValue) {
        if (allocateResponseBuilder_ == null) {
          allocateResponse_ = builderForValue.build();
          onChanged();
        } else {
          allocateResponseBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public Builder mergeAllocateResponse(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto value) {
        if (allocateResponseBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              allocateResponse_ != null &&
              allocateResponse_ != org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance()) {
            allocateResponse_ =
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.newBuilder(allocateResponse_).mergeFrom(value).buildPartial();
          } else {
            allocateResponse_ = value;
          }
          onChanged();
        } else {
          allocateResponseBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public Builder clearAllocateResponse() {
        if (allocateResponseBuilder_ == null) {
          allocateResponse_ = null;
          onChanged();
        } else {
          allocateResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder getAllocateResponseBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAllocateResponseFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder getAllocateResponseOrBuilder() {
        if (allocateResponseBuilder_ != null) {
          return allocateResponseBuilder_.getMessageOrBuilder();
        } else {
          return allocateResponse_ == null ?
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance() : allocateResponse_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.AllocateResponseProto allocate_response = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder> 
          getAllocateResponseFieldBuilder() {
        if (allocateResponseBuilder_ == null) {
          allocateResponseBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder>(
                  getAllocateResponse(),
                  getParentForChildren(),
                  isClean());
          allocateResponse_ = null;
        }
        return allocateResponseBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> nodesForScheduling_ =
        java.util.Collections.emptyList();
      private void ensureNodesForSchedulingIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          nodesForScheduling_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto>(nodesForScheduling_);
          bitField0_ |= 0x00000002;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> nodesForSchedulingBuilder_;

      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> getNodesForSchedulingList() {
        if (nodesForSchedulingBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodesForScheduling_);
        } else {
          return nodesForSchedulingBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public int getNodesForSchedulingCount() {
        if (nodesForSchedulingBuilder_ == null) {
          return nodesForScheduling_.size();
        } else {
          return nodesForSchedulingBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto getNodesForScheduling(int index) {
        if (nodesForSchedulingBuilder_ == null) {
          return nodesForScheduling_.get(index);
        } else {
          return nodesForSchedulingBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder setNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto value) {
        if (nodesForSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.set(index, value);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder setNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder builderForValue) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodesForSchedulingBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder addNodesForScheduling(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto value) {
        if (nodesForSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(value);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder addNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto value) {
        if (nodesForSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(index, value);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder addNodesForScheduling(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder builderForValue) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(builderForValue.build());
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder addNodesForScheduling(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder builderForValue) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder addAllNodesForScheduling(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto> values) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, nodesForScheduling_);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder clearNodesForScheduling() {
        if (nodesForSchedulingBuilder_ == null) {
          nodesForScheduling_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public Builder removeNodesForScheduling(int index) {
        if (nodesForSchedulingBuilder_ == null) {
          ensureNodesForSchedulingIsMutable();
          nodesForScheduling_.remove(index);
          onChanged();
        } else {
          nodesForSchedulingBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder getNodesForSchedulingBuilder(
          int index) {
        return getNodesForSchedulingFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder getNodesForSchedulingOrBuilder(
          int index) {
        if (nodesForSchedulingBuilder_ == null) {
          return nodesForScheduling_.get(index);  } else {
          return nodesForSchedulingBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
           getNodesForSchedulingOrBuilderList() {
        if (nodesForSchedulingBuilder_ != null) {
          return nodesForSchedulingBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodesForScheduling_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder addNodesForSchedulingBuilder() {
        return getNodesForSchedulingFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder addNodesForSchedulingBuilder(
          int index) {
        return getNodesForSchedulingFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.RemoteNodeProto nodes_for_scheduling = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder> 
           getNodesForSchedulingBuilderList() {
        return getNodesForSchedulingFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder> 
          getNodesForSchedulingFieldBuilder() {
        if (nodesForSchedulingBuilder_ == null) {
          nodesForSchedulingBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RemoteNodeProtoOrBuilder>(
                  nodesForScheduling_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          nodesForScheduling_ = null;
        }
        return nodesForSchedulingBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.DistributedSchedulingAllocateResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.DistributedSchedulingAllocateResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<DistributedSchedulingAllocateResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<DistributedSchedulingAllocateResponseProto>() {
      @java.lang.Override
      public DistributedSchedulingAllocateResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new DistributedSchedulingAllocateResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<DistributedSchedulingAllocateResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<DistributedSchedulingAllocateResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface DistributedSchedulingAllocateRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.DistributedSchedulingAllocateRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
     */
    boolean hasAllocateRequest();
    /**
     * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto getAllocateRequest();
    /**
     * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder getAllocateRequestOrBuilder();

    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> 
        getAllocatedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    int getAllocatedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getAllocatedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.DistributedSchedulingAllocateRequestProto}
   */
  public  static final class DistributedSchedulingAllocateRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.DistributedSchedulingAllocateRequestProto)
      DistributedSchedulingAllocateRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use DistributedSchedulingAllocateRequestProto.newBuilder() to construct.
    private DistributedSchedulingAllocateRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private DistributedSchedulingAllocateRequestProto() {
      allocatedContainers_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private DistributedSchedulingAllocateRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = allocateRequest_.toBuilder();
              }
              allocateRequest_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(allocateRequest_);
                allocateRequest_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                allocatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              allocatedContainers_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          allocatedContainers_ = java.util.Collections.unmodifiableList(allocatedContainers_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int ALLOCATE_REQUEST_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto allocateRequest_;
    /**
     * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
     */
    public boolean hasAllocateRequest() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto getAllocateRequest() {
      return allocateRequest_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance() : allocateRequest_;
    }
    /**
     * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder getAllocateRequestOrBuilder() {
      return allocateRequest_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance() : allocateRequest_;
    }

    public static final int ALLOCATED_CONTAINERS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> allocatedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getAllocatedContainersList() {
      return allocatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getAllocatedContainersOrBuilderList() {
      return allocatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    public int getAllocatedContainersCount() {
      return allocatedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index) {
      return allocatedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
        int index) {
      return allocatedContainers_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasAllocateRequest()) {
        if (!getAllocateRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getAllocatedContainersCount(); i++) {
        if (!getAllocatedContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAllocateRequest());
      }
      for (int i = 0; i < allocatedContainers_.size(); i++) {
        output.writeMessage(2, allocatedContainers_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAllocateRequest());
      }
      for (int i = 0; i < allocatedContainers_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, allocatedContainers_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto) obj;

      if (hasAllocateRequest() != other.hasAllocateRequest()) return false;
      if (hasAllocateRequest()) {
        if (!getAllocateRequest()
            .equals(other.getAllocateRequest())) return false;
      }
      if (!getAllocatedContainersList()
          .equals(other.getAllocatedContainersList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAllocateRequest()) {
        hash = (37 * hash) + ALLOCATE_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getAllocateRequest().hashCode();
      }
      if (getAllocatedContainersCount() > 0) {
        hash = (37 * hash) + ALLOCATED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getAllocatedContainersList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.DistributedSchedulingAllocateRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.DistributedSchedulingAllocateRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAllocateRequestFieldBuilder();
          getAllocatedContainersFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (allocateRequestBuilder_ == null) {
          allocateRequest_ = null;
        } else {
          allocateRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (allocatedContainersBuilder_ == null) {
          allocatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          allocatedContainersBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (allocateRequestBuilder_ == null) {
            result.allocateRequest_ = allocateRequest_;
          } else {
            result.allocateRequest_ = allocateRequestBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (allocatedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            allocatedContainers_ = java.util.Collections.unmodifiableList(allocatedContainers_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.allocatedContainers_ = allocatedContainers_;
        } else {
          result.allocatedContainers_ = allocatedContainersBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto.getDefaultInstance()) return this;
        if (other.hasAllocateRequest()) {
          mergeAllocateRequest(other.getAllocateRequest());
        }
        if (allocatedContainersBuilder_ == null) {
          if (!other.allocatedContainers_.isEmpty()) {
            if (allocatedContainers_.isEmpty()) {
              allocatedContainers_ = other.allocatedContainers_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureAllocatedContainersIsMutable();
              allocatedContainers_.addAll(other.allocatedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.allocatedContainers_.isEmpty()) {
            if (allocatedContainersBuilder_.isEmpty()) {
              allocatedContainersBuilder_.dispose();
              allocatedContainersBuilder_ = null;
              allocatedContainers_ = other.allocatedContainers_;
              bitField0_ = (bitField0_ & ~0x00000002);
              allocatedContainersBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAllocatedContainersFieldBuilder() : null;
            } else {
              allocatedContainersBuilder_.addAllMessages(other.allocatedContainers_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasAllocateRequest()) {
          if (!getAllocateRequest().isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getAllocatedContainersCount(); i++) {
          if (!getAllocatedContainers(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto allocateRequest_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder> allocateRequestBuilder_;
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public boolean hasAllocateRequest() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto getAllocateRequest() {
        if (allocateRequestBuilder_ == null) {
          return allocateRequest_ == null ? org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance() : allocateRequest_;
        } else {
          return allocateRequestBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public Builder setAllocateRequest(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto value) {
        if (allocateRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          allocateRequest_ = value;
          onChanged();
        } else {
          allocateRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public Builder setAllocateRequest(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder builderForValue) {
        if (allocateRequestBuilder_ == null) {
          allocateRequest_ = builderForValue.build();
          onChanged();
        } else {
          allocateRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public Builder mergeAllocateRequest(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto value) {
        if (allocateRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              allocateRequest_ != null &&
              allocateRequest_ != org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance()) {
            allocateRequest_ =
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.newBuilder(allocateRequest_).mergeFrom(value).buildPartial();
          } else {
            allocateRequest_ = value;
          }
          onChanged();
        } else {
          allocateRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public Builder clearAllocateRequest() {
        if (allocateRequestBuilder_ == null) {
          allocateRequest_ = null;
          onChanged();
        } else {
          allocateRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder getAllocateRequestBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAllocateRequestFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder getAllocateRequestOrBuilder() {
        if (allocateRequestBuilder_ != null) {
          return allocateRequestBuilder_.getMessageOrBuilder();
        } else {
          return allocateRequest_ == null ?
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance() : allocateRequest_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.AllocateRequestProto allocate_request = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder> 
          getAllocateRequestFieldBuilder() {
        if (allocateRequestBuilder_ == null) {
          allocateRequestBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder>(
                  getAllocateRequest(),
                  getParentForChildren(),
                  isClean());
          allocateRequest_ = null;
        }
        return allocateRequestBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> allocatedContainers_ =
        java.util.Collections.emptyList();
      private void ensureAllocatedContainersIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          allocatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>(allocatedContainers_);
          bitField0_ |= 0x00000002;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> allocatedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getAllocatedContainersList() {
        if (allocatedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(allocatedContainers_);
        } else {
          return allocatedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public int getAllocatedContainersCount() {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.size();
        } else {
          return allocatedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index) {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.get(index);
        } else {
          return allocatedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder setAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.set(index, value);
          onChanged();
        } else {
          allocatedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder setAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder addAllocatedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(value);
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder addAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(index, value);
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder addAllocatedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder addAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder addAllAllocatedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> values) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, allocatedContainers_);
          onChanged();
        } else {
          allocatedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder clearAllocatedContainers() {
        if (allocatedContainersBuilder_ == null) {
          allocatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          allocatedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public Builder removeAllocatedContainers(int index) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.remove(index);
          onChanged();
        } else {
          allocatedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getAllocatedContainersBuilder(
          int index) {
        return getAllocatedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
          int index) {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.get(index);  } else {
          return allocatedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
           getAllocatedContainersOrBuilderList() {
        if (allocatedContainersBuilder_ != null) {
          return allocatedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(allocatedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addAllocatedContainersBuilder() {
        return getAllocatedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addAllocatedContainersBuilder(
          int index) {
        return getAllocatedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder> 
           getAllocatedContainersBuilderList() {
        return getAllocatedContainersFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getAllocatedContainersFieldBuilder() {
        if (allocatedContainersBuilder_ == null) {
          allocatedContainersBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  allocatedContainers_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          allocatedContainers_ = null;
        }
        return allocatedContainersBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.DistributedSchedulingAllocateRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.DistributedSchedulingAllocateRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<DistributedSchedulingAllocateRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<DistributedSchedulingAllocateRequestProto>() {
      @java.lang.Override
      public DistributedSchedulingAllocateRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new DistributedSchedulingAllocateRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<DistributedSchedulingAllocateRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<DistributedSchedulingAllocateRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.DistributedSchedulingAllocateRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NodeLabelsProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.NodeLabelsProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> 
        getNodeLabelsList();
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto getNodeLabels(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    int getNodeLabelsCount();
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
        getNodeLabelsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder getNodeLabelsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.NodeLabelsProto}
   */
  public  static final class NodeLabelsProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.NodeLabelsProto)
      NodeLabelsProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NodeLabelsProto.newBuilder() to construct.
    private NodeLabelsProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NodeLabelsProto() {
      nodeLabels_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NodeLabelsProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                nodeLabels_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              nodeLabels_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          nodeLabels_ = java.util.Collections.unmodifiableList(nodeLabels_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeLabelsProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeLabelsProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder.class);
    }

    public static final int NODELABELS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> nodeLabels_;
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> getNodeLabelsList() {
      return nodeLabels_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
        getNodeLabelsOrBuilderList() {
      return nodeLabels_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    public int getNodeLabelsCount() {
      return nodeLabels_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto getNodeLabels(int index) {
      return nodeLabels_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder getNodeLabelsOrBuilder(
        int index) {
      return nodeLabels_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < nodeLabels_.size(); i++) {
        output.writeMessage(1, nodeLabels_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < nodeLabels_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeLabels_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto) obj;

      if (!getNodeLabelsList()
          .equals(other.getNodeLabelsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getNodeLabelsCount() > 0) {
        hash = (37 * hash) + NODELABELS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeLabelsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NodeLabelsProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.NodeLabelsProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeLabelsProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeLabelsProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeLabelsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodeLabelsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeLabelsProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto(this);
        int from_bitField0_ = bitField0_;
        if (nodeLabelsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            nodeLabels_ = java.util.Collections.unmodifiableList(nodeLabels_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.nodeLabels_ = nodeLabels_;
        } else {
          result.nodeLabels_ = nodeLabelsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance()) return this;
        if (nodeLabelsBuilder_ == null) {
          if (!other.nodeLabels_.isEmpty()) {
            if (nodeLabels_.isEmpty()) {
              nodeLabels_ = other.nodeLabels_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodeLabelsIsMutable();
              nodeLabels_.addAll(other.nodeLabels_);
            }
            onChanged();
          }
        } else {
          if (!other.nodeLabels_.isEmpty()) {
            if (nodeLabelsBuilder_.isEmpty()) {
              nodeLabelsBuilder_.dispose();
              nodeLabelsBuilder_ = null;
              nodeLabels_ = other.nodeLabels_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodeLabelsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodeLabelsFieldBuilder() : null;
            } else {
              nodeLabelsBuilder_.addAllMessages(other.nodeLabels_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> nodeLabels_ =
        java.util.Collections.emptyList();
      private void ensureNodeLabelsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          nodeLabels_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto>(nodeLabels_);
          bitField0_ |= 0x00000001;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> nodeLabelsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> getNodeLabelsList() {
        if (nodeLabelsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodeLabels_);
        } else {
          return nodeLabelsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public int getNodeLabelsCount() {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_.size();
        } else {
          return nodeLabelsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto getNodeLabels(int index) {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_.get(index);
        } else {
          return nodeLabelsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder setNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeLabelsIsMutable();
          nodeLabels_.set(index, value);
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder setNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder addNodeLabels(org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(value);
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder addNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(index, value);
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder addNodeLabels(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(builderForValue.build());
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder addNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder addAllNodeLabels(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> values) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, nodeLabels_);
          onChanged();
        } else {
          nodeLabelsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder clearNodeLabels() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodeLabelsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public Builder removeNodeLabels(int index) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.remove(index);
          onChanged();
        } else {
          nodeLabelsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder getNodeLabelsBuilder(
          int index) {
        return getNodeLabelsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder getNodeLabelsOrBuilder(
          int index) {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_.get(index);  } else {
          return nodeLabelsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
           getNodeLabelsOrBuilderList() {
        if (nodeLabelsBuilder_ != null) {
          return nodeLabelsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodeLabels_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder addNodeLabelsBuilder() {
        return getNodeLabelsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder addNodeLabelsBuilder(
          int index) {
        return getNodeLabelsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder> 
           getNodeLabelsBuilderList() {
        return getNodeLabelsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
          getNodeLabelsFieldBuilder() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabelsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder>(
                  nodeLabels_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          nodeLabels_ = null;
        }
        return nodeLabelsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NodeLabelsProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NodeLabelsProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<NodeLabelsProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<NodeLabelsProto>() {
      @java.lang.Override
      public NodeLabelsProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new NodeLabelsProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<NodeLabelsProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<NodeLabelsProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NodeAttributesProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.NodeAttributesProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto> 
        getNodeAttributesList();
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto getNodeAttributes(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    int getNodeAttributesCount();
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder> 
        getNodeAttributesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder getNodeAttributesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.NodeAttributesProto}
   */
  public  static final class NodeAttributesProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.NodeAttributesProto)
      NodeAttributesProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NodeAttributesProto.newBuilder() to construct.
    private NodeAttributesProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NodeAttributesProto() {
      nodeAttributes_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NodeAttributesProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                nodeAttributes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              nodeAttributes_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          nodeAttributes_ = java.util.Collections.unmodifiableList(nodeAttributes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeAttributesProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeAttributesProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder.class);
    }

    public static final int NODEATTRIBUTES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto> nodeAttributes_;
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto> getNodeAttributesList() {
      return nodeAttributes_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder> 
        getNodeAttributesOrBuilderList() {
      return nodeAttributes_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    public int getNodeAttributesCount() {
      return nodeAttributes_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto getNodeAttributes(int index) {
      return nodeAttributes_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder getNodeAttributesOrBuilder(
        int index) {
      return nodeAttributes_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getNodeAttributesCount(); i++) {
        if (!getNodeAttributes(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < nodeAttributes_.size(); i++) {
        output.writeMessage(1, nodeAttributes_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < nodeAttributes_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeAttributes_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto) obj;

      if (!getNodeAttributesList()
          .equals(other.getNodeAttributesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getNodeAttributesCount() > 0) {
        hash = (37 * hash) + NODEATTRIBUTES_FIELD_NUMBER;
        hash = (53 * hash) + getNodeAttributesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NodeAttributesProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.NodeAttributesProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeAttributesProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeAttributesProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeAttributesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodeAttributesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeAttributesProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto(this);
        int from_bitField0_ = bitField0_;
        if (nodeAttributesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            nodeAttributes_ = java.util.Collections.unmodifiableList(nodeAttributes_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.nodeAttributes_ = nodeAttributes_;
        } else {
          result.nodeAttributes_ = nodeAttributesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance()) return this;
        if (nodeAttributesBuilder_ == null) {
          if (!other.nodeAttributes_.isEmpty()) {
            if (nodeAttributes_.isEmpty()) {
              nodeAttributes_ = other.nodeAttributes_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodeAttributesIsMutable();
              nodeAttributes_.addAll(other.nodeAttributes_);
            }
            onChanged();
          }
        } else {
          if (!other.nodeAttributes_.isEmpty()) {
            if (nodeAttributesBuilder_.isEmpty()) {
              nodeAttributesBuilder_.dispose();
              nodeAttributesBuilder_ = null;
              nodeAttributes_ = other.nodeAttributes_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodeAttributesBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getNodeAttributesFieldBuilder() : null;
            } else {
              nodeAttributesBuilder_.addAllMessages(other.nodeAttributes_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getNodeAttributesCount(); i++) {
          if (!getNodeAttributes(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto> nodeAttributes_ =
        java.util.Collections.emptyList();
      private void ensureNodeAttributesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          nodeAttributes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto>(nodeAttributes_);
          bitField0_ |= 0x00000001;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder> nodeAttributesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto> getNodeAttributesList() {
        if (nodeAttributesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodeAttributes_);
        } else {
          return nodeAttributesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public int getNodeAttributesCount() {
        if (nodeAttributesBuilder_ == null) {
          return nodeAttributes_.size();
        } else {
          return nodeAttributesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto getNodeAttributes(int index) {
        if (nodeAttributesBuilder_ == null) {
          return nodeAttributes_.get(index);
        } else {
          return nodeAttributesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder setNodeAttributes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeAttributesIsMutable();
          nodeAttributes_.set(index, value);
          onChanged();
        } else {
          nodeAttributesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder setNodeAttributes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder builderForValue) {
        if (nodeAttributesBuilder_ == null) {
          ensureNodeAttributesIsMutable();
          nodeAttributes_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeAttributesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder addNodeAttributes(org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeAttributesIsMutable();
          nodeAttributes_.add(value);
          onChanged();
        } else {
          nodeAttributesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder addNodeAttributes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeAttributesIsMutable();
          nodeAttributes_.add(index, value);
          onChanged();
        } else {
          nodeAttributesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder addNodeAttributes(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder builderForValue) {
        if (nodeAttributesBuilder_ == null) {
          ensureNodeAttributesIsMutable();
          nodeAttributes_.add(builderForValue.build());
          onChanged();
        } else {
          nodeAttributesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder addNodeAttributes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder builderForValue) {
        if (nodeAttributesBuilder_ == null) {
          ensureNodeAttributesIsMutable();
          nodeAttributes_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeAttributesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder addAllNodeAttributes(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto> values) {
        if (nodeAttributesBuilder_ == null) {
          ensureNodeAttributesIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, nodeAttributes_);
          onChanged();
        } else {
          nodeAttributesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder clearNodeAttributes() {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodeAttributesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public Builder removeNodeAttributes(int index) {
        if (nodeAttributesBuilder_ == null) {
          ensureNodeAttributesIsMutable();
          nodeAttributes_.remove(index);
          onChanged();
        } else {
          nodeAttributesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder getNodeAttributesBuilder(
          int index) {
        return getNodeAttributesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder getNodeAttributesOrBuilder(
          int index) {
        if (nodeAttributesBuilder_ == null) {
          return nodeAttributes_.get(index);  } else {
          return nodeAttributesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder> 
           getNodeAttributesOrBuilderList() {
        if (nodeAttributesBuilder_ != null) {
          return nodeAttributesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodeAttributes_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder addNodeAttributesBuilder() {
        return getNodeAttributesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder addNodeAttributesBuilder(
          int index) {
        return getNodeAttributesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeAttributeProto nodeAttributes = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder> 
           getNodeAttributesBuilderList() {
        return getNodeAttributesFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder> 
          getNodeAttributesFieldBuilder() {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributesBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeAttributeProtoOrBuilder>(
                  nodeAttributes_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          nodeAttributes_ = null;
        }
        return nodeAttributesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NodeAttributesProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NodeAttributesProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<NodeAttributesProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<NodeAttributesProto>() {
      @java.lang.Override
      public NodeAttributesProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new NodeAttributesProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<NodeAttributesProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<NodeAttributesProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterNodeManagerRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.RegisterNodeManagerRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    boolean hasNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder();

    /**
     * <code>optional int32 http_port = 3;</code>
     */
    boolean hasHttpPort();
    /**
     * <code>optional int32 http_port = 3;</code>
     */
    int getHttpPort();

    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
     */
    boolean hasResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder();

    /**
     * <code>optional string nm_version = 5;</code>
     */
    boolean hasNmVersion();
    /**
     * <code>optional string nm_version = 5;</code>
     */
    java.lang.String getNmVersion();
    /**
     * <code>optional string nm_version = 5;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNmVersionBytes();

    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto> 
        getContainerStatusesList();
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto getContainerStatuses(int index);
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    int getContainerStatusesCount();
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder> 
        getContainerStatusesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder getContainerStatusesOrBuilder(
        int index);

    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> 
        getRunningApplicationsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getRunningApplications(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    int getRunningApplicationsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
        getRunningApplicationsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getRunningApplicationsOrBuilder(
        int index);

    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
     */
    boolean hasNodeLabels();
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getNodeLabels();
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder getNodeLabelsOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
     */
    boolean hasPhysicalResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getPhysicalResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getPhysicalResourceOrBuilder();

    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> 
        getLogAggregationReportsForAppsList();
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getLogAggregationReportsForApps(int index);
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    int getLogAggregationReportsForAppsCount();
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
        getLogAggregationReportsForAppsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder getLogAggregationReportsForAppsOrBuilder(
        int index);

    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
     */
    boolean hasNodeAttributes();
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getNodeAttributes();
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder getNodeAttributesOrBuilder();

    /**
     * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
     */
    boolean hasNodeStatus();
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto getNodeStatus();
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder getNodeStatusOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterNodeManagerRequestProto}
   */
  public  static final class RegisterNodeManagerRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.RegisterNodeManagerRequestProto)
      RegisterNodeManagerRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterNodeManagerRequestProto.newBuilder() to construct.
    private RegisterNodeManagerRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterNodeManagerRequestProto() {
      nmVersion_ = "";
      containerStatuses_ = java.util.Collections.emptyList();
      runningApplications_ = java.util.Collections.emptyList();
      logAggregationReportsForApps_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterNodeManagerRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = nodeId_.toBuilder();
              }
              nodeId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeId_);
                nodeId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              httpPort_ = input.readInt32();
              break;
            }
            case 34: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = resource_.toBuilder();
              }
              resource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(resource_);
                resource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 42: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000008;
              nmVersion_ = bs;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                containerStatuses_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto>();
                mutable_bitField0_ |= 0x00000010;
              }
              containerStatuses_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.PARSER, extensionRegistry));
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                runningApplications_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              runningApplications_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry));
              break;
            }
            case 66: {
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) != 0)) {
                subBuilder = nodeLabels_.toBuilder();
              }
              nodeLabels_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeLabels_);
                nodeLabels_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 74: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) != 0)) {
                subBuilder = physicalResource_.toBuilder();
              }
              physicalResource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(physicalResource_);
                physicalResource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                logAggregationReportsForApps_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto>();
                mutable_bitField0_ |= 0x00000100;
              }
              logAggregationReportsForApps_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.PARSER, extensionRegistry));
              break;
            }
            case 90: {
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000040) != 0)) {
                subBuilder = nodeAttributes_.toBuilder();
              }
              nodeAttributes_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeAttributes_);
                nodeAttributes_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000040;
              break;
            }
            case 98: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) != 0)) {
                subBuilder = nodeStatus_.toBuilder();
              }
              nodeStatus_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeStatus_);
                nodeStatus_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          containerStatuses_ = java.util.Collections.unmodifiableList(containerStatuses_);
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          runningApplications_ = java.util.Collections.unmodifiableList(runningApplications_);
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          logAggregationReportsForApps_ = java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int NODE_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public boolean hasNodeId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
      return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
      return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
    }

    public static final int HTTP_PORT_FIELD_NUMBER = 3;
    private int httpPort_;
    /**
     * <code>optional int32 http_port = 3;</code>
     */
    public boolean hasHttpPort() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 http_port = 3;</code>
     */
    public int getHttpPort() {
      return httpPort_;
    }

    public static final int RESOURCE_FIELD_NUMBER = 4;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
     */
    public boolean hasResource() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }

    public static final int NM_VERSION_FIELD_NUMBER = 5;
    private volatile java.lang.Object nmVersion_;
    /**
     * <code>optional string nm_version = 5;</code>
     */
    public boolean hasNmVersion() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional string nm_version = 5;</code>
     */
    public java.lang.String getNmVersion() {
      java.lang.Object ref = nmVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nmVersion_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nm_version = 5;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNmVersionBytes() {
      java.lang.Object ref = nmVersion_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nmVersion_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int CONTAINER_STATUSES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto> containerStatuses_;
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto> getContainerStatusesList() {
      return containerStatuses_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder> 
        getContainerStatusesOrBuilderList() {
      return containerStatuses_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    public int getContainerStatusesCount() {
      return containerStatuses_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto getContainerStatuses(int index) {
      return containerStatuses_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder getContainerStatusesOrBuilder(
        int index) {
      return containerStatuses_.get(index);
    }

    public static final int RUNNINGAPPLICATIONS_FIELD_NUMBER = 7;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> runningApplications_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> getRunningApplicationsList() {
      return runningApplications_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
        getRunningApplicationsOrBuilderList() {
      return runningApplications_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    public int getRunningApplicationsCount() {
      return runningApplications_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getRunningApplications(int index) {
      return runningApplications_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getRunningApplicationsOrBuilder(
        int index) {
      return runningApplications_.get(index);
    }

    public static final int NODELABELS_FIELD_NUMBER = 8;
    private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto nodeLabels_;
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
     */
    public boolean hasNodeLabels() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getNodeLabels() {
      return nodeLabels_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder getNodeLabelsOrBuilder() {
      return nodeLabels_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
    }

    public static final int PHYSICALRESOURCE_FIELD_NUMBER = 9;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto physicalResource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
     */
    public boolean hasPhysicalResource() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getPhysicalResource() {
      return physicalResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : physicalResource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getPhysicalResourceOrBuilder() {
      return physicalResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : physicalResource_;
    }

    public static final int LOG_AGGREGATION_REPORTS_FOR_APPS_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> logAggregationReportsForApps_;
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> getLogAggregationReportsForAppsList() {
      return logAggregationReportsForApps_;
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
        getLogAggregationReportsForAppsOrBuilderList() {
      return logAggregationReportsForApps_;
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    public int getLogAggregationReportsForAppsCount() {
      return logAggregationReportsForApps_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getLogAggregationReportsForApps(int index) {
      return logAggregationReportsForApps_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder getLogAggregationReportsForAppsOrBuilder(
        int index) {
      return logAggregationReportsForApps_.get(index);
    }

    public static final int NODEATTRIBUTES_FIELD_NUMBER = 11;
    private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto nodeAttributes_;
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
     */
    public boolean hasNodeAttributes() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getNodeAttributes() {
      return nodeAttributes_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder getNodeAttributesOrBuilder() {
      return nodeAttributes_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
    }

    public static final int NODESTATUS_FIELD_NUMBER = 12;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto nodeStatus_;
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
     */
    public boolean hasNodeStatus() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto getNodeStatus() {
      return nodeStatus_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder getNodeStatusOrBuilder() {
      return nodeStatus_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasResource()) {
        if (!getResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getContainerStatusesCount(); i++) {
        if (!getContainerStatuses(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasPhysicalResource()) {
        if (!getPhysicalResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasNodeAttributes()) {
        if (!getNodeAttributes().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasNodeStatus()) {
        if (!getNodeStatus().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getNodeId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(3, httpPort_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(4, getResource());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 5, nmVersion_);
      }
      for (int i = 0; i < containerStatuses_.size(); i++) {
        output.writeMessage(6, containerStatuses_.get(i));
      }
      for (int i = 0; i < runningApplications_.size(); i++) {
        output.writeMessage(7, runningApplications_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeMessage(8, getNodeLabels());
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeMessage(9, getPhysicalResource());
      }
      for (int i = 0; i < logAggregationReportsForApps_.size(); i++) {
        output.writeMessage(10, logAggregationReportsForApps_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeMessage(11, getNodeAttributes());
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeMessage(12, getNodeStatus());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getNodeId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(3, httpPort_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(4, getResource());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(5, nmVersion_);
      }
      for (int i = 0; i < containerStatuses_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(6, containerStatuses_.get(i));
      }
      for (int i = 0; i < runningApplications_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(7, runningApplications_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(8, getNodeLabels());
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(9, getPhysicalResource());
      }
      for (int i = 0; i < logAggregationReportsForApps_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(10, logAggregationReportsForApps_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(11, getNodeAttributes());
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(12, getNodeStatus());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto) obj;

      if (hasNodeId() != other.hasNodeId()) return false;
      if (hasNodeId()) {
        if (!getNodeId()
            .equals(other.getNodeId())) return false;
      }
      if (hasHttpPort() != other.hasHttpPort()) return false;
      if (hasHttpPort()) {
        if (getHttpPort()
            != other.getHttpPort()) return false;
      }
      if (hasResource() != other.hasResource()) return false;
      if (hasResource()) {
        if (!getResource()
            .equals(other.getResource())) return false;
      }
      if (hasNmVersion() != other.hasNmVersion()) return false;
      if (hasNmVersion()) {
        if (!getNmVersion()
            .equals(other.getNmVersion())) return false;
      }
      if (!getContainerStatusesList()
          .equals(other.getContainerStatusesList())) return false;
      if (!getRunningApplicationsList()
          .equals(other.getRunningApplicationsList())) return false;
      if (hasNodeLabels() != other.hasNodeLabels()) return false;
      if (hasNodeLabels()) {
        if (!getNodeLabels()
            .equals(other.getNodeLabels())) return false;
      }
      if (hasPhysicalResource() != other.hasPhysicalResource()) return false;
      if (hasPhysicalResource()) {
        if (!getPhysicalResource()
            .equals(other.getPhysicalResource())) return false;
      }
      if (!getLogAggregationReportsForAppsList()
          .equals(other.getLogAggregationReportsForAppsList())) return false;
      if (hasNodeAttributes() != other.hasNodeAttributes()) return false;
      if (hasNodeAttributes()) {
        if (!getNodeAttributes()
            .equals(other.getNodeAttributes())) return false;
      }
      if (hasNodeStatus() != other.hasNodeStatus()) return false;
      if (hasNodeStatus()) {
        if (!getNodeStatus()
            .equals(other.getNodeStatus())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasNodeId()) {
        hash = (37 * hash) + NODE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getNodeId().hashCode();
      }
      if (hasHttpPort()) {
        hash = (37 * hash) + HTTP_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getHttpPort();
      }
      if (hasResource()) {
        hash = (37 * hash) + RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getResource().hashCode();
      }
      if (hasNmVersion()) {
        hash = (37 * hash) + NM_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getNmVersion().hashCode();
      }
      if (getContainerStatusesCount() > 0) {
        hash = (37 * hash) + CONTAINER_STATUSES_FIELD_NUMBER;
        hash = (53 * hash) + getContainerStatusesList().hashCode();
      }
      if (getRunningApplicationsCount() > 0) {
        hash = (37 * hash) + RUNNINGAPPLICATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getRunningApplicationsList().hashCode();
      }
      if (hasNodeLabels()) {
        hash = (37 * hash) + NODELABELS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeLabels().hashCode();
      }
      if (hasPhysicalResource()) {
        hash = (37 * hash) + PHYSICALRESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getPhysicalResource().hashCode();
      }
      if (getLogAggregationReportsForAppsCount() > 0) {
        hash = (37 * hash) + LOG_AGGREGATION_REPORTS_FOR_APPS_FIELD_NUMBER;
        hash = (53 * hash) + getLogAggregationReportsForAppsList().hashCode();
      }
      if (hasNodeAttributes()) {
        hash = (37 * hash) + NODEATTRIBUTES_FIELD_NUMBER;
        hash = (53 * hash) + getNodeAttributes().hashCode();
      }
      if (hasNodeStatus()) {
        hash = (37 * hash) + NODESTATUS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeStatus().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterNodeManagerRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.RegisterNodeManagerRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeIdFieldBuilder();
          getResourceFieldBuilder();
          getContainerStatusesFieldBuilder();
          getRunningApplicationsFieldBuilder();
          getNodeLabelsFieldBuilder();
          getPhysicalResourceFieldBuilder();
          getLogAggregationReportsForAppsFieldBuilder();
          getNodeAttributesFieldBuilder();
          getNodeStatusFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (nodeIdBuilder_ == null) {
          nodeId_ = null;
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        httpPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (resourceBuilder_ == null) {
          resource_ = null;
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        nmVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        if (containerStatusesBuilder_ == null) {
          containerStatuses_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          containerStatusesBuilder_.clear();
        }
        if (runningApplicationsBuilder_ == null) {
          runningApplications_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          runningApplicationsBuilder_.clear();
        }
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = null;
        } else {
          nodeLabelsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        if (physicalResourceBuilder_ == null) {
          physicalResource_ = null;
        } else {
          physicalResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (logAggregationReportsForAppsBuilder_ == null) {
          logAggregationReportsForApps_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          logAggregationReportsForAppsBuilder_.clear();
        }
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = null;
        } else {
          nodeAttributesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        if (nodeStatusBuilder_ == null) {
          nodeStatus_ = null;
        } else {
          nodeStatusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (nodeIdBuilder_ == null) {
            result.nodeId_ = nodeId_;
          } else {
            result.nodeId_ = nodeIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.httpPort_ = httpPort_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (resourceBuilder_ == null) {
            result.resource_ = resource_;
          } else {
            result.resource_ = resourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.nmVersion_ = nmVersion_;
        if (containerStatusesBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            containerStatuses_ = java.util.Collections.unmodifiableList(containerStatuses_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.containerStatuses_ = containerStatuses_;
        } else {
          result.containerStatuses_ = containerStatusesBuilder_.build();
        }
        if (runningApplicationsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0)) {
            runningApplications_ = java.util.Collections.unmodifiableList(runningApplications_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.runningApplications_ = runningApplications_;
        } else {
          result.runningApplications_ = runningApplicationsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          if (nodeLabelsBuilder_ == null) {
            result.nodeLabels_ = nodeLabels_;
          } else {
            result.nodeLabels_ = nodeLabelsBuilder_.build();
          }
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (physicalResourceBuilder_ == null) {
            result.physicalResource_ = physicalResource_;
          } else {
            result.physicalResource_ = physicalResourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000020;
        }
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0)) {
            logAggregationReportsForApps_ = java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.logAggregationReportsForApps_ = logAggregationReportsForApps_;
        } else {
          result.logAggregationReportsForApps_ = logAggregationReportsForAppsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000200) != 0)) {
          if (nodeAttributesBuilder_ == null) {
            result.nodeAttributes_ = nodeAttributes_;
          } else {
            result.nodeAttributes_ = nodeAttributesBuilder_.build();
          }
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000400) != 0)) {
          if (nodeStatusBuilder_ == null) {
            result.nodeStatus_ = nodeStatus_;
          } else {
            result.nodeStatus_ = nodeStatusBuilder_.build();
          }
          to_bitField0_ |= 0x00000080;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto.getDefaultInstance()) return this;
        if (other.hasNodeId()) {
          mergeNodeId(other.getNodeId());
        }
        if (other.hasHttpPort()) {
          setHttpPort(other.getHttpPort());
        }
        if (other.hasResource()) {
          mergeResource(other.getResource());
        }
        if (other.hasNmVersion()) {
          bitField0_ |= 0x00000008;
          nmVersion_ = other.nmVersion_;
          onChanged();
        }
        if (containerStatusesBuilder_ == null) {
          if (!other.containerStatuses_.isEmpty()) {
            if (containerStatuses_.isEmpty()) {
              containerStatuses_ = other.containerStatuses_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureContainerStatusesIsMutable();
              containerStatuses_.addAll(other.containerStatuses_);
            }
            onChanged();
          }
        } else {
          if (!other.containerStatuses_.isEmpty()) {
            if (containerStatusesBuilder_.isEmpty()) {
              containerStatusesBuilder_.dispose();
              containerStatusesBuilder_ = null;
              containerStatuses_ = other.containerStatuses_;
              bitField0_ = (bitField0_ & ~0x00000010);
              containerStatusesBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getContainerStatusesFieldBuilder() : null;
            } else {
              containerStatusesBuilder_.addAllMessages(other.containerStatuses_);
            }
          }
        }
        if (runningApplicationsBuilder_ == null) {
          if (!other.runningApplications_.isEmpty()) {
            if (runningApplications_.isEmpty()) {
              runningApplications_ = other.runningApplications_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureRunningApplicationsIsMutable();
              runningApplications_.addAll(other.runningApplications_);
            }
            onChanged();
          }
        } else {
          if (!other.runningApplications_.isEmpty()) {
            if (runningApplicationsBuilder_.isEmpty()) {
              runningApplicationsBuilder_.dispose();
              runningApplicationsBuilder_ = null;
              runningApplications_ = other.runningApplications_;
              bitField0_ = (bitField0_ & ~0x00000020);
              runningApplicationsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getRunningApplicationsFieldBuilder() : null;
            } else {
              runningApplicationsBuilder_.addAllMessages(other.runningApplications_);
            }
          }
        }
        if (other.hasNodeLabels()) {
          mergeNodeLabels(other.getNodeLabels());
        }
        if (other.hasPhysicalResource()) {
          mergePhysicalResource(other.getPhysicalResource());
        }
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (!other.logAggregationReportsForApps_.isEmpty()) {
            if (logAggregationReportsForApps_.isEmpty()) {
              logAggregationReportsForApps_ = other.logAggregationReportsForApps_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureLogAggregationReportsForAppsIsMutable();
              logAggregationReportsForApps_.addAll(other.logAggregationReportsForApps_);
            }
            onChanged();
          }
        } else {
          if (!other.logAggregationReportsForApps_.isEmpty()) {
            if (logAggregationReportsForAppsBuilder_.isEmpty()) {
              logAggregationReportsForAppsBuilder_.dispose();
              logAggregationReportsForAppsBuilder_ = null;
              logAggregationReportsForApps_ = other.logAggregationReportsForApps_;
              bitField0_ = (bitField0_ & ~0x00000100);
              logAggregationReportsForAppsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getLogAggregationReportsForAppsFieldBuilder() : null;
            } else {
              logAggregationReportsForAppsBuilder_.addAllMessages(other.logAggregationReportsForApps_);
            }
          }
        }
        if (other.hasNodeAttributes()) {
          mergeNodeAttributes(other.getNodeAttributes());
        }
        if (other.hasNodeStatus()) {
          mergeNodeStatus(other.getNodeStatus());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasResource()) {
          if (!getResource().isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getContainerStatusesCount(); i++) {
          if (!getContainerStatuses(i).isInitialized()) {
            return false;
          }
        }
        if (hasPhysicalResource()) {
          if (!getPhysicalResource().isInitialized()) {
            return false;
          }
        }
        if (hasNodeAttributes()) {
          if (!getNodeAttributes().isInitialized()) {
            return false;
          }
        }
        if (hasNodeStatus()) {
          if (!getNodeStatus().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> nodeIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public boolean hasNodeId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
        if (nodeIdBuilder_ == null) {
          return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
        } else {
          return nodeIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder setNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeId_ = value;
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder setNodeId(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder builderForValue) {
        if (nodeIdBuilder_ == null) {
          nodeId_ = builderForValue.build();
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder mergeNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              nodeId_ != null &&
              nodeId_ != org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance()) {
            nodeId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.newBuilder(nodeId_).mergeFrom(value).buildPartial();
          } else {
            nodeId_ = value;
          }
          onChanged();
        } else {
          nodeIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder clearNodeId() {
        if (nodeIdBuilder_ == null) {
          nodeId_ = null;
          onChanged();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder getNodeIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNodeIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
        if (nodeIdBuilder_ != null) {
          return nodeIdBuilder_.getMessageOrBuilder();
        } else {
          return nodeId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> 
          getNodeIdFieldBuilder() {
        if (nodeIdBuilder_ == null) {
          nodeIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder>(
                  getNodeId(),
                  getParentForChildren(),
                  isClean());
          nodeId_ = null;
        }
        return nodeIdBuilder_;
      }

      private int httpPort_ ;
      /**
       * <code>optional int32 http_port = 3;</code>
       */
      public boolean hasHttpPort() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 http_port = 3;</code>
       */
      public int getHttpPort() {
        return httpPort_;
      }
      /**
       * <code>optional int32 http_port = 3;</code>
       */
      public Builder setHttpPort(int value) {
        bitField0_ |= 0x00000002;
        httpPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 http_port = 3;</code>
       */
      public Builder clearHttpPort() {
        bitField0_ = (bitField0_ & ~0x00000002);
        httpPort_ = 0;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> resourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public boolean hasResource() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
        if (resourceBuilder_ == null) {
          return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        } else {
          return resourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public Builder setResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          resource_ = value;
          onChanged();
        } else {
          resourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public Builder setResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (resourceBuilder_ == null) {
          resource_ = builderForValue.build();
          onChanged();
        } else {
          resourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public Builder mergeResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              resource_ != null &&
              resource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            resource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(resource_).mergeFrom(value).buildPartial();
          } else {
            resource_ = value;
          }
          onChanged();
        } else {
          resourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public Builder clearResource() {
        if (resourceBuilder_ == null) {
          resource_ = null;
          onChanged();
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getResourceBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
        if (resourceBuilder_ != null) {
          return resourceBuilder_.getMessageOrBuilder();
        } else {
          return resource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 4;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getResourceFieldBuilder() {
        if (resourceBuilder_ == null) {
          resourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getResource(),
                  getParentForChildren(),
                  isClean());
          resource_ = null;
        }
        return resourceBuilder_;
      }

      private java.lang.Object nmVersion_ = "";
      /**
       * <code>optional string nm_version = 5;</code>
       */
      public boolean hasNmVersion() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional string nm_version = 5;</code>
       */
      public java.lang.String getNmVersion() {
        java.lang.Object ref = nmVersion_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            nmVersion_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nm_version = 5;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNmVersionBytes() {
        java.lang.Object ref = nmVersion_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nmVersion_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nm_version = 5;</code>
       */
      public Builder setNmVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        nmVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nm_version = 5;</code>
       */
      public Builder clearNmVersion() {
        bitField0_ = (bitField0_ & ~0x00000008);
        nmVersion_ = getDefaultInstance().getNmVersion();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nm_version = 5;</code>
       */
      public Builder setNmVersionBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        nmVersion_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto> containerStatuses_ =
        java.util.Collections.emptyList();
      private void ensureContainerStatusesIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          containerStatuses_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto>(containerStatuses_);
          bitField0_ |= 0x00000010;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder> containerStatusesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto> getContainerStatusesList() {
        if (containerStatusesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containerStatuses_);
        } else {
          return containerStatusesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public int getContainerStatusesCount() {
        if (containerStatusesBuilder_ == null) {
          return containerStatuses_.size();
        } else {
          return containerStatusesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto getContainerStatuses(int index) {
        if (containerStatusesBuilder_ == null) {
          return containerStatuses_.get(index);
        } else {
          return containerStatusesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder setContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto value) {
        if (containerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerStatusesIsMutable();
          containerStatuses_.set(index, value);
          onChanged();
        } else {
          containerStatusesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder setContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder builderForValue) {
        if (containerStatusesBuilder_ == null) {
          ensureContainerStatusesIsMutable();
          containerStatuses_.set(index, builderForValue.build());
          onChanged();
        } else {
          containerStatusesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder addContainerStatuses(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto value) {
        if (containerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerStatusesIsMutable();
          containerStatuses_.add(value);
          onChanged();
        } else {
          containerStatusesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder addContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto value) {
        if (containerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerStatusesIsMutable();
          containerStatuses_.add(index, value);
          onChanged();
        } else {
          containerStatusesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder addContainerStatuses(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder builderForValue) {
        if (containerStatusesBuilder_ == null) {
          ensureContainerStatusesIsMutable();
          containerStatuses_.add(builderForValue.build());
          onChanged();
        } else {
          containerStatusesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder addContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder builderForValue) {
        if (containerStatusesBuilder_ == null) {
          ensureContainerStatusesIsMutable();
          containerStatuses_.add(index, builderForValue.build());
          onChanged();
        } else {
          containerStatusesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder addAllContainerStatuses(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto> values) {
        if (containerStatusesBuilder_ == null) {
          ensureContainerStatusesIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, containerStatuses_);
          onChanged();
        } else {
          containerStatusesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder clearContainerStatuses() {
        if (containerStatusesBuilder_ == null) {
          containerStatuses_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          containerStatusesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public Builder removeContainerStatuses(int index) {
        if (containerStatusesBuilder_ == null) {
          ensureContainerStatusesIsMutable();
          containerStatuses_.remove(index);
          onChanged();
        } else {
          containerStatusesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder getContainerStatusesBuilder(
          int index) {
        return getContainerStatusesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder getContainerStatusesOrBuilder(
          int index) {
        if (containerStatusesBuilder_ == null) {
          return containerStatuses_.get(index);  } else {
          return containerStatusesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder> 
           getContainerStatusesOrBuilderList() {
        if (containerStatusesBuilder_ != null) {
          return containerStatusesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containerStatuses_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder addContainerStatusesBuilder() {
        return getContainerStatusesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder addContainerStatusesBuilder(
          int index) {
        return getContainerStatusesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMContainerStatusProto container_statuses = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder> 
           getContainerStatusesBuilderList() {
        return getContainerStatusesFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder> 
          getContainerStatusesFieldBuilder() {
        if (containerStatusesBuilder_ == null) {
          containerStatusesBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder>(
                  containerStatuses_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          containerStatuses_ = null;
        }
        return containerStatusesBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> runningApplications_ =
        java.util.Collections.emptyList();
      private void ensureRunningApplicationsIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          runningApplications_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto>(runningApplications_);
          bitField0_ |= 0x00000020;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> runningApplicationsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> getRunningApplicationsList() {
        if (runningApplicationsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(runningApplications_);
        } else {
          return runningApplicationsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public int getRunningApplicationsCount() {
        if (runningApplicationsBuilder_ == null) {
          return runningApplications_.size();
        } else {
          return runningApplicationsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getRunningApplications(int index) {
        if (runningApplicationsBuilder_ == null) {
          return runningApplications_.get(index);
        } else {
          return runningApplicationsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder setRunningApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (runningApplicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRunningApplicationsIsMutable();
          runningApplications_.set(index, value);
          onChanged();
        } else {
          runningApplicationsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder setRunningApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (runningApplicationsBuilder_ == null) {
          ensureRunningApplicationsIsMutable();
          runningApplications_.set(index, builderForValue.build());
          onChanged();
        } else {
          runningApplicationsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder addRunningApplications(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (runningApplicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRunningApplicationsIsMutable();
          runningApplications_.add(value);
          onChanged();
        } else {
          runningApplicationsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder addRunningApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (runningApplicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRunningApplicationsIsMutable();
          runningApplications_.add(index, value);
          onChanged();
        } else {
          runningApplicationsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder addRunningApplications(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (runningApplicationsBuilder_ == null) {
          ensureRunningApplicationsIsMutable();
          runningApplications_.add(builderForValue.build());
          onChanged();
        } else {
          runningApplicationsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder addRunningApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (runningApplicationsBuilder_ == null) {
          ensureRunningApplicationsIsMutable();
          runningApplications_.add(index, builderForValue.build());
          onChanged();
        } else {
          runningApplicationsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder addAllRunningApplications(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> values) {
        if (runningApplicationsBuilder_ == null) {
          ensureRunningApplicationsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, runningApplications_);
          onChanged();
        } else {
          runningApplicationsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder clearRunningApplications() {
        if (runningApplicationsBuilder_ == null) {
          runningApplications_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          runningApplicationsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public Builder removeRunningApplications(int index) {
        if (runningApplicationsBuilder_ == null) {
          ensureRunningApplicationsIsMutable();
          runningApplications_.remove(index);
          onChanged();
        } else {
          runningApplicationsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getRunningApplicationsBuilder(
          int index) {
        return getRunningApplicationsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getRunningApplicationsOrBuilder(
          int index) {
        if (runningApplicationsBuilder_ == null) {
          return runningApplications_.get(index);  } else {
          return runningApplicationsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
           getRunningApplicationsOrBuilderList() {
        if (runningApplicationsBuilder_ != null) {
          return runningApplicationsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(runningApplications_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder addRunningApplicationsBuilder() {
        return getRunningApplicationsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder addRunningApplicationsBuilder(
          int index) {
        return getRunningApplicationsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto runningApplications = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder> 
           getRunningApplicationsBuilderList() {
        return getRunningApplicationsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getRunningApplicationsFieldBuilder() {
        if (runningApplicationsBuilder_ == null) {
          runningApplicationsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  runningApplications_,
                  ((bitField0_ & 0x00000020) != 0),
                  getParentForChildren(),
                  isClean());
          runningApplications_ = null;
        }
        return runningApplicationsBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto nodeLabels_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder> nodeLabelsBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public boolean hasNodeLabels() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getNodeLabels() {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
        } else {
          return nodeLabelsBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public Builder setNodeLabels(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeLabels_ = value;
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public Builder setNodeLabels(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = builderForValue.build();
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public Builder mergeNodeLabels(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0) &&
              nodeLabels_ != null &&
              nodeLabels_ != org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance()) {
            nodeLabels_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.newBuilder(nodeLabels_).mergeFrom(value).buildPartial();
          } else {
            nodeLabels_ = value;
          }
          onChanged();
        } else {
          nodeLabelsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public Builder clearNodeLabels() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = null;
          onChanged();
        } else {
          nodeLabelsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder getNodeLabelsBuilder() {
        bitField0_ |= 0x00000040;
        onChanged();
        return getNodeLabelsFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder getNodeLabelsOrBuilder() {
        if (nodeLabelsBuilder_ != null) {
          return nodeLabelsBuilder_.getMessageOrBuilder();
        } else {
          return nodeLabels_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 8;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder> 
          getNodeLabelsFieldBuilder() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabelsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder>(
                  getNodeLabels(),
                  getParentForChildren(),
                  isClean());
          nodeLabels_ = null;
        }
        return nodeLabelsBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto physicalResource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> physicalResourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public boolean hasPhysicalResource() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getPhysicalResource() {
        if (physicalResourceBuilder_ == null) {
          return physicalResource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : physicalResource_;
        } else {
          return physicalResourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public Builder setPhysicalResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (physicalResourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          physicalResource_ = value;
          onChanged();
        } else {
          physicalResourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public Builder setPhysicalResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (physicalResourceBuilder_ == null) {
          physicalResource_ = builderForValue.build();
          onChanged();
        } else {
          physicalResourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public Builder mergePhysicalResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (physicalResourceBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              physicalResource_ != null &&
              physicalResource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            physicalResource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(physicalResource_).mergeFrom(value).buildPartial();
          } else {
            physicalResource_ = value;
          }
          onChanged();
        } else {
          physicalResourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public Builder clearPhysicalResource() {
        if (physicalResourceBuilder_ == null) {
          physicalResource_ = null;
          onChanged();
        } else {
          physicalResourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getPhysicalResourceBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getPhysicalResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getPhysicalResourceOrBuilder() {
        if (physicalResourceBuilder_ != null) {
          return physicalResourceBuilder_.getMessageOrBuilder();
        } else {
          return physicalResource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : physicalResource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto physicalResource = 9;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getPhysicalResourceFieldBuilder() {
        if (physicalResourceBuilder_ == null) {
          physicalResourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getPhysicalResource(),
                  getParentForChildren(),
                  isClean());
          physicalResource_ = null;
        }
        return physicalResourceBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> logAggregationReportsForApps_ =
        java.util.Collections.emptyList();
      private void ensureLogAggregationReportsForAppsIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          logAggregationReportsForApps_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto>(logAggregationReportsForApps_);
          bitField0_ |= 0x00000100;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> logAggregationReportsForAppsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> getLogAggregationReportsForAppsList() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
        } else {
          return logAggregationReportsForAppsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public int getLogAggregationReportsForAppsCount() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return logAggregationReportsForApps_.size();
        } else {
          return logAggregationReportsForAppsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getLogAggregationReportsForApps(int index) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return logAggregationReportsForApps_.get(index);
        } else {
          return logAggregationReportsForAppsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder setLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto value) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.set(index, value);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder setLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder builderForValue) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.set(index, builderForValue.build());
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder addLogAggregationReportsForApps(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto value) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(value);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder addLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto value) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(index, value);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder addLogAggregationReportsForApps(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder builderForValue) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(builderForValue.build());
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder addLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder builderForValue) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(index, builderForValue.build());
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder addAllLogAggregationReportsForApps(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> values) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, logAggregationReportsForApps_);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder clearLogAggregationReportsForApps() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          logAggregationReportsForApps_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public Builder removeLogAggregationReportsForApps(int index) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.remove(index);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder getLogAggregationReportsForAppsBuilder(
          int index) {
        return getLogAggregationReportsForAppsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder getLogAggregationReportsForAppsOrBuilder(
          int index) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return logAggregationReportsForApps_.get(index);  } else {
          return logAggregationReportsForAppsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
           getLogAggregationReportsForAppsOrBuilderList() {
        if (logAggregationReportsForAppsBuilder_ != null) {
          return logAggregationReportsForAppsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder addLogAggregationReportsForAppsBuilder() {
        return getLogAggregationReportsForAppsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder addLogAggregationReportsForAppsBuilder(
          int index) {
        return getLogAggregationReportsForAppsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder> 
           getLogAggregationReportsForAppsBuilderList() {
        return getLogAggregationReportsForAppsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
          getLogAggregationReportsForAppsFieldBuilder() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          logAggregationReportsForAppsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder>(
                  logAggregationReportsForApps_,
                  ((bitField0_ & 0x00000100) != 0),
                  getParentForChildren(),
                  isClean());
          logAggregationReportsForApps_ = null;
        }
        return logAggregationReportsForAppsBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto nodeAttributes_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder> nodeAttributesBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public boolean hasNodeAttributes() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getNodeAttributes() {
        if (nodeAttributesBuilder_ == null) {
          return nodeAttributes_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
        } else {
          return nodeAttributesBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public Builder setNodeAttributes(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeAttributes_ = value;
          onChanged();
        } else {
          nodeAttributesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public Builder setNodeAttributes(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder builderForValue) {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = builderForValue.build();
          onChanged();
        } else {
          nodeAttributesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public Builder mergeNodeAttributes(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (((bitField0_ & 0x00000200) != 0) &&
              nodeAttributes_ != null &&
              nodeAttributes_ != org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance()) {
            nodeAttributes_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.newBuilder(nodeAttributes_).mergeFrom(value).buildPartial();
          } else {
            nodeAttributes_ = value;
          }
          onChanged();
        } else {
          nodeAttributesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public Builder clearNodeAttributes() {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = null;
          onChanged();
        } else {
          nodeAttributesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder getNodeAttributesBuilder() {
        bitField0_ |= 0x00000200;
        onChanged();
        return getNodeAttributesFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder getNodeAttributesOrBuilder() {
        if (nodeAttributesBuilder_ != null) {
          return nodeAttributesBuilder_.getMessageOrBuilder();
        } else {
          return nodeAttributes_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 11;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder> 
          getNodeAttributesFieldBuilder() {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributesBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder>(
                  getNodeAttributes(),
                  getParentForChildren(),
                  isClean());
          nodeAttributes_ = null;
        }
        return nodeAttributesBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto nodeStatus_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder> nodeStatusBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public boolean hasNodeStatus() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto getNodeStatus() {
        if (nodeStatusBuilder_ == null) {
          return nodeStatus_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
        } else {
          return nodeStatusBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public Builder setNodeStatus(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto value) {
        if (nodeStatusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeStatus_ = value;
          onChanged();
        } else {
          nodeStatusBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public Builder setNodeStatus(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder builderForValue) {
        if (nodeStatusBuilder_ == null) {
          nodeStatus_ = builderForValue.build();
          onChanged();
        } else {
          nodeStatusBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public Builder mergeNodeStatus(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto value) {
        if (nodeStatusBuilder_ == null) {
          if (((bitField0_ & 0x00000400) != 0) &&
              nodeStatus_ != null &&
              nodeStatus_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance()) {
            nodeStatus_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.newBuilder(nodeStatus_).mergeFrom(value).buildPartial();
          } else {
            nodeStatus_ = value;
          }
          onChanged();
        } else {
          nodeStatusBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public Builder clearNodeStatus() {
        if (nodeStatusBuilder_ == null) {
          nodeStatus_ = null;
          onChanged();
        } else {
          nodeStatusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder getNodeStatusBuilder() {
        bitField0_ |= 0x00000400;
        onChanged();
        return getNodeStatusFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder getNodeStatusOrBuilder() {
        if (nodeStatusBuilder_ != null) {
          return nodeStatusBuilder_.getMessageOrBuilder();
        } else {
          return nodeStatus_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto nodeStatus = 12;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder> 
          getNodeStatusFieldBuilder() {
        if (nodeStatusBuilder_ == null) {
          nodeStatusBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder>(
                  getNodeStatus(),
                  getParentForChildren(),
                  isClean());
          nodeStatus_ = null;
        }
        return nodeStatusBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterNodeManagerRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterNodeManagerRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<RegisterNodeManagerRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<RegisterNodeManagerRequestProto>() {
      @java.lang.Override
      public RegisterNodeManagerRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new RegisterNodeManagerRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<RegisterNodeManagerRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<RegisterNodeManagerRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterNodeManagerResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.RegisterNodeManagerResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
     */
    boolean hasContainerTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getContainerTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getContainerTokenMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
     */
    boolean hasNmTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNmTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNmTokenMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
     */
    boolean hasNodeAction();
    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto getNodeAction();

    /**
     * <code>optional int64 rm_identifier = 4;</code>
     */
    boolean hasRmIdentifier();
    /**
     * <code>optional int64 rm_identifier = 4;</code>
     */
    long getRmIdentifier();

    /**
     * <code>optional string diagnostics_message = 5;</code>
     */
    boolean hasDiagnosticsMessage();
    /**
     * <code>optional string diagnostics_message = 5;</code>
     */
    java.lang.String getDiagnosticsMessage();
    /**
     * <code>optional string diagnostics_message = 5;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsMessageBytes();

    /**
     * <code>optional string rm_version = 6;</code>
     */
    boolean hasRmVersion();
    /**
     * <code>optional string rm_version = 6;</code>
     */
    java.lang.String getRmVersion();
    /**
     * <code>optional string rm_version = 6;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getRmVersionBytes();

    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
     */
    boolean hasAreNodeLabelsAcceptedByRM();
    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
     */
    boolean getAreNodeLabelsAcceptedByRM();

    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
     */
    boolean hasResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder();

    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
     */
    boolean hasAreNodeAttributesAcceptedByRM();
    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
     */
    boolean getAreNodeAttributesAcceptedByRM();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterNodeManagerResponseProto}
   */
  public  static final class RegisterNodeManagerResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.RegisterNodeManagerResponseProto)
      RegisterNodeManagerResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterNodeManagerResponseProto.newBuilder() to construct.
    private RegisterNodeManagerResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterNodeManagerResponseProto() {
      nodeAction_ = 0;
      diagnosticsMessage_ = "";
      rmVersion_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterNodeManagerResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = containerTokenMasterKey_.toBuilder();
              }
              containerTokenMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerTokenMasterKey_);
                containerTokenMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = nmTokenMasterKey_.toBuilder();
              }
              nmTokenMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nmTokenMasterKey_);
                nmTokenMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto value = org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000004;
                nodeAction_ = rawValue;
              }
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              rmIdentifier_ = input.readInt64();
              break;
            }
            case 42: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000010;
              diagnosticsMessage_ = bs;
              break;
            }
            case 50: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000020;
              rmVersion_ = bs;
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              areNodeLabelsAcceptedByRM_ = input.readBool();
              break;
            }
            case 66: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) != 0)) {
                subBuilder = resource_.toBuilder();
              }
              resource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(resource_);
                resource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              areNodeAttributesAcceptedByRM_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int CONTAINER_TOKEN_MASTER_KEY_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto containerTokenMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
     */
    public boolean hasContainerTokenMasterKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getContainerTokenMasterKey() {
      return containerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getContainerTokenMasterKeyOrBuilder() {
      return containerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
    }

    public static final int NM_TOKEN_MASTER_KEY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto nmTokenMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
     */
    public boolean hasNmTokenMasterKey() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNmTokenMasterKey() {
      return nmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNmTokenMasterKeyOrBuilder() {
      return nmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
    }

    public static final int NODEACTION_FIELD_NUMBER = 3;
    private int nodeAction_;
    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
     */
    public boolean hasNodeAction() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto getNodeAction() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto result = org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.valueOf(nodeAction_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.NORMAL : result;
    }

    public static final int RM_IDENTIFIER_FIELD_NUMBER = 4;
    private long rmIdentifier_;
    /**
     * <code>optional int64 rm_identifier = 4;</code>
     */
    public boolean hasRmIdentifier() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 rm_identifier = 4;</code>
     */
    public long getRmIdentifier() {
      return rmIdentifier_;
    }

    public static final int DIAGNOSTICS_MESSAGE_FIELD_NUMBER = 5;
    private volatile java.lang.Object diagnosticsMessage_;
    /**
     * <code>optional string diagnostics_message = 5;</code>
     */
    public boolean hasDiagnosticsMessage() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional string diagnostics_message = 5;</code>
     */
    public java.lang.String getDiagnosticsMessage() {
      java.lang.Object ref = diagnosticsMessage_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnosticsMessage_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics_message = 5;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsMessageBytes() {
      java.lang.Object ref = diagnosticsMessage_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnosticsMessage_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int RM_VERSION_FIELD_NUMBER = 6;
    private volatile java.lang.Object rmVersion_;
    /**
     * <code>optional string rm_version = 6;</code>
     */
    public boolean hasRmVersion() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional string rm_version = 6;</code>
     */
    public java.lang.String getRmVersion() {
      java.lang.Object ref = rmVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          rmVersion_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string rm_version = 6;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getRmVersionBytes() {
      java.lang.Object ref = rmVersion_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        rmVersion_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int ARENODELABELSACCEPTEDBYRM_FIELD_NUMBER = 7;
    private boolean areNodeLabelsAcceptedByRM_;
    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
     */
    public boolean hasAreNodeLabelsAcceptedByRM() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
     */
    public boolean getAreNodeLabelsAcceptedByRM() {
      return areNodeLabelsAcceptedByRM_;
    }

    public static final int RESOURCE_FIELD_NUMBER = 8;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
     */
    public boolean hasResource() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }

    public static final int ARENODEATTRIBUTESACCEPTEDBYRM_FIELD_NUMBER = 9;
    private boolean areNodeAttributesAcceptedByRM_;
    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
     */
    public boolean hasAreNodeAttributesAcceptedByRM() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
     */
    public boolean getAreNodeAttributesAcceptedByRM() {
      return areNodeAttributesAcceptedByRM_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasResource()) {
        if (!getResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getContainerTokenMasterKey());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getNmTokenMasterKey());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeEnum(3, nodeAction_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, rmIdentifier_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 5, diagnosticsMessage_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 6, rmVersion_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeBool(7, areNodeLabelsAcceptedByRM_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeMessage(8, getResource());
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeBool(9, areNodeAttributesAcceptedByRM_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getContainerTokenMasterKey());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getNmTokenMasterKey());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(3, nodeAction_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(4, rmIdentifier_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(5, diagnosticsMessage_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(6, rmVersion_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(7, areNodeLabelsAcceptedByRM_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(8, getResource());
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(9, areNodeAttributesAcceptedByRM_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto) obj;

      if (hasContainerTokenMasterKey() != other.hasContainerTokenMasterKey()) return false;
      if (hasContainerTokenMasterKey()) {
        if (!getContainerTokenMasterKey()
            .equals(other.getContainerTokenMasterKey())) return false;
      }
      if (hasNmTokenMasterKey() != other.hasNmTokenMasterKey()) return false;
      if (hasNmTokenMasterKey()) {
        if (!getNmTokenMasterKey()
            .equals(other.getNmTokenMasterKey())) return false;
      }
      if (hasNodeAction() != other.hasNodeAction()) return false;
      if (hasNodeAction()) {
        if (nodeAction_ != other.nodeAction_) return false;
      }
      if (hasRmIdentifier() != other.hasRmIdentifier()) return false;
      if (hasRmIdentifier()) {
        if (getRmIdentifier()
            != other.getRmIdentifier()) return false;
      }
      if (hasDiagnosticsMessage() != other.hasDiagnosticsMessage()) return false;
      if (hasDiagnosticsMessage()) {
        if (!getDiagnosticsMessage()
            .equals(other.getDiagnosticsMessage())) return false;
      }
      if (hasRmVersion() != other.hasRmVersion()) return false;
      if (hasRmVersion()) {
        if (!getRmVersion()
            .equals(other.getRmVersion())) return false;
      }
      if (hasAreNodeLabelsAcceptedByRM() != other.hasAreNodeLabelsAcceptedByRM()) return false;
      if (hasAreNodeLabelsAcceptedByRM()) {
        if (getAreNodeLabelsAcceptedByRM()
            != other.getAreNodeLabelsAcceptedByRM()) return false;
      }
      if (hasResource() != other.hasResource()) return false;
      if (hasResource()) {
        if (!getResource()
            .equals(other.getResource())) return false;
      }
      if (hasAreNodeAttributesAcceptedByRM() != other.hasAreNodeAttributesAcceptedByRM()) return false;
      if (hasAreNodeAttributesAcceptedByRM()) {
        if (getAreNodeAttributesAcceptedByRM()
            != other.getAreNodeAttributesAcceptedByRM()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasContainerTokenMasterKey()) {
        hash = (37 * hash) + CONTAINER_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getContainerTokenMasterKey().hashCode();
      }
      if (hasNmTokenMasterKey()) {
        hash = (37 * hash) + NM_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getNmTokenMasterKey().hashCode();
      }
      if (hasNodeAction()) {
        hash = (37 * hash) + NODEACTION_FIELD_NUMBER;
        hash = (53 * hash) + nodeAction_;
      }
      if (hasRmIdentifier()) {
        hash = (37 * hash) + RM_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getRmIdentifier());
      }
      if (hasDiagnosticsMessage()) {
        hash = (37 * hash) + DIAGNOSTICS_MESSAGE_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnosticsMessage().hashCode();
      }
      if (hasRmVersion()) {
        hash = (37 * hash) + RM_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getRmVersion().hashCode();
      }
      if (hasAreNodeLabelsAcceptedByRM()) {
        hash = (37 * hash) + ARENODELABELSACCEPTEDBYRM_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getAreNodeLabelsAcceptedByRM());
      }
      if (hasResource()) {
        hash = (37 * hash) + RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getResource().hashCode();
      }
      if (hasAreNodeAttributesAcceptedByRM()) {
        hash = (37 * hash) + ARENODEATTRIBUTESACCEPTEDBYRM_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getAreNodeAttributesAcceptedByRM());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterNodeManagerResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.RegisterNodeManagerResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getContainerTokenMasterKeyFieldBuilder();
          getNmTokenMasterKeyFieldBuilder();
          getResourceFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKey_ = null;
        } else {
          containerTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKey_ = null;
        } else {
          nmTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        nodeAction_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        rmIdentifier_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        diagnosticsMessage_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        rmVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        areNodeLabelsAcceptedByRM_ = false;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (resourceBuilder_ == null) {
          resource_ = null;
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        areNodeAttributesAcceptedByRM_ = false;
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (containerTokenMasterKeyBuilder_ == null) {
            result.containerTokenMasterKey_ = containerTokenMasterKey_;
          } else {
            result.containerTokenMasterKey_ = containerTokenMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (nmTokenMasterKeyBuilder_ == null) {
            result.nmTokenMasterKey_ = nmTokenMasterKey_;
          } else {
            result.nmTokenMasterKey_ = nmTokenMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.nodeAction_ = nodeAction_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.rmIdentifier_ = rmIdentifier_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.diagnosticsMessage_ = diagnosticsMessage_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.rmVersion_ = rmVersion_;
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.areNodeLabelsAcceptedByRM_ = areNodeLabelsAcceptedByRM_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (resourceBuilder_ == null) {
            result.resource_ = resource_;
          } else {
            result.resource_ = resourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000080;
        }
        if (((from_bitField0_ & 0x00000100) != 0)) {
          result.areNodeAttributesAcceptedByRM_ = areNodeAttributesAcceptedByRM_;
          to_bitField0_ |= 0x00000100;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto.getDefaultInstance()) return this;
        if (other.hasContainerTokenMasterKey()) {
          mergeContainerTokenMasterKey(other.getContainerTokenMasterKey());
        }
        if (other.hasNmTokenMasterKey()) {
          mergeNmTokenMasterKey(other.getNmTokenMasterKey());
        }
        if (other.hasNodeAction()) {
          setNodeAction(other.getNodeAction());
        }
        if (other.hasRmIdentifier()) {
          setRmIdentifier(other.getRmIdentifier());
        }
        if (other.hasDiagnosticsMessage()) {
          bitField0_ |= 0x00000010;
          diagnosticsMessage_ = other.diagnosticsMessage_;
          onChanged();
        }
        if (other.hasRmVersion()) {
          bitField0_ |= 0x00000020;
          rmVersion_ = other.rmVersion_;
          onChanged();
        }
        if (other.hasAreNodeLabelsAcceptedByRM()) {
          setAreNodeLabelsAcceptedByRM(other.getAreNodeLabelsAcceptedByRM());
        }
        if (other.hasResource()) {
          mergeResource(other.getResource());
        }
        if (other.hasAreNodeAttributesAcceptedByRM()) {
          setAreNodeAttributesAcceptedByRM(other.getAreNodeAttributesAcceptedByRM());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasResource()) {
          if (!getResource().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto containerTokenMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> containerTokenMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public boolean hasContainerTokenMasterKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getContainerTokenMasterKey() {
        if (containerTokenMasterKeyBuilder_ == null) {
          return containerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
        } else {
          return containerTokenMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public Builder setContainerTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (containerTokenMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerTokenMasterKey_ = value;
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public Builder setContainerTokenMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public Builder mergeContainerTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (containerTokenMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              containerTokenMasterKey_ != null &&
              containerTokenMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            containerTokenMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(containerTokenMasterKey_).mergeFrom(value).buildPartial();
          } else {
            containerTokenMasterKey_ = value;
          }
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public Builder clearContainerTokenMasterKey() {
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKey_ = null;
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getContainerTokenMasterKeyBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerTokenMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getContainerTokenMasterKeyOrBuilder() {
        if (containerTokenMasterKeyBuilder_ != null) {
          return containerTokenMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return containerTokenMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getContainerTokenMasterKeyFieldBuilder() {
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getContainerTokenMasterKey(),
                  getParentForChildren(),
                  isClean());
          containerTokenMasterKey_ = null;
        }
        return containerTokenMasterKeyBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto nmTokenMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> nmTokenMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public boolean hasNmTokenMasterKey() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNmTokenMasterKey() {
        if (nmTokenMasterKeyBuilder_ == null) {
          return nmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
        } else {
          return nmTokenMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public Builder setNmTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (nmTokenMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nmTokenMasterKey_ = value;
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public Builder setNmTokenMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public Builder mergeNmTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (nmTokenMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              nmTokenMasterKey_ != null &&
              nmTokenMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            nmTokenMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(nmTokenMasterKey_).mergeFrom(value).buildPartial();
          } else {
            nmTokenMasterKey_ = value;
          }
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public Builder clearNmTokenMasterKey() {
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKey_ = null;
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getNmTokenMasterKeyBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getNmTokenMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNmTokenMasterKeyOrBuilder() {
        if (nmTokenMasterKeyBuilder_ != null) {
          return nmTokenMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return nmTokenMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getNmTokenMasterKeyFieldBuilder() {
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getNmTokenMasterKey(),
                  getParentForChildren(),
                  isClean());
          nmTokenMasterKey_ = null;
        }
        return nmTokenMasterKeyBuilder_;
      }

      private int nodeAction_ = 0;
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
       */
      public boolean hasNodeAction() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto getNodeAction() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto result = org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.valueOf(nodeAction_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.NORMAL : result;
      }
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
       */
      public Builder setNodeAction(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000004;
        nodeAction_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 3;</code>
       */
      public Builder clearNodeAction() {
        bitField0_ = (bitField0_ & ~0x00000004);
        nodeAction_ = 0;
        onChanged();
        return this;
      }

      private long rmIdentifier_ ;
      /**
       * <code>optional int64 rm_identifier = 4;</code>
       */
      public boolean hasRmIdentifier() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 rm_identifier = 4;</code>
       */
      public long getRmIdentifier() {
        return rmIdentifier_;
      }
      /**
       * <code>optional int64 rm_identifier = 4;</code>
       */
      public Builder setRmIdentifier(long value) {
        bitField0_ |= 0x00000008;
        rmIdentifier_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 rm_identifier = 4;</code>
       */
      public Builder clearRmIdentifier() {
        bitField0_ = (bitField0_ & ~0x00000008);
        rmIdentifier_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object diagnosticsMessage_ = "";
      /**
       * <code>optional string diagnostics_message = 5;</code>
       */
      public boolean hasDiagnosticsMessage() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional string diagnostics_message = 5;</code>
       */
      public java.lang.String getDiagnosticsMessage() {
        java.lang.Object ref = diagnosticsMessage_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnosticsMessage_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics_message = 5;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsMessageBytes() {
        java.lang.Object ref = diagnosticsMessage_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnosticsMessage_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics_message = 5;</code>
       */
      public Builder setDiagnosticsMessage(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        diagnosticsMessage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics_message = 5;</code>
       */
      public Builder clearDiagnosticsMessage() {
        bitField0_ = (bitField0_ & ~0x00000010);
        diagnosticsMessage_ = getDefaultInstance().getDiagnosticsMessage();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics_message = 5;</code>
       */
      public Builder setDiagnosticsMessageBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        diagnosticsMessage_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object rmVersion_ = "";
      /**
       * <code>optional string rm_version = 6;</code>
       */
      public boolean hasRmVersion() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional string rm_version = 6;</code>
       */
      public java.lang.String getRmVersion() {
        java.lang.Object ref = rmVersion_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            rmVersion_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string rm_version = 6;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getRmVersionBytes() {
        java.lang.Object ref = rmVersion_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          rmVersion_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string rm_version = 6;</code>
       */
      public Builder setRmVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        rmVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string rm_version = 6;</code>
       */
      public Builder clearRmVersion() {
        bitField0_ = (bitField0_ & ~0x00000020);
        rmVersion_ = getDefaultInstance().getRmVersion();
        onChanged();
        return this;
      }
      /**
       * <code>optional string rm_version = 6;</code>
       */
      public Builder setRmVersionBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        rmVersion_ = value;
        onChanged();
        return this;
      }

      private boolean areNodeLabelsAcceptedByRM_ ;
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
       */
      public boolean hasAreNodeLabelsAcceptedByRM() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
       */
      public boolean getAreNodeLabelsAcceptedByRM() {
        return areNodeLabelsAcceptedByRM_;
      }
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
       */
      public Builder setAreNodeLabelsAcceptedByRM(boolean value) {
        bitField0_ |= 0x00000040;
        areNodeLabelsAcceptedByRM_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 7 [default = false];</code>
       */
      public Builder clearAreNodeLabelsAcceptedByRM() {
        bitField0_ = (bitField0_ & ~0x00000040);
        areNodeLabelsAcceptedByRM_ = false;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> resourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public boolean hasResource() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
        if (resourceBuilder_ == null) {
          return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        } else {
          return resourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public Builder setResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          resource_ = value;
          onChanged();
        } else {
          resourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public Builder setResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (resourceBuilder_ == null) {
          resource_ = builderForValue.build();
          onChanged();
        } else {
          resourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public Builder mergeResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              resource_ != null &&
              resource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            resource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(resource_).mergeFrom(value).buildPartial();
          } else {
            resource_ = value;
          }
          onChanged();
        } else {
          resourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public Builder clearResource() {
        if (resourceBuilder_ == null) {
          resource_ = null;
          onChanged();
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getResourceBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
        if (resourceBuilder_ != null) {
          return resourceBuilder_.getMessageOrBuilder();
        } else {
          return resource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 8;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getResourceFieldBuilder() {
        if (resourceBuilder_ == null) {
          resourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getResource(),
                  getParentForChildren(),
                  isClean());
          resource_ = null;
        }
        return resourceBuilder_;
      }

      private boolean areNodeAttributesAcceptedByRM_ ;
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
       */
      public boolean hasAreNodeAttributesAcceptedByRM() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
       */
      public boolean getAreNodeAttributesAcceptedByRM() {
        return areNodeAttributesAcceptedByRM_;
      }
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
       */
      public Builder setAreNodeAttributesAcceptedByRM(boolean value) {
        bitField0_ |= 0x00000100;
        areNodeAttributesAcceptedByRM_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 9 [default = false];</code>
       */
      public Builder clearAreNodeAttributesAcceptedByRM() {
        bitField0_ = (bitField0_ & ~0x00000100);
        areNodeAttributesAcceptedByRM_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterNodeManagerResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterNodeManagerResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<RegisterNodeManagerResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<RegisterNodeManagerResponseProto>() {
      @java.lang.Override
      public RegisterNodeManagerResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new RegisterNodeManagerResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<RegisterNodeManagerResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<RegisterNodeManagerResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.RegisterNodeManagerResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface UnRegisterNodeManagerRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.UnRegisterNodeManagerRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    boolean hasNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UnRegisterNodeManagerRequestProto}
   */
  public  static final class UnRegisterNodeManagerRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.UnRegisterNodeManagerRequestProto)
      UnRegisterNodeManagerRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use UnRegisterNodeManagerRequestProto.newBuilder() to construct.
    private UnRegisterNodeManagerRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private UnRegisterNodeManagerRequestProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private UnRegisterNodeManagerRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = nodeId_.toBuilder();
              }
              nodeId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeId_);
                nodeId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int NODE_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public boolean hasNodeId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
      return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
      return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getNodeId());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getNodeId());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto) obj;

      if (hasNodeId() != other.hasNodeId()) return false;
      if (hasNodeId()) {
        if (!getNodeId()
            .equals(other.getNodeId())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasNodeId()) {
        hash = (37 * hash) + NODE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getNodeId().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UnRegisterNodeManagerRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.UnRegisterNodeManagerRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (nodeIdBuilder_ == null) {
          nodeId_ = null;
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (nodeIdBuilder_ == null) {
            result.nodeId_ = nodeId_;
          } else {
            result.nodeId_ = nodeIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto.getDefaultInstance()) return this;
        if (other.hasNodeId()) {
          mergeNodeId(other.getNodeId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> nodeIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public boolean hasNodeId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
        if (nodeIdBuilder_ == null) {
          return nodeId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
        } else {
          return nodeIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder setNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeId_ = value;
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder setNodeId(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder builderForValue) {
        if (nodeIdBuilder_ == null) {
          nodeId_ = builderForValue.build();
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder mergeNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              nodeId_ != null &&
              nodeId_ != org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance()) {
            nodeId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.newBuilder(nodeId_).mergeFrom(value).buildPartial();
          } else {
            nodeId_ = value;
          }
          onChanged();
        } else {
          nodeIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public Builder clearNodeId() {
        if (nodeIdBuilder_ == null) {
          nodeId_ = null;
          onChanged();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder getNodeIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNodeIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
        if (nodeIdBuilder_ != null) {
          return nodeIdBuilder_.getMessageOrBuilder();
        } else {
          return nodeId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance() : nodeId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto node_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> 
          getNodeIdFieldBuilder() {
        if (nodeIdBuilder_ == null) {
          nodeIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder>(
                  getNodeId(),
                  getParentForChildren(),
                  isClean());
          nodeId_ = null;
        }
        return nodeIdBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UnRegisterNodeManagerRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UnRegisterNodeManagerRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<UnRegisterNodeManagerRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<UnRegisterNodeManagerRequestProto>() {
      @java.lang.Override
      public UnRegisterNodeManagerRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new UnRegisterNodeManagerRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<UnRegisterNodeManagerRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<UnRegisterNodeManagerRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface UnRegisterNodeManagerResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.UnRegisterNodeManagerResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.UnRegisterNodeManagerResponseProto}
   */
  public  static final class UnRegisterNodeManagerResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.UnRegisterNodeManagerResponseProto)
      UnRegisterNodeManagerResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use UnRegisterNodeManagerResponseProto.newBuilder() to construct.
    private UnRegisterNodeManagerResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private UnRegisterNodeManagerResponseProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private UnRegisterNodeManagerResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UnRegisterNodeManagerResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.UnRegisterNodeManagerResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UnRegisterNodeManagerResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UnRegisterNodeManagerResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<UnRegisterNodeManagerResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<UnRegisterNodeManagerResponseProto>() {
      @java.lang.Override
      public UnRegisterNodeManagerResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new UnRegisterNodeManagerResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<UnRegisterNodeManagerResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<UnRegisterNodeManagerResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.UnRegisterNodeManagerResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NodeHeartbeatRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.NodeHeartbeatRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
     */
    boolean hasNodeStatus();
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto getNodeStatus();
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder getNodeStatusOrBuilder();

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
     */
    boolean hasLastKnownContainerTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getLastKnownContainerTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getLastKnownContainerTokenMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
     */
    boolean hasLastKnownNmTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getLastKnownNmTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getLastKnownNmTokenMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
     */
    boolean hasNodeLabels();
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getNodeLabels();
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder getNodeLabelsOrBuilder();

    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> 
        getLogAggregationReportsForAppsList();
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getLogAggregationReportsForApps(int index);
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    int getLogAggregationReportsForAppsCount();
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
        getLogAggregationReportsForAppsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder getLogAggregationReportsForAppsOrBuilder(
        int index);

    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> 
        getRegisteringCollectorsList();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getRegisteringCollectors(int index);
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    int getRegisteringCollectorsCount();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
        getRegisteringCollectorsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getRegisteringCollectorsOrBuilder(
        int index);

    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
     */
    boolean hasNodeAttributes();
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getNodeAttributes();
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder getNodeAttributesOrBuilder();

    /**
     * <code>optional int64 tokenSequenceNo = 8;</code>
     */
    boolean hasTokenSequenceNo();
    /**
     * <code>optional int64 tokenSequenceNo = 8;</code>
     */
    long getTokenSequenceNo();
  }
  /**
   * Protobuf type {@code hadoop.yarn.NodeHeartbeatRequestProto}
   */
  public  static final class NodeHeartbeatRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.NodeHeartbeatRequestProto)
      NodeHeartbeatRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NodeHeartbeatRequestProto.newBuilder() to construct.
    private NodeHeartbeatRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NodeHeartbeatRequestProto() {
      logAggregationReportsForApps_ = java.util.Collections.emptyList();
      registeringCollectors_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NodeHeartbeatRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = nodeStatus_.toBuilder();
              }
              nodeStatus_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeStatus_);
                nodeStatus_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = lastKnownContainerTokenMasterKey_.toBuilder();
              }
              lastKnownContainerTokenMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(lastKnownContainerTokenMasterKey_);
                lastKnownContainerTokenMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = lastKnownNmTokenMasterKey_.toBuilder();
              }
              lastKnownNmTokenMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(lastKnownNmTokenMasterKey_);
                lastKnownNmTokenMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000008) != 0)) {
                subBuilder = nodeLabels_.toBuilder();
              }
              nodeLabels_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeLabels_);
                nodeLabels_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000008;
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                logAggregationReportsForApps_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto>();
                mutable_bitField0_ |= 0x00000010;
              }
              logAggregationReportsForApps_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.PARSER, extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                registeringCollectors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              registeringCollectors_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.PARSER, extensionRegistry));
              break;
            }
            case 58: {
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) != 0)) {
                subBuilder = nodeAttributes_.toBuilder();
              }
              nodeAttributes_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeAttributes_);
                nodeAttributes_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 64: {
              bitField0_ |= 0x00000020;
              tokenSequenceNo_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          logAggregationReportsForApps_ = java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          registeringCollectors_ = java.util.Collections.unmodifiableList(registeringCollectors_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int NODE_STATUS_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto nodeStatus_;
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
     */
    public boolean hasNodeStatus() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto getNodeStatus() {
      return nodeStatus_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder getNodeStatusOrBuilder() {
      return nodeStatus_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
    }

    public static final int LAST_KNOWN_CONTAINER_TOKEN_MASTER_KEY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto lastKnownContainerTokenMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
     */
    public boolean hasLastKnownContainerTokenMasterKey() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getLastKnownContainerTokenMasterKey() {
      return lastKnownContainerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownContainerTokenMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getLastKnownContainerTokenMasterKeyOrBuilder() {
      return lastKnownContainerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownContainerTokenMasterKey_;
    }

    public static final int LAST_KNOWN_NM_TOKEN_MASTER_KEY_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto lastKnownNmTokenMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
     */
    public boolean hasLastKnownNmTokenMasterKey() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getLastKnownNmTokenMasterKey() {
      return lastKnownNmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownNmTokenMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getLastKnownNmTokenMasterKeyOrBuilder() {
      return lastKnownNmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownNmTokenMasterKey_;
    }

    public static final int NODELABELS_FIELD_NUMBER = 4;
    private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto nodeLabels_;
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
     */
    public boolean hasNodeLabels() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getNodeLabels() {
      return nodeLabels_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder getNodeLabelsOrBuilder() {
      return nodeLabels_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
    }

    public static final int LOG_AGGREGATION_REPORTS_FOR_APPS_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> logAggregationReportsForApps_;
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> getLogAggregationReportsForAppsList() {
      return logAggregationReportsForApps_;
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
        getLogAggregationReportsForAppsOrBuilderList() {
      return logAggregationReportsForApps_;
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    public int getLogAggregationReportsForAppsCount() {
      return logAggregationReportsForApps_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getLogAggregationReportsForApps(int index) {
      return logAggregationReportsForApps_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder getLogAggregationReportsForAppsOrBuilder(
        int index) {
      return logAggregationReportsForApps_.get(index);
    }

    public static final int REGISTERING_COLLECTORS_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> registeringCollectors_;
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> getRegisteringCollectorsList() {
      return registeringCollectors_;
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
        getRegisteringCollectorsOrBuilderList() {
      return registeringCollectors_;
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    public int getRegisteringCollectorsCount() {
      return registeringCollectors_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getRegisteringCollectors(int index) {
      return registeringCollectors_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getRegisteringCollectorsOrBuilder(
        int index) {
      return registeringCollectors_.get(index);
    }

    public static final int NODEATTRIBUTES_FIELD_NUMBER = 7;
    private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto nodeAttributes_;
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
     */
    public boolean hasNodeAttributes() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getNodeAttributes() {
      return nodeAttributes_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder getNodeAttributesOrBuilder() {
      return nodeAttributes_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
    }

    public static final int TOKENSEQUENCENO_FIELD_NUMBER = 8;
    private long tokenSequenceNo_;
    /**
     * <code>optional int64 tokenSequenceNo = 8;</code>
     */
    public boolean hasTokenSequenceNo() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional int64 tokenSequenceNo = 8;</code>
     */
    public long getTokenSequenceNo() {
      return tokenSequenceNo_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasNodeStatus()) {
        if (!getNodeStatus().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getRegisteringCollectorsCount(); i++) {
        if (!getRegisteringCollectors(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasNodeAttributes()) {
        if (!getNodeAttributes().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getNodeStatus());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getLastKnownContainerTokenMasterKey());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getLastKnownNmTokenMasterKey());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeMessage(4, getNodeLabels());
      }
      for (int i = 0; i < logAggregationReportsForApps_.size(); i++) {
        output.writeMessage(5, logAggregationReportsForApps_.get(i));
      }
      for (int i = 0; i < registeringCollectors_.size(); i++) {
        output.writeMessage(6, registeringCollectors_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeMessage(7, getNodeAttributes());
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeInt64(8, tokenSequenceNo_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getNodeStatus());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getLastKnownContainerTokenMasterKey());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(3, getLastKnownNmTokenMasterKey());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(4, getNodeLabels());
      }
      for (int i = 0; i < logAggregationReportsForApps_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(5, logAggregationReportsForApps_.get(i));
      }
      for (int i = 0; i < registeringCollectors_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(6, registeringCollectors_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(7, getNodeAttributes());
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(8, tokenSequenceNo_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto) obj;

      if (hasNodeStatus() != other.hasNodeStatus()) return false;
      if (hasNodeStatus()) {
        if (!getNodeStatus()
            .equals(other.getNodeStatus())) return false;
      }
      if (hasLastKnownContainerTokenMasterKey() != other.hasLastKnownContainerTokenMasterKey()) return false;
      if (hasLastKnownContainerTokenMasterKey()) {
        if (!getLastKnownContainerTokenMasterKey()
            .equals(other.getLastKnownContainerTokenMasterKey())) return false;
      }
      if (hasLastKnownNmTokenMasterKey() != other.hasLastKnownNmTokenMasterKey()) return false;
      if (hasLastKnownNmTokenMasterKey()) {
        if (!getLastKnownNmTokenMasterKey()
            .equals(other.getLastKnownNmTokenMasterKey())) return false;
      }
      if (hasNodeLabels() != other.hasNodeLabels()) return false;
      if (hasNodeLabels()) {
        if (!getNodeLabels()
            .equals(other.getNodeLabels())) return false;
      }
      if (!getLogAggregationReportsForAppsList()
          .equals(other.getLogAggregationReportsForAppsList())) return false;
      if (!getRegisteringCollectorsList()
          .equals(other.getRegisteringCollectorsList())) return false;
      if (hasNodeAttributes() != other.hasNodeAttributes()) return false;
      if (hasNodeAttributes()) {
        if (!getNodeAttributes()
            .equals(other.getNodeAttributes())) return false;
      }
      if (hasTokenSequenceNo() != other.hasTokenSequenceNo()) return false;
      if (hasTokenSequenceNo()) {
        if (getTokenSequenceNo()
            != other.getTokenSequenceNo()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasNodeStatus()) {
        hash = (37 * hash) + NODE_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeStatus().hashCode();
      }
      if (hasLastKnownContainerTokenMasterKey()) {
        hash = (37 * hash) + LAST_KNOWN_CONTAINER_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getLastKnownContainerTokenMasterKey().hashCode();
      }
      if (hasLastKnownNmTokenMasterKey()) {
        hash = (37 * hash) + LAST_KNOWN_NM_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getLastKnownNmTokenMasterKey().hashCode();
      }
      if (hasNodeLabels()) {
        hash = (37 * hash) + NODELABELS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeLabels().hashCode();
      }
      if (getLogAggregationReportsForAppsCount() > 0) {
        hash = (37 * hash) + LOG_AGGREGATION_REPORTS_FOR_APPS_FIELD_NUMBER;
        hash = (53 * hash) + getLogAggregationReportsForAppsList().hashCode();
      }
      if (getRegisteringCollectorsCount() > 0) {
        hash = (37 * hash) + REGISTERING_COLLECTORS_FIELD_NUMBER;
        hash = (53 * hash) + getRegisteringCollectorsList().hashCode();
      }
      if (hasNodeAttributes()) {
        hash = (37 * hash) + NODEATTRIBUTES_FIELD_NUMBER;
        hash = (53 * hash) + getNodeAttributes().hashCode();
      }
      if (hasTokenSequenceNo()) {
        hash = (37 * hash) + TOKENSEQUENCENO_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getTokenSequenceNo());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NodeHeartbeatRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.NodeHeartbeatRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getNodeStatusFieldBuilder();
          getLastKnownContainerTokenMasterKeyFieldBuilder();
          getLastKnownNmTokenMasterKeyFieldBuilder();
          getNodeLabelsFieldBuilder();
          getLogAggregationReportsForAppsFieldBuilder();
          getRegisteringCollectorsFieldBuilder();
          getNodeAttributesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (nodeStatusBuilder_ == null) {
          nodeStatus_ = null;
        } else {
          nodeStatusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          lastKnownContainerTokenMasterKey_ = null;
        } else {
          lastKnownContainerTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          lastKnownNmTokenMasterKey_ = null;
        } else {
          lastKnownNmTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = null;
        } else {
          nodeLabelsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        if (logAggregationReportsForAppsBuilder_ == null) {
          logAggregationReportsForApps_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          logAggregationReportsForAppsBuilder_.clear();
        }
        if (registeringCollectorsBuilder_ == null) {
          registeringCollectors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          registeringCollectorsBuilder_.clear();
        }
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = null;
        } else {
          nodeAttributesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        tokenSequenceNo_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (nodeStatusBuilder_ == null) {
            result.nodeStatus_ = nodeStatus_;
          } else {
            result.nodeStatus_ = nodeStatusBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
            result.lastKnownContainerTokenMasterKey_ = lastKnownContainerTokenMasterKey_;
          } else {
            result.lastKnownContainerTokenMasterKey_ = lastKnownContainerTokenMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (lastKnownNmTokenMasterKeyBuilder_ == null) {
            result.lastKnownNmTokenMasterKey_ = lastKnownNmTokenMasterKey_;
          } else {
            result.lastKnownNmTokenMasterKey_ = lastKnownNmTokenMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          if (nodeLabelsBuilder_ == null) {
            result.nodeLabels_ = nodeLabels_;
          } else {
            result.nodeLabels_ = nodeLabelsBuilder_.build();
          }
          to_bitField0_ |= 0x00000008;
        }
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            logAggregationReportsForApps_ = java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.logAggregationReportsForApps_ = logAggregationReportsForApps_;
        } else {
          result.logAggregationReportsForApps_ = logAggregationReportsForAppsBuilder_.build();
        }
        if (registeringCollectorsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0)) {
            registeringCollectors_ = java.util.Collections.unmodifiableList(registeringCollectors_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.registeringCollectors_ = registeringCollectors_;
        } else {
          result.registeringCollectors_ = registeringCollectorsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          if (nodeAttributesBuilder_ == null) {
            result.nodeAttributes_ = nodeAttributes_;
          } else {
            result.nodeAttributes_ = nodeAttributesBuilder_.build();
          }
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          result.tokenSequenceNo_ = tokenSequenceNo_;
          to_bitField0_ |= 0x00000020;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto.getDefaultInstance()) return this;
        if (other.hasNodeStatus()) {
          mergeNodeStatus(other.getNodeStatus());
        }
        if (other.hasLastKnownContainerTokenMasterKey()) {
          mergeLastKnownContainerTokenMasterKey(other.getLastKnownContainerTokenMasterKey());
        }
        if (other.hasLastKnownNmTokenMasterKey()) {
          mergeLastKnownNmTokenMasterKey(other.getLastKnownNmTokenMasterKey());
        }
        if (other.hasNodeLabels()) {
          mergeNodeLabels(other.getNodeLabels());
        }
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (!other.logAggregationReportsForApps_.isEmpty()) {
            if (logAggregationReportsForApps_.isEmpty()) {
              logAggregationReportsForApps_ = other.logAggregationReportsForApps_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureLogAggregationReportsForAppsIsMutable();
              logAggregationReportsForApps_.addAll(other.logAggregationReportsForApps_);
            }
            onChanged();
          }
        } else {
          if (!other.logAggregationReportsForApps_.isEmpty()) {
            if (logAggregationReportsForAppsBuilder_.isEmpty()) {
              logAggregationReportsForAppsBuilder_.dispose();
              logAggregationReportsForAppsBuilder_ = null;
              logAggregationReportsForApps_ = other.logAggregationReportsForApps_;
              bitField0_ = (bitField0_ & ~0x00000010);
              logAggregationReportsForAppsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getLogAggregationReportsForAppsFieldBuilder() : null;
            } else {
              logAggregationReportsForAppsBuilder_.addAllMessages(other.logAggregationReportsForApps_);
            }
          }
        }
        if (registeringCollectorsBuilder_ == null) {
          if (!other.registeringCollectors_.isEmpty()) {
            if (registeringCollectors_.isEmpty()) {
              registeringCollectors_ = other.registeringCollectors_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureRegisteringCollectorsIsMutable();
              registeringCollectors_.addAll(other.registeringCollectors_);
            }
            onChanged();
          }
        } else {
          if (!other.registeringCollectors_.isEmpty()) {
            if (registeringCollectorsBuilder_.isEmpty()) {
              registeringCollectorsBuilder_.dispose();
              registeringCollectorsBuilder_ = null;
              registeringCollectors_ = other.registeringCollectors_;
              bitField0_ = (bitField0_ & ~0x00000020);
              registeringCollectorsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getRegisteringCollectorsFieldBuilder() : null;
            } else {
              registeringCollectorsBuilder_.addAllMessages(other.registeringCollectors_);
            }
          }
        }
        if (other.hasNodeAttributes()) {
          mergeNodeAttributes(other.getNodeAttributes());
        }
        if (other.hasTokenSequenceNo()) {
          setTokenSequenceNo(other.getTokenSequenceNo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasNodeStatus()) {
          if (!getNodeStatus().isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getRegisteringCollectorsCount(); i++) {
          if (!getRegisteringCollectors(i).isInitialized()) {
            return false;
          }
        }
        if (hasNodeAttributes()) {
          if (!getNodeAttributes().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto nodeStatus_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder> nodeStatusBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public boolean hasNodeStatus() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto getNodeStatus() {
        if (nodeStatusBuilder_ == null) {
          return nodeStatus_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
        } else {
          return nodeStatusBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public Builder setNodeStatus(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto value) {
        if (nodeStatusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeStatus_ = value;
          onChanged();
        } else {
          nodeStatusBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public Builder setNodeStatus(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder builderForValue) {
        if (nodeStatusBuilder_ == null) {
          nodeStatus_ = builderForValue.build();
          onChanged();
        } else {
          nodeStatusBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public Builder mergeNodeStatus(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto value) {
        if (nodeStatusBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              nodeStatus_ != null &&
              nodeStatus_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance()) {
            nodeStatus_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.newBuilder(nodeStatus_).mergeFrom(value).buildPartial();
          } else {
            nodeStatus_ = value;
          }
          onChanged();
        } else {
          nodeStatusBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public Builder clearNodeStatus() {
        if (nodeStatusBuilder_ == null) {
          nodeStatus_ = null;
          onChanged();
        } else {
          nodeStatusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder getNodeStatusBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNodeStatusFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder getNodeStatusOrBuilder() {
        if (nodeStatusBuilder_ != null) {
          return nodeStatusBuilder_.getMessageOrBuilder();
        } else {
          return nodeStatus_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.getDefaultInstance() : nodeStatus_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeStatusProto node_status = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder> 
          getNodeStatusFieldBuilder() {
        if (nodeStatusBuilder_ == null) {
          nodeStatusBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProtoOrBuilder>(
                  getNodeStatus(),
                  getParentForChildren(),
                  isClean());
          nodeStatus_ = null;
        }
        return nodeStatusBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto lastKnownContainerTokenMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> lastKnownContainerTokenMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public boolean hasLastKnownContainerTokenMasterKey() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getLastKnownContainerTokenMasterKey() {
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          return lastKnownContainerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownContainerTokenMasterKey_;
        } else {
          return lastKnownContainerTokenMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public Builder setLastKnownContainerTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          lastKnownContainerTokenMasterKey_ = value;
          onChanged();
        } else {
          lastKnownContainerTokenMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public Builder setLastKnownContainerTokenMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          lastKnownContainerTokenMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          lastKnownContainerTokenMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public Builder mergeLastKnownContainerTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              lastKnownContainerTokenMasterKey_ != null &&
              lastKnownContainerTokenMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            lastKnownContainerTokenMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(lastKnownContainerTokenMasterKey_).mergeFrom(value).buildPartial();
          } else {
            lastKnownContainerTokenMasterKey_ = value;
          }
          onChanged();
        } else {
          lastKnownContainerTokenMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public Builder clearLastKnownContainerTokenMasterKey() {
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          lastKnownContainerTokenMasterKey_ = null;
          onChanged();
        } else {
          lastKnownContainerTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getLastKnownContainerTokenMasterKeyBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getLastKnownContainerTokenMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getLastKnownContainerTokenMasterKeyOrBuilder() {
        if (lastKnownContainerTokenMasterKeyBuilder_ != null) {
          return lastKnownContainerTokenMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return lastKnownContainerTokenMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownContainerTokenMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_container_token_master_key = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getLastKnownContainerTokenMasterKeyFieldBuilder() {
        if (lastKnownContainerTokenMasterKeyBuilder_ == null) {
          lastKnownContainerTokenMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getLastKnownContainerTokenMasterKey(),
                  getParentForChildren(),
                  isClean());
          lastKnownContainerTokenMasterKey_ = null;
        }
        return lastKnownContainerTokenMasterKeyBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto lastKnownNmTokenMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> lastKnownNmTokenMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public boolean hasLastKnownNmTokenMasterKey() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getLastKnownNmTokenMasterKey() {
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          return lastKnownNmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownNmTokenMasterKey_;
        } else {
          return lastKnownNmTokenMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public Builder setLastKnownNmTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          lastKnownNmTokenMasterKey_ = value;
          onChanged();
        } else {
          lastKnownNmTokenMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public Builder setLastKnownNmTokenMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          lastKnownNmTokenMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          lastKnownNmTokenMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public Builder mergeLastKnownNmTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              lastKnownNmTokenMasterKey_ != null &&
              lastKnownNmTokenMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            lastKnownNmTokenMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(lastKnownNmTokenMasterKey_).mergeFrom(value).buildPartial();
          } else {
            lastKnownNmTokenMasterKey_ = value;
          }
          onChanged();
        } else {
          lastKnownNmTokenMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public Builder clearLastKnownNmTokenMasterKey() {
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          lastKnownNmTokenMasterKey_ = null;
          onChanged();
        } else {
          lastKnownNmTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getLastKnownNmTokenMasterKeyBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getLastKnownNmTokenMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getLastKnownNmTokenMasterKeyOrBuilder() {
        if (lastKnownNmTokenMasterKeyBuilder_ != null) {
          return lastKnownNmTokenMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return lastKnownNmTokenMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : lastKnownNmTokenMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto last_known_nm_token_master_key = 3;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getLastKnownNmTokenMasterKeyFieldBuilder() {
        if (lastKnownNmTokenMasterKeyBuilder_ == null) {
          lastKnownNmTokenMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getLastKnownNmTokenMasterKey(),
                  getParentForChildren(),
                  isClean());
          lastKnownNmTokenMasterKey_ = null;
        }
        return lastKnownNmTokenMasterKeyBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto nodeLabels_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder> nodeLabelsBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public boolean hasNodeLabels() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto getNodeLabels() {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
        } else {
          return nodeLabelsBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public Builder setNodeLabels(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeLabels_ = value;
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public Builder setNodeLabels(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = builderForValue.build();
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public Builder mergeNodeLabels(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0) &&
              nodeLabels_ != null &&
              nodeLabels_ != org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance()) {
            nodeLabels_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.newBuilder(nodeLabels_).mergeFrom(value).buildPartial();
          } else {
            nodeLabels_ = value;
          }
          onChanged();
        } else {
          nodeLabelsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public Builder clearNodeLabels() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = null;
          onChanged();
        } else {
          nodeLabelsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder getNodeLabelsBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getNodeLabelsFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder getNodeLabelsOrBuilder() {
        if (nodeLabelsBuilder_ != null) {
          return nodeLabelsBuilder_.getMessageOrBuilder();
        } else {
          return nodeLabels_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.getDefaultInstance() : nodeLabels_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeLabelsProto nodeLabels = 4;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder> 
          getNodeLabelsFieldBuilder() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabelsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeLabelsProtoOrBuilder>(
                  getNodeLabels(),
                  getParentForChildren(),
                  isClean());
          nodeLabels_ = null;
        }
        return nodeLabelsBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> logAggregationReportsForApps_ =
        java.util.Collections.emptyList();
      private void ensureLogAggregationReportsForAppsIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          logAggregationReportsForApps_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto>(logAggregationReportsForApps_);
          bitField0_ |= 0x00000010;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> logAggregationReportsForAppsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> getLogAggregationReportsForAppsList() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
        } else {
          return logAggregationReportsForAppsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public int getLogAggregationReportsForAppsCount() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return logAggregationReportsForApps_.size();
        } else {
          return logAggregationReportsForAppsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getLogAggregationReportsForApps(int index) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return logAggregationReportsForApps_.get(index);
        } else {
          return logAggregationReportsForAppsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder setLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto value) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.set(index, value);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder setLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder builderForValue) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.set(index, builderForValue.build());
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder addLogAggregationReportsForApps(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto value) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(value);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder addLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto value) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(index, value);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder addLogAggregationReportsForApps(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder builderForValue) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(builderForValue.build());
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder addLogAggregationReportsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder builderForValue) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.add(index, builderForValue.build());
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder addAllLogAggregationReportsForApps(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto> values) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, logAggregationReportsForApps_);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder clearLogAggregationReportsForApps() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          logAggregationReportsForApps_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public Builder removeLogAggregationReportsForApps(int index) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          ensureLogAggregationReportsForAppsIsMutable();
          logAggregationReportsForApps_.remove(index);
          onChanged();
        } else {
          logAggregationReportsForAppsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder getLogAggregationReportsForAppsBuilder(
          int index) {
        return getLogAggregationReportsForAppsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder getLogAggregationReportsForAppsOrBuilder(
          int index) {
        if (logAggregationReportsForAppsBuilder_ == null) {
          return logAggregationReportsForApps_.get(index);  } else {
          return logAggregationReportsForAppsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
           getLogAggregationReportsForAppsOrBuilderList() {
        if (logAggregationReportsForAppsBuilder_ != null) {
          return logAggregationReportsForAppsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(logAggregationReportsForApps_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder addLogAggregationReportsForAppsBuilder() {
        return getLogAggregationReportsForAppsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder addLogAggregationReportsForAppsBuilder(
          int index) {
        return getLogAggregationReportsForAppsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.LogAggregationReportProto log_aggregation_reports_for_apps = 5;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder> 
           getLogAggregationReportsForAppsBuilderList() {
        return getLogAggregationReportsForAppsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder> 
          getLogAggregationReportsForAppsFieldBuilder() {
        if (logAggregationReportsForAppsBuilder_ == null) {
          logAggregationReportsForAppsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder>(
                  logAggregationReportsForApps_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          logAggregationReportsForApps_ = null;
        }
        return logAggregationReportsForAppsBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> registeringCollectors_ =
        java.util.Collections.emptyList();
      private void ensureRegisteringCollectorsIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          registeringCollectors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto>(registeringCollectors_);
          bitField0_ |= 0x00000020;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> registeringCollectorsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> getRegisteringCollectorsList() {
        if (registeringCollectorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(registeringCollectors_);
        } else {
          return registeringCollectorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public int getRegisteringCollectorsCount() {
        if (registeringCollectorsBuilder_ == null) {
          return registeringCollectors_.size();
        } else {
          return registeringCollectorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getRegisteringCollectors(int index) {
        if (registeringCollectorsBuilder_ == null) {
          return registeringCollectors_.get(index);
        } else {
          return registeringCollectorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder setRegisteringCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (registeringCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.set(index, value);
          onChanged();
        } else {
          registeringCollectorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder setRegisteringCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (registeringCollectorsBuilder_ == null) {
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.set(index, builderForValue.build());
          onChanged();
        } else {
          registeringCollectorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder addRegisteringCollectors(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (registeringCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.add(value);
          onChanged();
        } else {
          registeringCollectorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder addRegisteringCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (registeringCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.add(index, value);
          onChanged();
        } else {
          registeringCollectorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder addRegisteringCollectors(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (registeringCollectorsBuilder_ == null) {
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.add(builderForValue.build());
          onChanged();
        } else {
          registeringCollectorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder addRegisteringCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (registeringCollectorsBuilder_ == null) {
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.add(index, builderForValue.build());
          onChanged();
        } else {
          registeringCollectorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder addAllRegisteringCollectors(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> values) {
        if (registeringCollectorsBuilder_ == null) {
          ensureRegisteringCollectorsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, registeringCollectors_);
          onChanged();
        } else {
          registeringCollectorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder clearRegisteringCollectors() {
        if (registeringCollectorsBuilder_ == null) {
          registeringCollectors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          registeringCollectorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public Builder removeRegisteringCollectors(int index) {
        if (registeringCollectorsBuilder_ == null) {
          ensureRegisteringCollectorsIsMutable();
          registeringCollectors_.remove(index);
          onChanged();
        } else {
          registeringCollectorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder getRegisteringCollectorsBuilder(
          int index) {
        return getRegisteringCollectorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getRegisteringCollectorsOrBuilder(
          int index) {
        if (registeringCollectorsBuilder_ == null) {
          return registeringCollectors_.get(index);  } else {
          return registeringCollectorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
           getRegisteringCollectorsOrBuilderList() {
        if (registeringCollectorsBuilder_ != null) {
          return registeringCollectorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(registeringCollectors_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder addRegisteringCollectorsBuilder() {
        return getRegisteringCollectorsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder addRegisteringCollectorsBuilder(
          int index) {
        return getRegisteringCollectorsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto registering_collectors = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder> 
           getRegisteringCollectorsBuilderList() {
        return getRegisteringCollectorsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
          getRegisteringCollectorsFieldBuilder() {
        if (registeringCollectorsBuilder_ == null) {
          registeringCollectorsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder>(
                  registeringCollectors_,
                  ((bitField0_ & 0x00000020) != 0),
                  getParentForChildren(),
                  isClean());
          registeringCollectors_ = null;
        }
        return registeringCollectorsBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto nodeAttributes_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder> nodeAttributesBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public boolean hasNodeAttributes() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto getNodeAttributes() {
        if (nodeAttributesBuilder_ == null) {
          return nodeAttributes_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
        } else {
          return nodeAttributesBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public Builder setNodeAttributes(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeAttributes_ = value;
          onChanged();
        } else {
          nodeAttributesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public Builder setNodeAttributes(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder builderForValue) {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = builderForValue.build();
          onChanged();
        } else {
          nodeAttributesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public Builder mergeNodeAttributes(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto value) {
        if (nodeAttributesBuilder_ == null) {
          if (((bitField0_ & 0x00000040) != 0) &&
              nodeAttributes_ != null &&
              nodeAttributes_ != org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance()) {
            nodeAttributes_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.newBuilder(nodeAttributes_).mergeFrom(value).buildPartial();
          } else {
            nodeAttributes_ = value;
          }
          onChanged();
        } else {
          nodeAttributesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public Builder clearNodeAttributes() {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributes_ = null;
          onChanged();
        } else {
          nodeAttributesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder getNodeAttributesBuilder() {
        bitField0_ |= 0x00000040;
        onChanged();
        return getNodeAttributesFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder getNodeAttributesOrBuilder() {
        if (nodeAttributesBuilder_ != null) {
          return nodeAttributesBuilder_.getMessageOrBuilder();
        } else {
          return nodeAttributes_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.getDefaultInstance() : nodeAttributes_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeAttributesProto nodeAttributes = 7;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder> 
          getNodeAttributesFieldBuilder() {
        if (nodeAttributesBuilder_ == null) {
          nodeAttributesBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeAttributesProtoOrBuilder>(
                  getNodeAttributes(),
                  getParentForChildren(),
                  isClean());
          nodeAttributes_ = null;
        }
        return nodeAttributesBuilder_;
      }

      private long tokenSequenceNo_ ;
      /**
       * <code>optional int64 tokenSequenceNo = 8;</code>
       */
      public boolean hasTokenSequenceNo() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional int64 tokenSequenceNo = 8;</code>
       */
      public long getTokenSequenceNo() {
        return tokenSequenceNo_;
      }
      /**
       * <code>optional int64 tokenSequenceNo = 8;</code>
       */
      public Builder setTokenSequenceNo(long value) {
        bitField0_ |= 0x00000080;
        tokenSequenceNo_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 tokenSequenceNo = 8;</code>
       */
      public Builder clearTokenSequenceNo() {
        bitField0_ = (bitField0_ & ~0x00000080);
        tokenSequenceNo_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NodeHeartbeatRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NodeHeartbeatRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<NodeHeartbeatRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<NodeHeartbeatRequestProto>() {
      @java.lang.Override
      public NodeHeartbeatRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new NodeHeartbeatRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<NodeHeartbeatRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<NodeHeartbeatRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface LogAggregationReportProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.LogAggregationReportProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    /**
     * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
     */
    boolean hasLogAggregationStatus();
    /**
     * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto getLogAggregationStatus();

    /**
     * <code>optional string diagnostics = 3 [default = "N/A"];</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 3 [default = "N/A"];</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 3 [default = "N/A"];</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.LogAggregationReportProto}
   */
  public  static final class LogAggregationReportProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.LogAggregationReportProto)
      LogAggregationReportProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use LogAggregationReportProto.newBuilder() to construct.
    private LogAggregationReportProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private LogAggregationReportProto() {
      logAggregationStatus_ = 1;
      diagnostics_ = "N/A";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private LogAggregationReportProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto value = org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                logAggregationStatus_ = rawValue;
              }
              break;
            }
            case 26: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              diagnostics_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_LogAggregationReportProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_LogAggregationReportProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder.class);
    }

    private int bitField0_;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : applicationId_;
    }

    public static final int LOG_AGGREGATION_STATUS_FIELD_NUMBER = 2;
    private int logAggregationStatus_;
    /**
     * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
     */
    public boolean hasLogAggregationStatus() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto getLogAggregationStatus() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto result = org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto.valueOf(logAggregationStatus_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto.LOG_DISABLED : result;
    }

    public static final int DIAGNOSTICS_FIELD_NUMBER = 3;
    private volatile java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 3 [default = "N/A"];</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional string diagnostics = 3 [default = "N/A"];</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 3 [default = "N/A"];</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getApplicationId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, logAggregationStatus_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 3, diagnostics_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getApplicationId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, logAggregationStatus_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(3, diagnostics_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto) obj;

      if (hasApplicationId() != other.hasApplicationId()) return false;
      if (hasApplicationId()) {
        if (!getApplicationId()
            .equals(other.getApplicationId())) return false;
      }
      if (hasLogAggregationStatus() != other.hasLogAggregationStatus()) return false;
      if (hasLogAggregationStatus()) {
        if (logAggregationStatus_ != other.logAggregationStatus_) return false;
      }
      if (hasDiagnostics() != other.hasDiagnostics()) return false;
      if (hasDiagnostics()) {
        if (!getDiagnostics()
            .equals(other.getDiagnostics())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasLogAggregationStatus()) {
        hash = (37 * hash) + LOG_AGGREGATION_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + logAggregationStatus_;
      }
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.LogAggregationReportProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.LogAggregationReportProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_LogAggregationReportProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_LogAggregationReportProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = null;
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        logAggregationStatus_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        diagnostics_ = "N/A";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_LogAggregationReportProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (applicationIdBuilder_ == null) {
            result.applicationId_ = applicationId_;
          } else {
            result.applicationId_ = applicationIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.logAggregationStatus_ = logAggregationStatus_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.diagnostics_ = diagnostics_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasLogAggregationStatus()) {
          setLogAggregationStatus(other.getLogAggregationStatus());
        }
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000004;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              applicationId_ != null &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = null;
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  getApplicationId(),
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      private int logAggregationStatus_ = 1;
      /**
       * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
       */
      public boolean hasLogAggregationStatus() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto getLogAggregationStatus() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto result = org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto.valueOf(logAggregationStatus_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto.LOG_DISABLED : result;
      }
      /**
       * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
       */
      public Builder setLogAggregationStatus(org.apache.hadoop.yarn.proto.YarnProtos.LogAggregationStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        logAggregationStatus_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.LogAggregationStatusProto log_aggregation_status = 2;</code>
       */
      public Builder clearLogAggregationStatus() {
        bitField0_ = (bitField0_ & ~0x00000002);
        logAggregationStatus_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object diagnostics_ = "N/A";
      /**
       * <code>optional string diagnostics = 3 [default = "N/A"];</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional string diagnostics = 3 [default = "N/A"];</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnostics_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 3 [default = "N/A"];</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 3 [default = "N/A"];</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 3 [default = "N/A"];</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000004);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 3 [default = "N/A"];</code>
       */
      public Builder setDiagnosticsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.LogAggregationReportProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.LogAggregationReportProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<LogAggregationReportProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<LogAggregationReportProto>() {
      @java.lang.Override
      public LogAggregationReportProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new LogAggregationReportProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<LogAggregationReportProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<LogAggregationReportProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.LogAggregationReportProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NodeHeartbeatResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.NodeHeartbeatResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional int32 response_id = 1;</code>
     */
    boolean hasResponseId();
    /**
     * <code>optional int32 response_id = 1;</code>
     */
    int getResponseId();

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
     */
    boolean hasContainerTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getContainerTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getContainerTokenMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
     */
    boolean hasNmTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNmTokenMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNmTokenMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
     */
    boolean hasNodeAction();
    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto getNodeAction();

    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getContainersToCleanupList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainersToCleanup(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    int getContainersToCleanupCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainersToCleanupOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainersToCleanupOrBuilder(
        int index);

    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> 
        getApplicationsToCleanupList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationsToCleanup(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    int getApplicationsToCleanupCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
        getApplicationsToCleanupOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationsToCleanupOrBuilder(
        int index);

    /**
     * <code>optional int64 nextHeartBeatInterval = 7;</code>
     */
    boolean hasNextHeartBeatInterval();
    /**
     * <code>optional int64 nextHeartBeatInterval = 7;</code>
     */
    long getNextHeartBeatInterval();

    /**
     * <code>optional string diagnostics_message = 8;</code>
     */
    boolean hasDiagnosticsMessage();
    /**
     * <code>optional string diagnostics_message = 8;</code>
     */
    java.lang.String getDiagnosticsMessage();
    /**
     * <code>optional string diagnostics_message = 8;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsMessageBytes();

    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getContainersToBeRemovedFromNmList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainersToBeRemovedFromNm(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    int getContainersToBeRemovedFromNmCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainersToBeRemovedFromNmOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainersToBeRemovedFromNmOrBuilder(
        int index);

    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto> 
        getSystemCredentialsForAppsList();
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto getSystemCredentialsForApps(int index);
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    int getSystemCredentialsForAppsCount();
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder> 
        getSystemCredentialsForAppsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder getSystemCredentialsForAppsOrBuilder(
        int index);

    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
     */
    boolean hasAreNodeLabelsAcceptedByRM();
    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
     */
    boolean getAreNodeLabelsAcceptedByRM();

    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> 
        getContainersToDecreaseList();
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersToDecrease(int index);
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    int getContainersToDecreaseCount();
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getContainersToDecreaseOrBuilderList();
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersToDecreaseOrBuilder(
        int index);

    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto> 
        getContainersToSignalList();
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto getContainersToSignal(int index);
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    int getContainersToSignalCount();
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder> 
        getContainersToSignalOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder getContainersToSignalOrBuilder(
        int index);

    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
     */
    boolean hasResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
     */
    boolean hasContainerQueuingLimit();
    /**
     * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto getContainerQueuingLimit();
    /**
     * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder getContainerQueuingLimitOrBuilder();

    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> 
        getAppCollectorsList();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getAppCollectors(int index);
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    int getAppCollectorsCount();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
        getAppCollectorsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getAppCollectorsOrBuilder(
        int index);

    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> 
        getContainersToUpdateList();
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersToUpdate(int index);
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    int getContainersToUpdateCount();
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getContainersToUpdateOrBuilderList();
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersToUpdateOrBuilder(
        int index);

    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
     */
    boolean hasAreNodeAttributesAcceptedByRM();
    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
     */
    boolean getAreNodeAttributesAcceptedByRM();

    /**
     * <code>optional int64 tokenSequenceNo = 19;</code>
     */
    boolean hasTokenSequenceNo();
    /**
     * <code>optional int64 tokenSequenceNo = 19;</code>
     */
    long getTokenSequenceNo();
  }
  /**
   * Protobuf type {@code hadoop.yarn.NodeHeartbeatResponseProto}
   */
  public  static final class NodeHeartbeatResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.NodeHeartbeatResponseProto)
      NodeHeartbeatResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NodeHeartbeatResponseProto.newBuilder() to construct.
    private NodeHeartbeatResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NodeHeartbeatResponseProto() {
      nodeAction_ = 0;
      containersToCleanup_ = java.util.Collections.emptyList();
      applicationsToCleanup_ = java.util.Collections.emptyList();
      diagnosticsMessage_ = "";
      containersToBeRemovedFromNm_ = java.util.Collections.emptyList();
      systemCredentialsForApps_ = java.util.Collections.emptyList();
      containersToDecrease_ = java.util.Collections.emptyList();
      containersToSignal_ = java.util.Collections.emptyList();
      appCollectors_ = java.util.Collections.emptyList();
      containersToUpdate_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NodeHeartbeatResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              responseId_ = input.readInt32();
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = containerTokenMasterKey_.toBuilder();
              }
              containerTokenMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerTokenMasterKey_);
                containerTokenMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = nmTokenMasterKey_.toBuilder();
              }
              nmTokenMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nmTokenMasterKey_);
                nmTokenMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 32: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto value = org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(4, rawValue);
              } else {
                bitField0_ |= 0x00000008;
                nodeAction_ = rawValue;
              }
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                containersToCleanup_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000010;
              }
              containersToCleanup_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                applicationsToCleanup_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              applicationsToCleanup_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000010;
              nextHeartBeatInterval_ = input.readInt64();
              break;
            }
            case 66: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000020;
              diagnosticsMessage_ = bs;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                containersToBeRemovedFromNm_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000100;
              }
              containersToBeRemovedFromNm_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                systemCredentialsForApps_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto>();
                mutable_bitField0_ |= 0x00000200;
              }
              systemCredentialsForApps_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.PARSER, extensionRegistry));
              break;
            }
            case 88: {
              bitField0_ |= 0x00000040;
              areNodeLabelsAcceptedByRM_ = input.readBool();
              break;
            }
            case 98: {
              if (!((mutable_bitField0_ & 0x00000800) != 0)) {
                containersToDecrease_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>();
                mutable_bitField0_ |= 0x00000800;
              }
              containersToDecrease_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry));
              break;
            }
            case 106: {
              if (!((mutable_bitField0_ & 0x00001000) != 0)) {
                containersToSignal_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto>();
                mutable_bitField0_ |= 0x00001000;
              }
              containersToSignal_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.PARSER, extensionRegistry));
              break;
            }
            case 114: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) != 0)) {
                subBuilder = resource_.toBuilder();
              }
              resource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(resource_);
                resource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 122: {
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) != 0)) {
                subBuilder = containerQueuingLimit_.toBuilder();
              }
              containerQueuingLimit_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerQueuingLimit_);
                containerQueuingLimit_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
            case 130: {
              if (!((mutable_bitField0_ & 0x00008000) != 0)) {
                appCollectors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto>();
                mutable_bitField0_ |= 0x00008000;
              }
              appCollectors_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.PARSER, extensionRegistry));
              break;
            }
            case 138: {
              if (!((mutable_bitField0_ & 0x00010000) != 0)) {
                containersToUpdate_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>();
                mutable_bitField0_ |= 0x00010000;
              }
              containersToUpdate_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry));
              break;
            }
            case 144: {
              bitField0_ |= 0x00000200;
              areNodeAttributesAcceptedByRM_ = input.readBool();
              break;
            }
            case 152: {
              bitField0_ |= 0x00000400;
              tokenSequenceNo_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          containersToCleanup_ = java.util.Collections.unmodifiableList(containersToCleanup_);
        }
        if (((mutable_bitField0_ & 0x00000020) != 0)) {
          applicationsToCleanup_ = java.util.Collections.unmodifiableList(applicationsToCleanup_);
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          containersToBeRemovedFromNm_ = java.util.Collections.unmodifiableList(containersToBeRemovedFromNm_);
        }
        if (((mutable_bitField0_ & 0x00000200) != 0)) {
          systemCredentialsForApps_ = java.util.Collections.unmodifiableList(systemCredentialsForApps_);
        }
        if (((mutable_bitField0_ & 0x00000800) != 0)) {
          containersToDecrease_ = java.util.Collections.unmodifiableList(containersToDecrease_);
        }
        if (((mutable_bitField0_ & 0x00001000) != 0)) {
          containersToSignal_ = java.util.Collections.unmodifiableList(containersToSignal_);
        }
        if (((mutable_bitField0_ & 0x00008000) != 0)) {
          appCollectors_ = java.util.Collections.unmodifiableList(appCollectors_);
        }
        if (((mutable_bitField0_ & 0x00010000) != 0)) {
          containersToUpdate_ = java.util.Collections.unmodifiableList(containersToUpdate_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int RESPONSE_ID_FIELD_NUMBER = 1;
    private int responseId_;
    /**
     * <code>optional int32 response_id = 1;</code>
     */
    public boolean hasResponseId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int32 response_id = 1;</code>
     */
    public int getResponseId() {
      return responseId_;
    }

    public static final int CONTAINER_TOKEN_MASTER_KEY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto containerTokenMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
     */
    public boolean hasContainerTokenMasterKey() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getContainerTokenMasterKey() {
      return containerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getContainerTokenMasterKeyOrBuilder() {
      return containerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
    }

    public static final int NM_TOKEN_MASTER_KEY_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto nmTokenMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
     */
    public boolean hasNmTokenMasterKey() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNmTokenMasterKey() {
      return nmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNmTokenMasterKeyOrBuilder() {
      return nmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
    }

    public static final int NODEACTION_FIELD_NUMBER = 4;
    private int nodeAction_;
    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
     */
    public boolean hasNodeAction() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto getNodeAction() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto result = org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.valueOf(nodeAction_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.NORMAL : result;
    }

    public static final int CONTAINERS_TO_CLEANUP_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containersToCleanup_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainersToCleanupList() {
      return containersToCleanup_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainersToCleanupOrBuilderList() {
      return containersToCleanup_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    public int getContainersToCleanupCount() {
      return containersToCleanup_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainersToCleanup(int index) {
      return containersToCleanup_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainersToCleanupOrBuilder(
        int index) {
      return containersToCleanup_.get(index);
    }

    public static final int APPLICATIONS_TO_CLEANUP_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> applicationsToCleanup_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> getApplicationsToCleanupList() {
      return applicationsToCleanup_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
        getApplicationsToCleanupOrBuilderList() {
      return applicationsToCleanup_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    public int getApplicationsToCleanupCount() {
      return applicationsToCleanup_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationsToCleanup(int index) {
      return applicationsToCleanup_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationsToCleanupOrBuilder(
        int index) {
      return applicationsToCleanup_.get(index);
    }

    public static final int NEXTHEARTBEATINTERVAL_FIELD_NUMBER = 7;
    private long nextHeartBeatInterval_;
    /**
     * <code>optional int64 nextHeartBeatInterval = 7;</code>
     */
    public boolean hasNextHeartBeatInterval() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int64 nextHeartBeatInterval = 7;</code>
     */
    public long getNextHeartBeatInterval() {
      return nextHeartBeatInterval_;
    }

    public static final int DIAGNOSTICS_MESSAGE_FIELD_NUMBER = 8;
    private volatile java.lang.Object diagnosticsMessage_;
    /**
     * <code>optional string diagnostics_message = 8;</code>
     */
    public boolean hasDiagnosticsMessage() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional string diagnostics_message = 8;</code>
     */
    public java.lang.String getDiagnosticsMessage() {
      java.lang.Object ref = diagnosticsMessage_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnosticsMessage_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics_message = 8;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsMessageBytes() {
      java.lang.Object ref = diagnosticsMessage_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnosticsMessage_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int CONTAINERS_TO_BE_REMOVED_FROM_NM_FIELD_NUMBER = 9;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containersToBeRemovedFromNm_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainersToBeRemovedFromNmList() {
      return containersToBeRemovedFromNm_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainersToBeRemovedFromNmOrBuilderList() {
      return containersToBeRemovedFromNm_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    public int getContainersToBeRemovedFromNmCount() {
      return containersToBeRemovedFromNm_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainersToBeRemovedFromNm(int index) {
      return containersToBeRemovedFromNm_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainersToBeRemovedFromNmOrBuilder(
        int index) {
      return containersToBeRemovedFromNm_.get(index);
    }

    public static final int SYSTEM_CREDENTIALS_FOR_APPS_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto> systemCredentialsForApps_;
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto> getSystemCredentialsForAppsList() {
      return systemCredentialsForApps_;
    }
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder> 
        getSystemCredentialsForAppsOrBuilderList() {
      return systemCredentialsForApps_;
    }
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    public int getSystemCredentialsForAppsCount() {
      return systemCredentialsForApps_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto getSystemCredentialsForApps(int index) {
      return systemCredentialsForApps_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder getSystemCredentialsForAppsOrBuilder(
        int index) {
      return systemCredentialsForApps_.get(index);
    }

    public static final int ARENODELABELSACCEPTEDBYRM_FIELD_NUMBER = 11;
    private boolean areNodeLabelsAcceptedByRM_;
    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
     */
    public boolean hasAreNodeLabelsAcceptedByRM() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
     */
    public boolean getAreNodeLabelsAcceptedByRM() {
      return areNodeLabelsAcceptedByRM_;
    }

    public static final int CONTAINERS_TO_DECREASE_FIELD_NUMBER = 12;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> containersToDecrease_;
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getContainersToDecreaseList() {
      return containersToDecrease_;
    }
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getContainersToDecreaseOrBuilderList() {
      return containersToDecrease_;
    }
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    public int getContainersToDecreaseCount() {
      return containersToDecrease_.size();
    }
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersToDecrease(int index) {
      return containersToDecrease_.get(index);
    }
    /**
     * <pre>
     * to be deprecated in favour of containers_to_update
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersToDecreaseOrBuilder(
        int index) {
      return containersToDecrease_.get(index);
    }

    public static final int CONTAINERS_TO_SIGNAL_FIELD_NUMBER = 13;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto> containersToSignal_;
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto> getContainersToSignalList() {
      return containersToSignal_;
    }
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder> 
        getContainersToSignalOrBuilderList() {
      return containersToSignal_;
    }
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    public int getContainersToSignalCount() {
      return containersToSignal_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto getContainersToSignal(int index) {
      return containersToSignal_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder getContainersToSignalOrBuilder(
        int index) {
      return containersToSignal_.get(index);
    }

    public static final int RESOURCE_FIELD_NUMBER = 14;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
     */
    public boolean hasResource() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }

    public static final int CONTAINER_QUEUING_LIMIT_FIELD_NUMBER = 15;
    private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto containerQueuingLimit_;
    /**
     * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
     */
    public boolean hasContainerQueuingLimit() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto getContainerQueuingLimit() {
      return containerQueuingLimit_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance() : containerQueuingLimit_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder getContainerQueuingLimitOrBuilder() {
      return containerQueuingLimit_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance() : containerQueuingLimit_;
    }

    public static final int APP_COLLECTORS_FIELD_NUMBER = 16;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> appCollectors_;
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> getAppCollectorsList() {
      return appCollectors_;
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
        getAppCollectorsOrBuilderList() {
      return appCollectors_;
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    public int getAppCollectorsCount() {
      return appCollectors_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getAppCollectors(int index) {
      return appCollectors_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getAppCollectorsOrBuilder(
        int index) {
      return appCollectors_.get(index);
    }

    public static final int CONTAINERS_TO_UPDATE_FIELD_NUMBER = 17;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> containersToUpdate_;
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getContainersToUpdateList() {
      return containersToUpdate_;
    }
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getContainersToUpdateOrBuilderList() {
      return containersToUpdate_;
    }
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    public int getContainersToUpdateCount() {
      return containersToUpdate_.size();
    }
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersToUpdate(int index) {
      return containersToUpdate_.get(index);
    }
    /**
     * <pre>
     * to be used in place of containers_to_decrease
     * </pre>
     *
     * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersToUpdateOrBuilder(
        int index) {
      return containersToUpdate_.get(index);
    }

    public static final int ARENODEATTRIBUTESACCEPTEDBYRM_FIELD_NUMBER = 18;
    private boolean areNodeAttributesAcceptedByRM_;
    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
     */
    public boolean hasAreNodeAttributesAcceptedByRM() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
     */
    public boolean getAreNodeAttributesAcceptedByRM() {
      return areNodeAttributesAcceptedByRM_;
    }

    public static final int TOKENSEQUENCENO_FIELD_NUMBER = 19;
    private long tokenSequenceNo_;
    /**
     * <code>optional int64 tokenSequenceNo = 19;</code>
     */
    public boolean hasTokenSequenceNo() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional int64 tokenSequenceNo = 19;</code>
     */
    public long getTokenSequenceNo() {
      return tokenSequenceNo_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getContainersToDecreaseCount(); i++) {
        if (!getContainersToDecrease(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getContainersToSignalCount(); i++) {
        if (!getContainersToSignal(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasResource()) {
        if (!getResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getAppCollectorsCount(); i++) {
        if (!getAppCollectors(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getContainersToUpdateCount(); i++) {
        if (!getContainersToUpdate(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt32(1, responseId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getContainerTokenMasterKey());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getNmTokenMasterKey());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeEnum(4, nodeAction_);
      }
      for (int i = 0; i < containersToCleanup_.size(); i++) {
        output.writeMessage(5, containersToCleanup_.get(i));
      }
      for (int i = 0; i < applicationsToCleanup_.size(); i++) {
        output.writeMessage(6, applicationsToCleanup_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt64(7, nextHeartBeatInterval_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 8, diagnosticsMessage_);
      }
      for (int i = 0; i < containersToBeRemovedFromNm_.size(); i++) {
        output.writeMessage(9, containersToBeRemovedFromNm_.get(i));
      }
      for (int i = 0; i < systemCredentialsForApps_.size(); i++) {
        output.writeMessage(10, systemCredentialsForApps_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeBool(11, areNodeLabelsAcceptedByRM_);
      }
      for (int i = 0; i < containersToDecrease_.size(); i++) {
        output.writeMessage(12, containersToDecrease_.get(i));
      }
      for (int i = 0; i < containersToSignal_.size(); i++) {
        output.writeMessage(13, containersToSignal_.get(i));
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeMessage(14, getResource());
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeMessage(15, getContainerQueuingLimit());
      }
      for (int i = 0; i < appCollectors_.size(); i++) {
        output.writeMessage(16, appCollectors_.get(i));
      }
      for (int i = 0; i < containersToUpdate_.size(); i++) {
        output.writeMessage(17, containersToUpdate_.get(i));
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeBool(18, areNodeAttributesAcceptedByRM_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeInt64(19, tokenSequenceNo_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(1, responseId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getContainerTokenMasterKey());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(3, getNmTokenMasterKey());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(4, nodeAction_);
      }
      for (int i = 0; i < containersToCleanup_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(5, containersToCleanup_.get(i));
      }
      for (int i = 0; i < applicationsToCleanup_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(6, applicationsToCleanup_.get(i));
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(7, nextHeartBeatInterval_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(8, diagnosticsMessage_);
      }
      for (int i = 0; i < containersToBeRemovedFromNm_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(9, containersToBeRemovedFromNm_.get(i));
      }
      for (int i = 0; i < systemCredentialsForApps_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(10, systemCredentialsForApps_.get(i));
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(11, areNodeLabelsAcceptedByRM_);
      }
      for (int i = 0; i < containersToDecrease_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(12, containersToDecrease_.get(i));
      }
      for (int i = 0; i < containersToSignal_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(13, containersToSignal_.get(i));
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(14, getResource());
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(15, getContainerQueuingLimit());
      }
      for (int i = 0; i < appCollectors_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(16, appCollectors_.get(i));
      }
      for (int i = 0; i < containersToUpdate_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(17, containersToUpdate_.get(i));
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(18, areNodeAttributesAcceptedByRM_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(19, tokenSequenceNo_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto) obj;

      if (hasResponseId() != other.hasResponseId()) return false;
      if (hasResponseId()) {
        if (getResponseId()
            != other.getResponseId()) return false;
      }
      if (hasContainerTokenMasterKey() != other.hasContainerTokenMasterKey()) return false;
      if (hasContainerTokenMasterKey()) {
        if (!getContainerTokenMasterKey()
            .equals(other.getContainerTokenMasterKey())) return false;
      }
      if (hasNmTokenMasterKey() != other.hasNmTokenMasterKey()) return false;
      if (hasNmTokenMasterKey()) {
        if (!getNmTokenMasterKey()
            .equals(other.getNmTokenMasterKey())) return false;
      }
      if (hasNodeAction() != other.hasNodeAction()) return false;
      if (hasNodeAction()) {
        if (nodeAction_ != other.nodeAction_) return false;
      }
      if (!getContainersToCleanupList()
          .equals(other.getContainersToCleanupList())) return false;
      if (!getApplicationsToCleanupList()
          .equals(other.getApplicationsToCleanupList())) return false;
      if (hasNextHeartBeatInterval() != other.hasNextHeartBeatInterval()) return false;
      if (hasNextHeartBeatInterval()) {
        if (getNextHeartBeatInterval()
            != other.getNextHeartBeatInterval()) return false;
      }
      if (hasDiagnosticsMessage() != other.hasDiagnosticsMessage()) return false;
      if (hasDiagnosticsMessage()) {
        if (!getDiagnosticsMessage()
            .equals(other.getDiagnosticsMessage())) return false;
      }
      if (!getContainersToBeRemovedFromNmList()
          .equals(other.getContainersToBeRemovedFromNmList())) return false;
      if (!getSystemCredentialsForAppsList()
          .equals(other.getSystemCredentialsForAppsList())) return false;
      if (hasAreNodeLabelsAcceptedByRM() != other.hasAreNodeLabelsAcceptedByRM()) return false;
      if (hasAreNodeLabelsAcceptedByRM()) {
        if (getAreNodeLabelsAcceptedByRM()
            != other.getAreNodeLabelsAcceptedByRM()) return false;
      }
      if (!getContainersToDecreaseList()
          .equals(other.getContainersToDecreaseList())) return false;
      if (!getContainersToSignalList()
          .equals(other.getContainersToSignalList())) return false;
      if (hasResource() != other.hasResource()) return false;
      if (hasResource()) {
        if (!getResource()
            .equals(other.getResource())) return false;
      }
      if (hasContainerQueuingLimit() != other.hasContainerQueuingLimit()) return false;
      if (hasContainerQueuingLimit()) {
        if (!getContainerQueuingLimit()
            .equals(other.getContainerQueuingLimit())) return false;
      }
      if (!getAppCollectorsList()
          .equals(other.getAppCollectorsList())) return false;
      if (!getContainersToUpdateList()
          .equals(other.getContainersToUpdateList())) return false;
      if (hasAreNodeAttributesAcceptedByRM() != other.hasAreNodeAttributesAcceptedByRM()) return false;
      if (hasAreNodeAttributesAcceptedByRM()) {
        if (getAreNodeAttributesAcceptedByRM()
            != other.getAreNodeAttributesAcceptedByRM()) return false;
      }
      if (hasTokenSequenceNo() != other.hasTokenSequenceNo()) return false;
      if (hasTokenSequenceNo()) {
        if (getTokenSequenceNo()
            != other.getTokenSequenceNo()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResponseId()) {
        hash = (37 * hash) + RESPONSE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getResponseId();
      }
      if (hasContainerTokenMasterKey()) {
        hash = (37 * hash) + CONTAINER_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getContainerTokenMasterKey().hashCode();
      }
      if (hasNmTokenMasterKey()) {
        hash = (37 * hash) + NM_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getNmTokenMasterKey().hashCode();
      }
      if (hasNodeAction()) {
        hash = (37 * hash) + NODEACTION_FIELD_NUMBER;
        hash = (53 * hash) + nodeAction_;
      }
      if (getContainersToCleanupCount() > 0) {
        hash = (37 * hash) + CONTAINERS_TO_CLEANUP_FIELD_NUMBER;
        hash = (53 * hash) + getContainersToCleanupList().hashCode();
      }
      if (getApplicationsToCleanupCount() > 0) {
        hash = (37 * hash) + APPLICATIONS_TO_CLEANUP_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationsToCleanupList().hashCode();
      }
      if (hasNextHeartBeatInterval()) {
        hash = (37 * hash) + NEXTHEARTBEATINTERVAL_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getNextHeartBeatInterval());
      }
      if (hasDiagnosticsMessage()) {
        hash = (37 * hash) + DIAGNOSTICS_MESSAGE_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnosticsMessage().hashCode();
      }
      if (getContainersToBeRemovedFromNmCount() > 0) {
        hash = (37 * hash) + CONTAINERS_TO_BE_REMOVED_FROM_NM_FIELD_NUMBER;
        hash = (53 * hash) + getContainersToBeRemovedFromNmList().hashCode();
      }
      if (getSystemCredentialsForAppsCount() > 0) {
        hash = (37 * hash) + SYSTEM_CREDENTIALS_FOR_APPS_FIELD_NUMBER;
        hash = (53 * hash) + getSystemCredentialsForAppsList().hashCode();
      }
      if (hasAreNodeLabelsAcceptedByRM()) {
        hash = (37 * hash) + ARENODELABELSACCEPTEDBYRM_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getAreNodeLabelsAcceptedByRM());
      }
      if (getContainersToDecreaseCount() > 0) {
        hash = (37 * hash) + CONTAINERS_TO_DECREASE_FIELD_NUMBER;
        hash = (53 * hash) + getContainersToDecreaseList().hashCode();
      }
      if (getContainersToSignalCount() > 0) {
        hash = (37 * hash) + CONTAINERS_TO_SIGNAL_FIELD_NUMBER;
        hash = (53 * hash) + getContainersToSignalList().hashCode();
      }
      if (hasResource()) {
        hash = (37 * hash) + RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getResource().hashCode();
      }
      if (hasContainerQueuingLimit()) {
        hash = (37 * hash) + CONTAINER_QUEUING_LIMIT_FIELD_NUMBER;
        hash = (53 * hash) + getContainerQueuingLimit().hashCode();
      }
      if (getAppCollectorsCount() > 0) {
        hash = (37 * hash) + APP_COLLECTORS_FIELD_NUMBER;
        hash = (53 * hash) + getAppCollectorsList().hashCode();
      }
      if (getContainersToUpdateCount() > 0) {
        hash = (37 * hash) + CONTAINERS_TO_UPDATE_FIELD_NUMBER;
        hash = (53 * hash) + getContainersToUpdateList().hashCode();
      }
      if (hasAreNodeAttributesAcceptedByRM()) {
        hash = (37 * hash) + ARENODEATTRIBUTESACCEPTEDBYRM_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getAreNodeAttributesAcceptedByRM());
      }
      if (hasTokenSequenceNo()) {
        hash = (37 * hash) + TOKENSEQUENCENO_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getTokenSequenceNo());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NodeHeartbeatResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.NodeHeartbeatResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getContainerTokenMasterKeyFieldBuilder();
          getNmTokenMasterKeyFieldBuilder();
          getContainersToCleanupFieldBuilder();
          getApplicationsToCleanupFieldBuilder();
          getContainersToBeRemovedFromNmFieldBuilder();
          getSystemCredentialsForAppsFieldBuilder();
          getContainersToDecreaseFieldBuilder();
          getContainersToSignalFieldBuilder();
          getResourceFieldBuilder();
          getContainerQueuingLimitFieldBuilder();
          getAppCollectorsFieldBuilder();
          getContainersToUpdateFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        responseId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKey_ = null;
        } else {
          containerTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKey_ = null;
        } else {
          nmTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        nodeAction_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (containersToCleanupBuilder_ == null) {
          containersToCleanup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          containersToCleanupBuilder_.clear();
        }
        if (applicationsToCleanupBuilder_ == null) {
          applicationsToCleanup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          applicationsToCleanupBuilder_.clear();
        }
        nextHeartBeatInterval_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        diagnosticsMessage_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        if (containersToBeRemovedFromNmBuilder_ == null) {
          containersToBeRemovedFromNm_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          containersToBeRemovedFromNmBuilder_.clear();
        }
        if (systemCredentialsForAppsBuilder_ == null) {
          systemCredentialsForApps_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          systemCredentialsForAppsBuilder_.clear();
        }
        areNodeLabelsAcceptedByRM_ = false;
        bitField0_ = (bitField0_ & ~0x00000400);
        if (containersToDecreaseBuilder_ == null) {
          containersToDecrease_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000800);
        } else {
          containersToDecreaseBuilder_.clear();
        }
        if (containersToSignalBuilder_ == null) {
          containersToSignal_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00001000);
        } else {
          containersToSignalBuilder_.clear();
        }
        if (resourceBuilder_ == null) {
          resource_ = null;
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00002000);
        if (containerQueuingLimitBuilder_ == null) {
          containerQueuingLimit_ = null;
        } else {
          containerQueuingLimitBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        if (appCollectorsBuilder_ == null) {
          appCollectors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00008000);
        } else {
          appCollectorsBuilder_.clear();
        }
        if (containersToUpdateBuilder_ == null) {
          containersToUpdate_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00010000);
        } else {
          containersToUpdateBuilder_.clear();
        }
        areNodeAttributesAcceptedByRM_ = false;
        bitField0_ = (bitField0_ & ~0x00020000);
        tokenSequenceNo_ = 0L;
        bitField0_ = (bitField0_ & ~0x00040000);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NodeHeartbeatResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.responseId_ = responseId_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (containerTokenMasterKeyBuilder_ == null) {
            result.containerTokenMasterKey_ = containerTokenMasterKey_;
          } else {
            result.containerTokenMasterKey_ = containerTokenMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (nmTokenMasterKeyBuilder_ == null) {
            result.nmTokenMasterKey_ = nmTokenMasterKey_;
          } else {
            result.nmTokenMasterKey_ = nmTokenMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.nodeAction_ = nodeAction_;
        if (containersToCleanupBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            containersToCleanup_ = java.util.Collections.unmodifiableList(containersToCleanup_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.containersToCleanup_ = containersToCleanup_;
        } else {
          result.containersToCleanup_ = containersToCleanupBuilder_.build();
        }
        if (applicationsToCleanupBuilder_ == null) {
          if (((bitField0_ & 0x00000020) != 0)) {
            applicationsToCleanup_ = java.util.Collections.unmodifiableList(applicationsToCleanup_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.applicationsToCleanup_ = applicationsToCleanup_;
        } else {
          result.applicationsToCleanup_ = applicationsToCleanupBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.nextHeartBeatInterval_ = nextHeartBeatInterval_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.diagnosticsMessage_ = diagnosticsMessage_;
        if (containersToBeRemovedFromNmBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0)) {
            containersToBeRemovedFromNm_ = java.util.Collections.unmodifiableList(containersToBeRemovedFromNm_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.containersToBeRemovedFromNm_ = containersToBeRemovedFromNm_;
        } else {
          result.containersToBeRemovedFromNm_ = containersToBeRemovedFromNmBuilder_.build();
        }
        if (systemCredentialsForAppsBuilder_ == null) {
          if (((bitField0_ & 0x00000200) != 0)) {
            systemCredentialsForApps_ = java.util.Collections.unmodifiableList(systemCredentialsForApps_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.systemCredentialsForApps_ = systemCredentialsForApps_;
        } else {
          result.systemCredentialsForApps_ = systemCredentialsForAppsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000400) != 0)) {
          result.areNodeLabelsAcceptedByRM_ = areNodeLabelsAcceptedByRM_;
          to_bitField0_ |= 0x00000040;
        }
        if (containersToDecreaseBuilder_ == null) {
          if (((bitField0_ & 0x00000800) != 0)) {
            containersToDecrease_ = java.util.Collections.unmodifiableList(containersToDecrease_);
            bitField0_ = (bitField0_ & ~0x00000800);
          }
          result.containersToDecrease_ = containersToDecrease_;
        } else {
          result.containersToDecrease_ = containersToDecreaseBuilder_.build();
        }
        if (containersToSignalBuilder_ == null) {
          if (((bitField0_ & 0x00001000) != 0)) {
            containersToSignal_ = java.util.Collections.unmodifiableList(containersToSignal_);
            bitField0_ = (bitField0_ & ~0x00001000);
          }
          result.containersToSignal_ = containersToSignal_;
        } else {
          result.containersToSignal_ = containersToSignalBuilder_.build();
        }
        if (((from_bitField0_ & 0x00002000) != 0)) {
          if (resourceBuilder_ == null) {
            result.resource_ = resource_;
          } else {
            result.resource_ = resourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000080;
        }
        if (((from_bitField0_ & 0x00004000) != 0)) {
          if (containerQueuingLimitBuilder_ == null) {
            result.containerQueuingLimit_ = containerQueuingLimit_;
          } else {
            result.containerQueuingLimit_ = containerQueuingLimitBuilder_.build();
          }
          to_bitField0_ |= 0x00000100;
        }
        if (appCollectorsBuilder_ == null) {
          if (((bitField0_ & 0x00008000) != 0)) {
            appCollectors_ = java.util.Collections.unmodifiableList(appCollectors_);
            bitField0_ = (bitField0_ & ~0x00008000);
          }
          result.appCollectors_ = appCollectors_;
        } else {
          result.appCollectors_ = appCollectorsBuilder_.build();
        }
        if (containersToUpdateBuilder_ == null) {
          if (((bitField0_ & 0x00010000) != 0)) {
            containersToUpdate_ = java.util.Collections.unmodifiableList(containersToUpdate_);
            bitField0_ = (bitField0_ & ~0x00010000);
          }
          result.containersToUpdate_ = containersToUpdate_;
        } else {
          result.containersToUpdate_ = containersToUpdateBuilder_.build();
        }
        if (((from_bitField0_ & 0x00020000) != 0)) {
          result.areNodeAttributesAcceptedByRM_ = areNodeAttributesAcceptedByRM_;
          to_bitField0_ |= 0x00000200;
        }
        if (((from_bitField0_ & 0x00040000) != 0)) {
          result.tokenSequenceNo_ = tokenSequenceNo_;
          to_bitField0_ |= 0x00000400;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto.getDefaultInstance()) return this;
        if (other.hasResponseId()) {
          setResponseId(other.getResponseId());
        }
        if (other.hasContainerTokenMasterKey()) {
          mergeContainerTokenMasterKey(other.getContainerTokenMasterKey());
        }
        if (other.hasNmTokenMasterKey()) {
          mergeNmTokenMasterKey(other.getNmTokenMasterKey());
        }
        if (other.hasNodeAction()) {
          setNodeAction(other.getNodeAction());
        }
        if (containersToCleanupBuilder_ == null) {
          if (!other.containersToCleanup_.isEmpty()) {
            if (containersToCleanup_.isEmpty()) {
              containersToCleanup_ = other.containersToCleanup_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureContainersToCleanupIsMutable();
              containersToCleanup_.addAll(other.containersToCleanup_);
            }
            onChanged();
          }
        } else {
          if (!other.containersToCleanup_.isEmpty()) {
            if (containersToCleanupBuilder_.isEmpty()) {
              containersToCleanupBuilder_.dispose();
              containersToCleanupBuilder_ = null;
              containersToCleanup_ = other.containersToCleanup_;
              bitField0_ = (bitField0_ & ~0x00000010);
              containersToCleanupBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getContainersToCleanupFieldBuilder() : null;
            } else {
              containersToCleanupBuilder_.addAllMessages(other.containersToCleanup_);
            }
          }
        }
        if (applicationsToCleanupBuilder_ == null) {
          if (!other.applicationsToCleanup_.isEmpty()) {
            if (applicationsToCleanup_.isEmpty()) {
              applicationsToCleanup_ = other.applicationsToCleanup_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureApplicationsToCleanupIsMutable();
              applicationsToCleanup_.addAll(other.applicationsToCleanup_);
            }
            onChanged();
          }
        } else {
          if (!other.applicationsToCleanup_.isEmpty()) {
            if (applicationsToCleanupBuilder_.isEmpty()) {
              applicationsToCleanupBuilder_.dispose();
              applicationsToCleanupBuilder_ = null;
              applicationsToCleanup_ = other.applicationsToCleanup_;
              bitField0_ = (bitField0_ & ~0x00000020);
              applicationsToCleanupBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getApplicationsToCleanupFieldBuilder() : null;
            } else {
              applicationsToCleanupBuilder_.addAllMessages(other.applicationsToCleanup_);
            }
          }
        }
        if (other.hasNextHeartBeatInterval()) {
          setNextHeartBeatInterval(other.getNextHeartBeatInterval());
        }
        if (other.hasDiagnosticsMessage()) {
          bitField0_ |= 0x00000080;
          diagnosticsMessage_ = other.diagnosticsMessage_;
          onChanged();
        }
        if (containersToBeRemovedFromNmBuilder_ == null) {
          if (!other.containersToBeRemovedFromNm_.isEmpty()) {
            if (containersToBeRemovedFromNm_.isEmpty()) {
              containersToBeRemovedFromNm_ = other.containersToBeRemovedFromNm_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureContainersToBeRemovedFromNmIsMutable();
              containersToBeRemovedFromNm_.addAll(other.containersToBeRemovedFromNm_);
            }
            onChanged();
          }
        } else {
          if (!other.containersToBeRemovedFromNm_.isEmpty()) {
            if (containersToBeRemovedFromNmBuilder_.isEmpty()) {
              containersToBeRemovedFromNmBuilder_.dispose();
              containersToBeRemovedFromNmBuilder_ = null;
              containersToBeRemovedFromNm_ = other.containersToBeRemovedFromNm_;
              bitField0_ = (bitField0_ & ~0x00000100);
              containersToBeRemovedFromNmBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getContainersToBeRemovedFromNmFieldBuilder() : null;
            } else {
              containersToBeRemovedFromNmBuilder_.addAllMessages(other.containersToBeRemovedFromNm_);
            }
          }
        }
        if (systemCredentialsForAppsBuilder_ == null) {
          if (!other.systemCredentialsForApps_.isEmpty()) {
            if (systemCredentialsForApps_.isEmpty()) {
              systemCredentialsForApps_ = other.systemCredentialsForApps_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureSystemCredentialsForAppsIsMutable();
              systemCredentialsForApps_.addAll(other.systemCredentialsForApps_);
            }
            onChanged();
          }
        } else {
          if (!other.systemCredentialsForApps_.isEmpty()) {
            if (systemCredentialsForAppsBuilder_.isEmpty()) {
              systemCredentialsForAppsBuilder_.dispose();
              systemCredentialsForAppsBuilder_ = null;
              systemCredentialsForApps_ = other.systemCredentialsForApps_;
              bitField0_ = (bitField0_ & ~0x00000200);
              systemCredentialsForAppsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getSystemCredentialsForAppsFieldBuilder() : null;
            } else {
              systemCredentialsForAppsBuilder_.addAllMessages(other.systemCredentialsForApps_);
            }
          }
        }
        if (other.hasAreNodeLabelsAcceptedByRM()) {
          setAreNodeLabelsAcceptedByRM(other.getAreNodeLabelsAcceptedByRM());
        }
        if (containersToDecreaseBuilder_ == null) {
          if (!other.containersToDecrease_.isEmpty()) {
            if (containersToDecrease_.isEmpty()) {
              containersToDecrease_ = other.containersToDecrease_;
              bitField0_ = (bitField0_ & ~0x00000800);
            } else {
              ensureContainersToDecreaseIsMutable();
              containersToDecrease_.addAll(other.containersToDecrease_);
            }
            onChanged();
          }
        } else {
          if (!other.containersToDecrease_.isEmpty()) {
            if (containersToDecreaseBuilder_.isEmpty()) {
              containersToDecreaseBuilder_.dispose();
              containersToDecreaseBuilder_ = null;
              containersToDecrease_ = other.containersToDecrease_;
              bitField0_ = (bitField0_ & ~0x00000800);
              containersToDecreaseBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getContainersToDecreaseFieldBuilder() : null;
            } else {
              containersToDecreaseBuilder_.addAllMessages(other.containersToDecrease_);
            }
          }
        }
        if (containersToSignalBuilder_ == null) {
          if (!other.containersToSignal_.isEmpty()) {
            if (containersToSignal_.isEmpty()) {
              containersToSignal_ = other.containersToSignal_;
              bitField0_ = (bitField0_ & ~0x00001000);
            } else {
              ensureContainersToSignalIsMutable();
              containersToSignal_.addAll(other.containersToSignal_);
            }
            onChanged();
          }
        } else {
          if (!other.containersToSignal_.isEmpty()) {
            if (containersToSignalBuilder_.isEmpty()) {
              containersToSignalBuilder_.dispose();
              containersToSignalBuilder_ = null;
              containersToSignal_ = other.containersToSignal_;
              bitField0_ = (bitField0_ & ~0x00001000);
              containersToSignalBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getContainersToSignalFieldBuilder() : null;
            } else {
              containersToSignalBuilder_.addAllMessages(other.containersToSignal_);
            }
          }
        }
        if (other.hasResource()) {
          mergeResource(other.getResource());
        }
        if (other.hasContainerQueuingLimit()) {
          mergeContainerQueuingLimit(other.getContainerQueuingLimit());
        }
        if (appCollectorsBuilder_ == null) {
          if (!other.appCollectors_.isEmpty()) {
            if (appCollectors_.isEmpty()) {
              appCollectors_ = other.appCollectors_;
              bitField0_ = (bitField0_ & ~0x00008000);
            } else {
              ensureAppCollectorsIsMutable();
              appCollectors_.addAll(other.appCollectors_);
            }
            onChanged();
          }
        } else {
          if (!other.appCollectors_.isEmpty()) {
            if (appCollectorsBuilder_.isEmpty()) {
              appCollectorsBuilder_.dispose();
              appCollectorsBuilder_ = null;
              appCollectors_ = other.appCollectors_;
              bitField0_ = (bitField0_ & ~0x00008000);
              appCollectorsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAppCollectorsFieldBuilder() : null;
            } else {
              appCollectorsBuilder_.addAllMessages(other.appCollectors_);
            }
          }
        }
        if (containersToUpdateBuilder_ == null) {
          if (!other.containersToUpdate_.isEmpty()) {
            if (containersToUpdate_.isEmpty()) {
              containersToUpdate_ = other.containersToUpdate_;
              bitField0_ = (bitField0_ & ~0x00010000);
            } else {
              ensureContainersToUpdateIsMutable();
              containersToUpdate_.addAll(other.containersToUpdate_);
            }
            onChanged();
          }
        } else {
          if (!other.containersToUpdate_.isEmpty()) {
            if (containersToUpdateBuilder_.isEmpty()) {
              containersToUpdateBuilder_.dispose();
              containersToUpdateBuilder_ = null;
              containersToUpdate_ = other.containersToUpdate_;
              bitField0_ = (bitField0_ & ~0x00010000);
              containersToUpdateBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getContainersToUpdateFieldBuilder() : null;
            } else {
              containersToUpdateBuilder_.addAllMessages(other.containersToUpdate_);
            }
          }
        }
        if (other.hasAreNodeAttributesAcceptedByRM()) {
          setAreNodeAttributesAcceptedByRM(other.getAreNodeAttributesAcceptedByRM());
        }
        if (other.hasTokenSequenceNo()) {
          setTokenSequenceNo(other.getTokenSequenceNo());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getContainersToDecreaseCount(); i++) {
          if (!getContainersToDecrease(i).isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getContainersToSignalCount(); i++) {
          if (!getContainersToSignal(i).isInitialized()) {
            return false;
          }
        }
        if (hasResource()) {
          if (!getResource().isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getAppCollectorsCount(); i++) {
          if (!getAppCollectors(i).isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getContainersToUpdateCount(); i++) {
          if (!getContainersToUpdate(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int responseId_ ;
      /**
       * <code>optional int32 response_id = 1;</code>
       */
      public boolean hasResponseId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int32 response_id = 1;</code>
       */
      public int getResponseId() {
        return responseId_;
      }
      /**
       * <code>optional int32 response_id = 1;</code>
       */
      public Builder setResponseId(int value) {
        bitField0_ |= 0x00000001;
        responseId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 response_id = 1;</code>
       */
      public Builder clearResponseId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        responseId_ = 0;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto containerTokenMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> containerTokenMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public boolean hasContainerTokenMasterKey() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getContainerTokenMasterKey() {
        if (containerTokenMasterKeyBuilder_ == null) {
          return containerTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
        } else {
          return containerTokenMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public Builder setContainerTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (containerTokenMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerTokenMasterKey_ = value;
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public Builder setContainerTokenMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public Builder mergeContainerTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (containerTokenMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              containerTokenMasterKey_ != null &&
              containerTokenMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            containerTokenMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(containerTokenMasterKey_).mergeFrom(value).buildPartial();
          } else {
            containerTokenMasterKey_ = value;
          }
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public Builder clearContainerTokenMasterKey() {
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKey_ = null;
          onChanged();
        } else {
          containerTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getContainerTokenMasterKeyBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getContainerTokenMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getContainerTokenMasterKeyOrBuilder() {
        if (containerTokenMasterKeyBuilder_ != null) {
          return containerTokenMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return containerTokenMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : containerTokenMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto container_token_master_key = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getContainerTokenMasterKeyFieldBuilder() {
        if (containerTokenMasterKeyBuilder_ == null) {
          containerTokenMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getContainerTokenMasterKey(),
                  getParentForChildren(),
                  isClean());
          containerTokenMasterKey_ = null;
        }
        return containerTokenMasterKeyBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto nmTokenMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> nmTokenMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public boolean hasNmTokenMasterKey() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNmTokenMasterKey() {
        if (nmTokenMasterKeyBuilder_ == null) {
          return nmTokenMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
        } else {
          return nmTokenMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public Builder setNmTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (nmTokenMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nmTokenMasterKey_ = value;
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public Builder setNmTokenMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public Builder mergeNmTokenMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (nmTokenMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              nmTokenMasterKey_ != null &&
              nmTokenMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            nmTokenMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(nmTokenMasterKey_).mergeFrom(value).buildPartial();
          } else {
            nmTokenMasterKey_ = value;
          }
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public Builder clearNmTokenMasterKey() {
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKey_ = null;
          onChanged();
        } else {
          nmTokenMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getNmTokenMasterKeyBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getNmTokenMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNmTokenMasterKeyOrBuilder() {
        if (nmTokenMasterKeyBuilder_ != null) {
          return nmTokenMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return nmTokenMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nmTokenMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto nm_token_master_key = 3;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getNmTokenMasterKeyFieldBuilder() {
        if (nmTokenMasterKeyBuilder_ == null) {
          nmTokenMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getNmTokenMasterKey(),
                  getParentForChildren(),
                  isClean());
          nmTokenMasterKey_ = null;
        }
        return nmTokenMasterKeyBuilder_;
      }

      private int nodeAction_ = 0;
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
       */
      public boolean hasNodeAction() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto getNodeAction() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto result = org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.valueOf(nodeAction_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto.NORMAL : result;
      }
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
       */
      public Builder setNodeAction(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeActionProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000008;
        nodeAction_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeActionProto nodeAction = 4;</code>
       */
      public Builder clearNodeAction() {
        bitField0_ = (bitField0_ & ~0x00000008);
        nodeAction_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containersToCleanup_ =
        java.util.Collections.emptyList();
      private void ensureContainersToCleanupIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          containersToCleanup_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(containersToCleanup_);
          bitField0_ |= 0x00000010;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containersToCleanupBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainersToCleanupList() {
        if (containersToCleanupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containersToCleanup_);
        } else {
          return containersToCleanupBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public int getContainersToCleanupCount() {
        if (containersToCleanupBuilder_ == null) {
          return containersToCleanup_.size();
        } else {
          return containersToCleanupBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainersToCleanup(int index) {
        if (containersToCleanupBuilder_ == null) {
          return containersToCleanup_.get(index);
        } else {
          return containersToCleanupBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder setContainersToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containersToCleanupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.set(index, value);
          onChanged();
        } else {
          containersToCleanupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder setContainersToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containersToCleanupBuilder_ == null) {
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersToCleanupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder addContainersToCleanup(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containersToCleanupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.add(value);
          onChanged();
        } else {
          containersToCleanupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder addContainersToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containersToCleanupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.add(index, value);
          onChanged();
        } else {
          containersToCleanupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder addContainersToCleanup(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containersToCleanupBuilder_ == null) {
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.add(builderForValue.build());
          onChanged();
        } else {
          containersToCleanupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder addContainersToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containersToCleanupBuilder_ == null) {
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersToCleanupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder addAllContainersToCleanup(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (containersToCleanupBuilder_ == null) {
          ensureContainersToCleanupIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, containersToCleanup_);
          onChanged();
        } else {
          containersToCleanupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder clearContainersToCleanup() {
        if (containersToCleanupBuilder_ == null) {
          containersToCleanup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          containersToCleanupBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public Builder removeContainersToCleanup(int index) {
        if (containersToCleanupBuilder_ == null) {
          ensureContainersToCleanupIsMutable();
          containersToCleanup_.remove(index);
          onChanged();
        } else {
          containersToCleanupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainersToCleanupBuilder(
          int index) {
        return getContainersToCleanupFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainersToCleanupOrBuilder(
          int index) {
        if (containersToCleanupBuilder_ == null) {
          return containersToCleanup_.get(index);  } else {
          return containersToCleanupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getContainersToCleanupOrBuilderList() {
        if (containersToCleanupBuilder_ != null) {
          return containersToCleanupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containersToCleanup_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainersToCleanupBuilder() {
        return getContainersToCleanupFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainersToCleanupBuilder(
          int index) {
        return getContainersToCleanupFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_cleanup = 5;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getContainersToCleanupBuilderList() {
        return getContainersToCleanupFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainersToCleanupFieldBuilder() {
        if (containersToCleanupBuilder_ == null) {
          containersToCleanupBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containersToCleanup_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          containersToCleanup_ = null;
        }
        return containersToCleanupBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> applicationsToCleanup_ =
        java.util.Collections.emptyList();
      private void ensureApplicationsToCleanupIsMutable() {
        if (!((bitField0_ & 0x00000020) != 0)) {
          applicationsToCleanup_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto>(applicationsToCleanup_);
          bitField0_ |= 0x00000020;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationsToCleanupBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> getApplicationsToCleanupList() {
        if (applicationsToCleanupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applicationsToCleanup_);
        } else {
          return applicationsToCleanupBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public int getApplicationsToCleanupCount() {
        if (applicationsToCleanupBuilder_ == null) {
          return applicationsToCleanup_.size();
        } else {
          return applicationsToCleanupBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationsToCleanup(int index) {
        if (applicationsToCleanupBuilder_ == null) {
          return applicationsToCleanup_.get(index);
        } else {
          return applicationsToCleanupBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder setApplicationsToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationsToCleanupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.set(index, value);
          onChanged();
        } else {
          applicationsToCleanupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder setApplicationsToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationsToCleanupBuilder_ == null) {
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationsToCleanupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder addApplicationsToCleanup(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationsToCleanupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.add(value);
          onChanged();
        } else {
          applicationsToCleanupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder addApplicationsToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationsToCleanupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.add(index, value);
          onChanged();
        } else {
          applicationsToCleanupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder addApplicationsToCleanup(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationsToCleanupBuilder_ == null) {
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.add(builderForValue.build());
          onChanged();
        } else {
          applicationsToCleanupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder addApplicationsToCleanup(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationsToCleanupBuilder_ == null) {
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationsToCleanupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder addAllApplicationsToCleanup(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto> values) {
        if (applicationsToCleanupBuilder_ == null) {
          ensureApplicationsToCleanupIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, applicationsToCleanup_);
          onChanged();
        } else {
          applicationsToCleanupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder clearApplicationsToCleanup() {
        if (applicationsToCleanupBuilder_ == null) {
          applicationsToCleanup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          applicationsToCleanupBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public Builder removeApplicationsToCleanup(int index) {
        if (applicationsToCleanupBuilder_ == null) {
          ensureApplicationsToCleanupIsMutable();
          applicationsToCleanup_.remove(index);
          onChanged();
        } else {
          applicationsToCleanupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationsToCleanupBuilder(
          int index) {
        return getApplicationsToCleanupFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationsToCleanupOrBuilder(
          int index) {
        if (applicationsToCleanupBuilder_ == null) {
          return applicationsToCleanup_.get(index);  } else {
          return applicationsToCleanupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
           getApplicationsToCleanupOrBuilderList() {
        if (applicationsToCleanupBuilder_ != null) {
          return applicationsToCleanupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applicationsToCleanup_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder addApplicationsToCleanupBuilder() {
        return getApplicationsToCleanupFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder addApplicationsToCleanupBuilder(
          int index) {
        return getApplicationsToCleanupFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationIdProto applications_to_cleanup = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder> 
           getApplicationsToCleanupBuilderList() {
        return getApplicationsToCleanupFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationsToCleanupFieldBuilder() {
        if (applicationsToCleanupBuilder_ == null) {
          applicationsToCleanupBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationsToCleanup_,
                  ((bitField0_ & 0x00000020) != 0),
                  getParentForChildren(),
                  isClean());
          applicationsToCleanup_ = null;
        }
        return applicationsToCleanupBuilder_;
      }

      private long nextHeartBeatInterval_ ;
      /**
       * <code>optional int64 nextHeartBeatInterval = 7;</code>
       */
      public boolean hasNextHeartBeatInterval() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int64 nextHeartBeatInterval = 7;</code>
       */
      public long getNextHeartBeatInterval() {
        return nextHeartBeatInterval_;
      }
      /**
       * <code>optional int64 nextHeartBeatInterval = 7;</code>
       */
      public Builder setNextHeartBeatInterval(long value) {
        bitField0_ |= 0x00000040;
        nextHeartBeatInterval_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 nextHeartBeatInterval = 7;</code>
       */
      public Builder clearNextHeartBeatInterval() {
        bitField0_ = (bitField0_ & ~0x00000040);
        nextHeartBeatInterval_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object diagnosticsMessage_ = "";
      /**
       * <code>optional string diagnostics_message = 8;</code>
       */
      public boolean hasDiagnosticsMessage() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional string diagnostics_message = 8;</code>
       */
      public java.lang.String getDiagnosticsMessage() {
        java.lang.Object ref = diagnosticsMessage_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnosticsMessage_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics_message = 8;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsMessageBytes() {
        java.lang.Object ref = diagnosticsMessage_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnosticsMessage_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics_message = 8;</code>
       */
      public Builder setDiagnosticsMessage(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        diagnosticsMessage_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics_message = 8;</code>
       */
      public Builder clearDiagnosticsMessage() {
        bitField0_ = (bitField0_ & ~0x00000080);
        diagnosticsMessage_ = getDefaultInstance().getDiagnosticsMessage();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics_message = 8;</code>
       */
      public Builder setDiagnosticsMessageBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        diagnosticsMessage_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containersToBeRemovedFromNm_ =
        java.util.Collections.emptyList();
      private void ensureContainersToBeRemovedFromNmIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          containersToBeRemovedFromNm_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(containersToBeRemovedFromNm_);
          bitField0_ |= 0x00000100;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containersToBeRemovedFromNmBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainersToBeRemovedFromNmList() {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containersToBeRemovedFromNm_);
        } else {
          return containersToBeRemovedFromNmBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public int getContainersToBeRemovedFromNmCount() {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          return containersToBeRemovedFromNm_.size();
        } else {
          return containersToBeRemovedFromNmBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainersToBeRemovedFromNm(int index) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          return containersToBeRemovedFromNm_.get(index);
        } else {
          return containersToBeRemovedFromNmBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder setContainersToBeRemovedFromNm(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.set(index, value);
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder setContainersToBeRemovedFromNm(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder addContainersToBeRemovedFromNm(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.add(value);
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder addContainersToBeRemovedFromNm(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.add(index, value);
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder addContainersToBeRemovedFromNm(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.add(builderForValue.build());
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder addContainersToBeRemovedFromNm(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder addAllContainersToBeRemovedFromNm(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          ensureContainersToBeRemovedFromNmIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, containersToBeRemovedFromNm_);
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder clearContainersToBeRemovedFromNm() {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          containersToBeRemovedFromNm_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public Builder removeContainersToBeRemovedFromNm(int index) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          ensureContainersToBeRemovedFromNmIsMutable();
          containersToBeRemovedFromNm_.remove(index);
          onChanged();
        } else {
          containersToBeRemovedFromNmBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainersToBeRemovedFromNmBuilder(
          int index) {
        return getContainersToBeRemovedFromNmFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainersToBeRemovedFromNmOrBuilder(
          int index) {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          return containersToBeRemovedFromNm_.get(index);  } else {
          return containersToBeRemovedFromNmBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getContainersToBeRemovedFromNmOrBuilderList() {
        if (containersToBeRemovedFromNmBuilder_ != null) {
          return containersToBeRemovedFromNmBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containersToBeRemovedFromNm_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainersToBeRemovedFromNmBuilder() {
        return getContainersToBeRemovedFromNmFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainersToBeRemovedFromNmBuilder(
          int index) {
        return getContainersToBeRemovedFromNmFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto containers_to_be_removed_from_nm = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getContainersToBeRemovedFromNmBuilderList() {
        return getContainersToBeRemovedFromNmFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainersToBeRemovedFromNmFieldBuilder() {
        if (containersToBeRemovedFromNmBuilder_ == null) {
          containersToBeRemovedFromNmBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containersToBeRemovedFromNm_,
                  ((bitField0_ & 0x00000100) != 0),
                  getParentForChildren(),
                  isClean());
          containersToBeRemovedFromNm_ = null;
        }
        return containersToBeRemovedFromNmBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto> systemCredentialsForApps_ =
        java.util.Collections.emptyList();
      private void ensureSystemCredentialsForAppsIsMutable() {
        if (!((bitField0_ & 0x00000200) != 0)) {
          systemCredentialsForApps_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto>(systemCredentialsForApps_);
          bitField0_ |= 0x00000200;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder> systemCredentialsForAppsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto> getSystemCredentialsForAppsList() {
        if (systemCredentialsForAppsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(systemCredentialsForApps_);
        } else {
          return systemCredentialsForAppsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public int getSystemCredentialsForAppsCount() {
        if (systemCredentialsForAppsBuilder_ == null) {
          return systemCredentialsForApps_.size();
        } else {
          return systemCredentialsForAppsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto getSystemCredentialsForApps(int index) {
        if (systemCredentialsForAppsBuilder_ == null) {
          return systemCredentialsForApps_.get(index);
        } else {
          return systemCredentialsForAppsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder setSystemCredentialsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto value) {
        if (systemCredentialsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.set(index, value);
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder setSystemCredentialsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder builderForValue) {
        if (systemCredentialsForAppsBuilder_ == null) {
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.set(index, builderForValue.build());
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder addSystemCredentialsForApps(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto value) {
        if (systemCredentialsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.add(value);
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder addSystemCredentialsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto value) {
        if (systemCredentialsForAppsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.add(index, value);
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder addSystemCredentialsForApps(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder builderForValue) {
        if (systemCredentialsForAppsBuilder_ == null) {
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.add(builderForValue.build());
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder addSystemCredentialsForApps(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder builderForValue) {
        if (systemCredentialsForAppsBuilder_ == null) {
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.add(index, builderForValue.build());
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder addAllSystemCredentialsForApps(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto> values) {
        if (systemCredentialsForAppsBuilder_ == null) {
          ensureSystemCredentialsForAppsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, systemCredentialsForApps_);
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder clearSystemCredentialsForApps() {
        if (systemCredentialsForAppsBuilder_ == null) {
          systemCredentialsForApps_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public Builder removeSystemCredentialsForApps(int index) {
        if (systemCredentialsForAppsBuilder_ == null) {
          ensureSystemCredentialsForAppsIsMutable();
          systemCredentialsForApps_.remove(index);
          onChanged();
        } else {
          systemCredentialsForAppsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder getSystemCredentialsForAppsBuilder(
          int index) {
        return getSystemCredentialsForAppsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder getSystemCredentialsForAppsOrBuilder(
          int index) {
        if (systemCredentialsForAppsBuilder_ == null) {
          return systemCredentialsForApps_.get(index);  } else {
          return systemCredentialsForAppsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder> 
           getSystemCredentialsForAppsOrBuilderList() {
        if (systemCredentialsForAppsBuilder_ != null) {
          return systemCredentialsForAppsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(systemCredentialsForApps_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder addSystemCredentialsForAppsBuilder() {
        return getSystemCredentialsForAppsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder addSystemCredentialsForAppsBuilder(
          int index) {
        return getSystemCredentialsForAppsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.SystemCredentialsForAppsProto system_credentials_for_apps = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder> 
           getSystemCredentialsForAppsBuilderList() {
        return getSystemCredentialsForAppsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder> 
          getSystemCredentialsForAppsFieldBuilder() {
        if (systemCredentialsForAppsBuilder_ == null) {
          systemCredentialsForAppsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder>(
                  systemCredentialsForApps_,
                  ((bitField0_ & 0x00000200) != 0),
                  getParentForChildren(),
                  isClean());
          systemCredentialsForApps_ = null;
        }
        return systemCredentialsForAppsBuilder_;
      }

      private boolean areNodeLabelsAcceptedByRM_ ;
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
       */
      public boolean hasAreNodeLabelsAcceptedByRM() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
       */
      public boolean getAreNodeLabelsAcceptedByRM() {
        return areNodeLabelsAcceptedByRM_;
      }
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
       */
      public Builder setAreNodeLabelsAcceptedByRM(boolean value) {
        bitField0_ |= 0x00000400;
        areNodeLabelsAcceptedByRM_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool areNodeLabelsAcceptedByRM = 11 [default = false];</code>
       */
      public Builder clearAreNodeLabelsAcceptedByRM() {
        bitField0_ = (bitField0_ & ~0x00000400);
        areNodeLabelsAcceptedByRM_ = false;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> containersToDecrease_ =
        java.util.Collections.emptyList();
      private void ensureContainersToDecreaseIsMutable() {
        if (!((bitField0_ & 0x00000800) != 0)) {
          containersToDecrease_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>(containersToDecrease_);
          bitField0_ |= 0x00000800;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> containersToDecreaseBuilder_;

      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getContainersToDecreaseList() {
        if (containersToDecreaseBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containersToDecrease_);
        } else {
          return containersToDecreaseBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public int getContainersToDecreaseCount() {
        if (containersToDecreaseBuilder_ == null) {
          return containersToDecrease_.size();
        } else {
          return containersToDecreaseBuilder_.getCount();
        }
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersToDecrease(int index) {
        if (containersToDecreaseBuilder_ == null) {
          return containersToDecrease_.get(index);
        } else {
          return containersToDecreaseBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder setContainersToDecrease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersToDecreaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.set(index, value);
          onChanged();
        } else {
          containersToDecreaseBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder setContainersToDecrease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersToDecreaseBuilder_ == null) {
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersToDecreaseBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder addContainersToDecrease(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersToDecreaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.add(value);
          onChanged();
        } else {
          containersToDecreaseBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder addContainersToDecrease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersToDecreaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.add(index, value);
          onChanged();
        } else {
          containersToDecreaseBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder addContainersToDecrease(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersToDecreaseBuilder_ == null) {
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.add(builderForValue.build());
          onChanged();
        } else {
          containersToDecreaseBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder addContainersToDecrease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersToDecreaseBuilder_ == null) {
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersToDecreaseBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder addAllContainersToDecrease(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> values) {
        if (containersToDecreaseBuilder_ == null) {
          ensureContainersToDecreaseIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, containersToDecrease_);
          onChanged();
        } else {
          containersToDecreaseBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder clearContainersToDecrease() {
        if (containersToDecreaseBuilder_ == null) {
          containersToDecrease_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000800);
          onChanged();
        } else {
          containersToDecreaseBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public Builder removeContainersToDecrease(int index) {
        if (containersToDecreaseBuilder_ == null) {
          ensureContainersToDecreaseIsMutable();
          containersToDecrease_.remove(index);
          onChanged();
        } else {
          containersToDecreaseBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getContainersToDecreaseBuilder(
          int index) {
        return getContainersToDecreaseFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersToDecreaseOrBuilder(
          int index) {
        if (containersToDecreaseBuilder_ == null) {
          return containersToDecrease_.get(index);  } else {
          return containersToDecreaseBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
           getContainersToDecreaseOrBuilderList() {
        if (containersToDecreaseBuilder_ != null) {
          return containersToDecreaseBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containersToDecrease_);
        }
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addContainersToDecreaseBuilder() {
        return getContainersToDecreaseFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addContainersToDecreaseBuilder(
          int index) {
        return getContainersToDecreaseFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <pre>
       * to be deprecated in favour of containers_to_update
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_decrease = 12;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder> 
           getContainersToDecreaseBuilderList() {
        return getContainersToDecreaseFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getContainersToDecreaseFieldBuilder() {
        if (containersToDecreaseBuilder_ == null) {
          containersToDecreaseBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  containersToDecrease_,
                  ((bitField0_ & 0x00000800) != 0),
                  getParentForChildren(),
                  isClean());
          containersToDecrease_ = null;
        }
        return containersToDecreaseBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto> containersToSignal_ =
        java.util.Collections.emptyList();
      private void ensureContainersToSignalIsMutable() {
        if (!((bitField0_ & 0x00001000) != 0)) {
          containersToSignal_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto>(containersToSignal_);
          bitField0_ |= 0x00001000;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder> containersToSignalBuilder_;

      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto> getContainersToSignalList() {
        if (containersToSignalBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containersToSignal_);
        } else {
          return containersToSignalBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public int getContainersToSignalCount() {
        if (containersToSignalBuilder_ == null) {
          return containersToSignal_.size();
        } else {
          return containersToSignalBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto getContainersToSignal(int index) {
        if (containersToSignalBuilder_ == null) {
          return containersToSignal_.get(index);
        } else {
          return containersToSignalBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder setContainersToSignal(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto value) {
        if (containersToSignalBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToSignalIsMutable();
          containersToSignal_.set(index, value);
          onChanged();
        } else {
          containersToSignalBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder setContainersToSignal(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder builderForValue) {
        if (containersToSignalBuilder_ == null) {
          ensureContainersToSignalIsMutable();
          containersToSignal_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersToSignalBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder addContainersToSignal(org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto value) {
        if (containersToSignalBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToSignalIsMutable();
          containersToSignal_.add(value);
          onChanged();
        } else {
          containersToSignalBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder addContainersToSignal(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto value) {
        if (containersToSignalBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToSignalIsMutable();
          containersToSignal_.add(index, value);
          onChanged();
        } else {
          containersToSignalBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder addContainersToSignal(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder builderForValue) {
        if (containersToSignalBuilder_ == null) {
          ensureContainersToSignalIsMutable();
          containersToSignal_.add(builderForValue.build());
          onChanged();
        } else {
          containersToSignalBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder addContainersToSignal(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder builderForValue) {
        if (containersToSignalBuilder_ == null) {
          ensureContainersToSignalIsMutable();
          containersToSignal_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersToSignalBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder addAllContainersToSignal(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto> values) {
        if (containersToSignalBuilder_ == null) {
          ensureContainersToSignalIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, containersToSignal_);
          onChanged();
        } else {
          containersToSignalBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder clearContainersToSignal() {
        if (containersToSignalBuilder_ == null) {
          containersToSignal_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00001000);
          onChanged();
        } else {
          containersToSignalBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public Builder removeContainersToSignal(int index) {
        if (containersToSignalBuilder_ == null) {
          ensureContainersToSignalIsMutable();
          containersToSignal_.remove(index);
          onChanged();
        } else {
          containersToSignalBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder getContainersToSignalBuilder(
          int index) {
        return getContainersToSignalFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder getContainersToSignalOrBuilder(
          int index) {
        if (containersToSignalBuilder_ == null) {
          return containersToSignal_.get(index);  } else {
          return containersToSignalBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder> 
           getContainersToSignalOrBuilderList() {
        if (containersToSignalBuilder_ != null) {
          return containersToSignalBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containersToSignal_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder addContainersToSignalBuilder() {
        return getContainersToSignalFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder addContainersToSignalBuilder(
          int index) {
        return getContainersToSignalFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.SignalContainerRequestProto containers_to_signal = 13;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder> 
           getContainersToSignalBuilderList() {
        return getContainersToSignalFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder> 
          getContainersToSignalFieldBuilder() {
        if (containersToSignalBuilder_ == null) {
          containersToSignalBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder>(
                  containersToSignal_,
                  ((bitField0_ & 0x00001000) != 0),
                  getParentForChildren(),
                  isClean());
          containersToSignal_ = null;
        }
        return containersToSignalBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> resourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public boolean hasResource() {
        return ((bitField0_ & 0x00002000) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
        if (resourceBuilder_ == null) {
          return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        } else {
          return resourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public Builder setResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          resource_ = value;
          onChanged();
        } else {
          resourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00002000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public Builder setResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (resourceBuilder_ == null) {
          resource_ = builderForValue.build();
          onChanged();
        } else {
          resourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00002000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public Builder mergeResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (((bitField0_ & 0x00002000) != 0) &&
              resource_ != null &&
              resource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            resource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(resource_).mergeFrom(value).buildPartial();
          } else {
            resource_ = value;
          }
          onChanged();
        } else {
          resourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00002000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public Builder clearResource() {
        if (resourceBuilder_ == null) {
          resource_ = null;
          onChanged();
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00002000);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getResourceBuilder() {
        bitField0_ |= 0x00002000;
        onChanged();
        return getResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
        if (resourceBuilder_ != null) {
          return resourceBuilder_.getMessageOrBuilder();
        } else {
          return resource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 14;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getResourceFieldBuilder() {
        if (resourceBuilder_ == null) {
          resourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getResource(),
                  getParentForChildren(),
                  isClean());
          resource_ = null;
        }
        return resourceBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto containerQueuingLimit_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder> containerQueuingLimitBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public boolean hasContainerQueuingLimit() {
        return ((bitField0_ & 0x00004000) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto getContainerQueuingLimit() {
        if (containerQueuingLimitBuilder_ == null) {
          return containerQueuingLimit_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance() : containerQueuingLimit_;
        } else {
          return containerQueuingLimitBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public Builder setContainerQueuingLimit(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto value) {
        if (containerQueuingLimitBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerQueuingLimit_ = value;
          onChanged();
        } else {
          containerQueuingLimitBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public Builder setContainerQueuingLimit(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder builderForValue) {
        if (containerQueuingLimitBuilder_ == null) {
          containerQueuingLimit_ = builderForValue.build();
          onChanged();
        } else {
          containerQueuingLimitBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public Builder mergeContainerQueuingLimit(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto value) {
        if (containerQueuingLimitBuilder_ == null) {
          if (((bitField0_ & 0x00004000) != 0) &&
              containerQueuingLimit_ != null &&
              containerQueuingLimit_ != org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance()) {
            containerQueuingLimit_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.newBuilder(containerQueuingLimit_).mergeFrom(value).buildPartial();
          } else {
            containerQueuingLimit_ = value;
          }
          onChanged();
        } else {
          containerQueuingLimitBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00004000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public Builder clearContainerQueuingLimit() {
        if (containerQueuingLimitBuilder_ == null) {
          containerQueuingLimit_ = null;
          onChanged();
        } else {
          containerQueuingLimitBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00004000);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder getContainerQueuingLimitBuilder() {
        bitField0_ |= 0x00004000;
        onChanged();
        return getContainerQueuingLimitFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder getContainerQueuingLimitOrBuilder() {
        if (containerQueuingLimitBuilder_ != null) {
          return containerQueuingLimitBuilder_.getMessageOrBuilder();
        } else {
          return containerQueuingLimit_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance() : containerQueuingLimit_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerQueuingLimitProto container_queuing_limit = 15;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder> 
          getContainerQueuingLimitFieldBuilder() {
        if (containerQueuingLimitBuilder_ == null) {
          containerQueuingLimitBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder>(
                  getContainerQueuingLimit(),
                  getParentForChildren(),
                  isClean());
          containerQueuingLimit_ = null;
        }
        return containerQueuingLimitBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> appCollectors_ =
        java.util.Collections.emptyList();
      private void ensureAppCollectorsIsMutable() {
        if (!((bitField0_ & 0x00008000) != 0)) {
          appCollectors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto>(appCollectors_);
          bitField0_ |= 0x00008000;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> appCollectorsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> getAppCollectorsList() {
        if (appCollectorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(appCollectors_);
        } else {
          return appCollectorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public int getAppCollectorsCount() {
        if (appCollectorsBuilder_ == null) {
          return appCollectors_.size();
        } else {
          return appCollectorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getAppCollectors(int index) {
        if (appCollectorsBuilder_ == null) {
          return appCollectors_.get(index);
        } else {
          return appCollectorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder setAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (appCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAppCollectorsIsMutable();
          appCollectors_.set(index, value);
          onChanged();
        } else {
          appCollectorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder setAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.set(index, builderForValue.build());
          onChanged();
        } else {
          appCollectorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder addAppCollectors(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (appCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAppCollectorsIsMutable();
          appCollectors_.add(value);
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder addAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (appCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAppCollectorsIsMutable();
          appCollectors_.add(index, value);
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder addAppCollectors(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.add(builderForValue.build());
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder addAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.add(index, builderForValue.build());
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder addAllAppCollectors(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> values) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, appCollectors_);
          onChanged();
        } else {
          appCollectorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder clearAppCollectors() {
        if (appCollectorsBuilder_ == null) {
          appCollectors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00008000);
          onChanged();
        } else {
          appCollectorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public Builder removeAppCollectors(int index) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.remove(index);
          onChanged();
        } else {
          appCollectorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder getAppCollectorsBuilder(
          int index) {
        return getAppCollectorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getAppCollectorsOrBuilder(
          int index) {
        if (appCollectorsBuilder_ == null) {
          return appCollectors_.get(index);  } else {
          return appCollectorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
           getAppCollectorsOrBuilderList() {
        if (appCollectorsBuilder_ != null) {
          return appCollectorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(appCollectors_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder addAppCollectorsBuilder() {
        return getAppCollectorsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder addAppCollectorsBuilder(
          int index) {
        return getAppCollectorsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 16;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder> 
           getAppCollectorsBuilderList() {
        return getAppCollectorsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
          getAppCollectorsFieldBuilder() {
        if (appCollectorsBuilder_ == null) {
          appCollectorsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder>(
                  appCollectors_,
                  ((bitField0_ & 0x00008000) != 0),
                  getParentForChildren(),
                  isClean());
          appCollectors_ = null;
        }
        return appCollectorsBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> containersToUpdate_ =
        java.util.Collections.emptyList();
      private void ensureContainersToUpdateIsMutable() {
        if (!((bitField0_ & 0x00010000) != 0)) {
          containersToUpdate_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>(containersToUpdate_);
          bitField0_ |= 0x00010000;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> containersToUpdateBuilder_;

      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getContainersToUpdateList() {
        if (containersToUpdateBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containersToUpdate_);
        } else {
          return containersToUpdateBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public int getContainersToUpdateCount() {
        if (containersToUpdateBuilder_ == null) {
          return containersToUpdate_.size();
        } else {
          return containersToUpdateBuilder_.getCount();
        }
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersToUpdate(int index) {
        if (containersToUpdateBuilder_ == null) {
          return containersToUpdate_.get(index);
        } else {
          return containersToUpdateBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder setContainersToUpdate(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersToUpdateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.set(index, value);
          onChanged();
        } else {
          containersToUpdateBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder setContainersToUpdate(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersToUpdateBuilder_ == null) {
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersToUpdateBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder addContainersToUpdate(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersToUpdateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.add(value);
          onChanged();
        } else {
          containersToUpdateBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder addContainersToUpdate(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersToUpdateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.add(index, value);
          onChanged();
        } else {
          containersToUpdateBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder addContainersToUpdate(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersToUpdateBuilder_ == null) {
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.add(builderForValue.build());
          onChanged();
        } else {
          containersToUpdateBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder addContainersToUpdate(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersToUpdateBuilder_ == null) {
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersToUpdateBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder addAllContainersToUpdate(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> values) {
        if (containersToUpdateBuilder_ == null) {
          ensureContainersToUpdateIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, containersToUpdate_);
          onChanged();
        } else {
          containersToUpdateBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder clearContainersToUpdate() {
        if (containersToUpdateBuilder_ == null) {
          containersToUpdate_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00010000);
          onChanged();
        } else {
          containersToUpdateBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public Builder removeContainersToUpdate(int index) {
        if (containersToUpdateBuilder_ == null) {
          ensureContainersToUpdateIsMutable();
          containersToUpdate_.remove(index);
          onChanged();
        } else {
          containersToUpdateBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getContainersToUpdateBuilder(
          int index) {
        return getContainersToUpdateFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersToUpdateOrBuilder(
          int index) {
        if (containersToUpdateBuilder_ == null) {
          return containersToUpdate_.get(index);  } else {
          return containersToUpdateBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
           getContainersToUpdateOrBuilderList() {
        if (containersToUpdateBuilder_ != null) {
          return containersToUpdateBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containersToUpdate_);
        }
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addContainersToUpdateBuilder() {
        return getContainersToUpdateFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addContainersToUpdateBuilder(
          int index) {
        return getContainersToUpdateFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <pre>
       * to be used in place of containers_to_decrease
       * </pre>
       *
       * <code>repeated .hadoop.yarn.ContainerProto containers_to_update = 17;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder> 
           getContainersToUpdateBuilderList() {
        return getContainersToUpdateFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getContainersToUpdateFieldBuilder() {
        if (containersToUpdateBuilder_ == null) {
          containersToUpdateBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  containersToUpdate_,
                  ((bitField0_ & 0x00010000) != 0),
                  getParentForChildren(),
                  isClean());
          containersToUpdate_ = null;
        }
        return containersToUpdateBuilder_;
      }

      private boolean areNodeAttributesAcceptedByRM_ ;
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
       */
      public boolean hasAreNodeAttributesAcceptedByRM() {
        return ((bitField0_ & 0x00020000) != 0);
      }
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
       */
      public boolean getAreNodeAttributesAcceptedByRM() {
        return areNodeAttributesAcceptedByRM_;
      }
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
       */
      public Builder setAreNodeAttributesAcceptedByRM(boolean value) {
        bitField0_ |= 0x00020000;
        areNodeAttributesAcceptedByRM_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool areNodeAttributesAcceptedByRM = 18 [default = false];</code>
       */
      public Builder clearAreNodeAttributesAcceptedByRM() {
        bitField0_ = (bitField0_ & ~0x00020000);
        areNodeAttributesAcceptedByRM_ = false;
        onChanged();
        return this;
      }

      private long tokenSequenceNo_ ;
      /**
       * <code>optional int64 tokenSequenceNo = 19;</code>
       */
      public boolean hasTokenSequenceNo() {
        return ((bitField0_ & 0x00040000) != 0);
      }
      /**
       * <code>optional int64 tokenSequenceNo = 19;</code>
       */
      public long getTokenSequenceNo() {
        return tokenSequenceNo_;
      }
      /**
       * <code>optional int64 tokenSequenceNo = 19;</code>
       */
      public Builder setTokenSequenceNo(long value) {
        bitField0_ |= 0x00040000;
        tokenSequenceNo_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 tokenSequenceNo = 19;</code>
       */
      public Builder clearTokenSequenceNo() {
        bitField0_ = (bitField0_ & ~0x00040000);
        tokenSequenceNo_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NodeHeartbeatResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NodeHeartbeatResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<NodeHeartbeatResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<NodeHeartbeatResponseProto>() {
      @java.lang.Override
      public NodeHeartbeatResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new NodeHeartbeatResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<NodeHeartbeatResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<NodeHeartbeatResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ContainerQueuingLimitProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.ContainerQueuingLimitProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional int32 max_queue_length = 1;</code>
     */
    boolean hasMaxQueueLength();
    /**
     * <code>optional int32 max_queue_length = 1;</code>
     */
    int getMaxQueueLength();

    /**
     * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
     */
    boolean hasMaxQueueWaitTimeInMs();
    /**
     * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
     */
    int getMaxQueueWaitTimeInMs();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ContainerQueuingLimitProto}
   */
  public  static final class ContainerQueuingLimitProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.ContainerQueuingLimitProto)
      ContainerQueuingLimitProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ContainerQueuingLimitProto.newBuilder() to construct.
    private ContainerQueuingLimitProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ContainerQueuingLimitProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ContainerQueuingLimitProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              maxQueueLength_ = input.readInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              maxQueueWaitTimeInMs_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ContainerQueuingLimitProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ContainerQueuingLimitProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder.class);
    }

    private int bitField0_;
    public static final int MAX_QUEUE_LENGTH_FIELD_NUMBER = 1;
    private int maxQueueLength_;
    /**
     * <code>optional int32 max_queue_length = 1;</code>
     */
    public boolean hasMaxQueueLength() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int32 max_queue_length = 1;</code>
     */
    public int getMaxQueueLength() {
      return maxQueueLength_;
    }

    public static final int MAX_QUEUE_WAIT_TIME_IN_MS_FIELD_NUMBER = 2;
    private int maxQueueWaitTimeInMs_;
    /**
     * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
     */
    public boolean hasMaxQueueWaitTimeInMs() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
     */
    public int getMaxQueueWaitTimeInMs() {
      return maxQueueWaitTimeInMs_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt32(1, maxQueueLength_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, maxQueueWaitTimeInMs_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(1, maxQueueLength_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(2, maxQueueWaitTimeInMs_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto) obj;

      if (hasMaxQueueLength() != other.hasMaxQueueLength()) return false;
      if (hasMaxQueueLength()) {
        if (getMaxQueueLength()
            != other.getMaxQueueLength()) return false;
      }
      if (hasMaxQueueWaitTimeInMs() != other.hasMaxQueueWaitTimeInMs()) return false;
      if (hasMaxQueueWaitTimeInMs()) {
        if (getMaxQueueWaitTimeInMs()
            != other.getMaxQueueWaitTimeInMs()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasMaxQueueLength()) {
        hash = (37 * hash) + MAX_QUEUE_LENGTH_FIELD_NUMBER;
        hash = (53 * hash) + getMaxQueueLength();
      }
      if (hasMaxQueueWaitTimeInMs()) {
        hash = (37 * hash) + MAX_QUEUE_WAIT_TIME_IN_MS_FIELD_NUMBER;
        hash = (53 * hash) + getMaxQueueWaitTimeInMs();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ContainerQueuingLimitProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.ContainerQueuingLimitProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ContainerQueuingLimitProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ContainerQueuingLimitProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        maxQueueLength_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        maxQueueWaitTimeInMs_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ContainerQueuingLimitProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.maxQueueLength_ = maxQueueLength_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.maxQueueWaitTimeInMs_ = maxQueueWaitTimeInMs_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto.getDefaultInstance()) return this;
        if (other.hasMaxQueueLength()) {
          setMaxQueueLength(other.getMaxQueueLength());
        }
        if (other.hasMaxQueueWaitTimeInMs()) {
          setMaxQueueWaitTimeInMs(other.getMaxQueueWaitTimeInMs());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int maxQueueLength_ ;
      /**
       * <code>optional int32 max_queue_length = 1;</code>
       */
      public boolean hasMaxQueueLength() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int32 max_queue_length = 1;</code>
       */
      public int getMaxQueueLength() {
        return maxQueueLength_;
      }
      /**
       * <code>optional int32 max_queue_length = 1;</code>
       */
      public Builder setMaxQueueLength(int value) {
        bitField0_ |= 0x00000001;
        maxQueueLength_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 max_queue_length = 1;</code>
       */
      public Builder clearMaxQueueLength() {
        bitField0_ = (bitField0_ & ~0x00000001);
        maxQueueLength_ = 0;
        onChanged();
        return this;
      }

      private int maxQueueWaitTimeInMs_ ;
      /**
       * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
       */
      public boolean hasMaxQueueWaitTimeInMs() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
       */
      public int getMaxQueueWaitTimeInMs() {
        return maxQueueWaitTimeInMs_;
      }
      /**
       * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
       */
      public Builder setMaxQueueWaitTimeInMs(int value) {
        bitField0_ |= 0x00000002;
        maxQueueWaitTimeInMs_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 max_queue_wait_time_in_ms = 2;</code>
       */
      public Builder clearMaxQueueWaitTimeInMs() {
        bitField0_ = (bitField0_ & ~0x00000002);
        maxQueueWaitTimeInMs_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ContainerQueuingLimitProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ContainerQueuingLimitProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<ContainerQueuingLimitProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<ContainerQueuingLimitProto>() {
      @java.lang.Override
      public ContainerQueuingLimitProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new ContainerQueuingLimitProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<ContainerQueuingLimitProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<ContainerQueuingLimitProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ContainerQueuingLimitProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SystemCredentialsForAppsProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.SystemCredentialsForAppsProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    boolean hasAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder();

    /**
     * <code>optional bytes credentialsForApp = 2;</code>
     */
    boolean hasCredentialsForApp();
    /**
     * <code>optional bytes credentialsForApp = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString getCredentialsForApp();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SystemCredentialsForAppsProto}
   */
  public  static final class SystemCredentialsForAppsProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.SystemCredentialsForAppsProto)
      SystemCredentialsForAppsProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SystemCredentialsForAppsProto.newBuilder() to construct.
    private SystemCredentialsForAppsProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SystemCredentialsForAppsProto() {
      credentialsForApp_ = org.apache.hadoop.thirdparty.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SystemCredentialsForAppsProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = appId_.toBuilder();
              }
              appId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(appId_);
                appId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              credentialsForApp_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SystemCredentialsForAppsProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SystemCredentialsForAppsProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder.class);
    }

    private int bitField0_;
    public static final int APPID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    public boolean hasAppId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }

    public static final int CREDENTIALSFORAPP_FIELD_NUMBER = 2;
    private org.apache.hadoop.thirdparty.protobuf.ByteString credentialsForApp_;
    /**
     * <code>optional bytes credentialsForApp = 2;</code>
     */
    public boolean hasCredentialsForApp() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional bytes credentialsForApp = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString getCredentialsForApp() {
      return credentialsForApp_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAppId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeBytes(2, credentialsForApp_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAppId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBytesSize(2, credentialsForApp_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto) obj;

      if (hasAppId() != other.hasAppId()) return false;
      if (hasAppId()) {
        if (!getAppId()
            .equals(other.getAppId())) return false;
      }
      if (hasCredentialsForApp() != other.hasCredentialsForApp()) return false;
      if (hasCredentialsForApp()) {
        if (!getCredentialsForApp()
            .equals(other.getCredentialsForApp())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAppId()) {
        hash = (37 * hash) + APPID_FIELD_NUMBER;
        hash = (53 * hash) + getAppId().hashCode();
      }
      if (hasCredentialsForApp()) {
        hash = (37 * hash) + CREDENTIALSFORAPP_FIELD_NUMBER;
        hash = (53 * hash) + getCredentialsForApp().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SystemCredentialsForAppsProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.SystemCredentialsForAppsProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SystemCredentialsForAppsProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SystemCredentialsForAppsProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAppIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (appIdBuilder_ == null) {
          appId_ = null;
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        credentialsForApp_ = org.apache.hadoop.thirdparty.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SystemCredentialsForAppsProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (appIdBuilder_ == null) {
            result.appId_ = appId_;
          } else {
            result.appId_ = appIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.credentialsForApp_ = credentialsForApp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto.getDefaultInstance()) return this;
        if (other.hasAppId()) {
          mergeAppId(other.getAppId());
        }
        if (other.hasCredentialsForApp()) {
          setCredentialsForApp(other.getCredentialsForApp());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> appIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public boolean hasAppId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
        if (appIdBuilder_ == null) {
          return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        } else {
          return appIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder setAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          appId_ = value;
          onChanged();
        } else {
          appIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder setAppId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (appIdBuilder_ == null) {
          appId_ = builderForValue.build();
          onChanged();
        } else {
          appIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder mergeAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              appId_ != null &&
              appId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            appId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(appId_).mergeFrom(value).buildPartial();
          } else {
            appId_ = value;
          }
          onChanged();
        } else {
          appIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder clearAppId() {
        if (appIdBuilder_ == null) {
          appId_ = null;
          onChanged();
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getAppIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAppIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
        if (appIdBuilder_ != null) {
          return appIdBuilder_.getMessageOrBuilder();
        } else {
          return appId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getAppIdFieldBuilder() {
        if (appIdBuilder_ == null) {
          appIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  getAppId(),
                  getParentForChildren(),
                  isClean());
          appId_ = null;
        }
        return appIdBuilder_;
      }

      private org.apache.hadoop.thirdparty.protobuf.ByteString credentialsForApp_ = org.apache.hadoop.thirdparty.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes credentialsForApp = 2;</code>
       */
      public boolean hasCredentialsForApp() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional bytes credentialsForApp = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString getCredentialsForApp() {
        return credentialsForApp_;
      }
      /**
       * <code>optional bytes credentialsForApp = 2;</code>
       */
      public Builder setCredentialsForApp(org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        credentialsForApp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes credentialsForApp = 2;</code>
       */
      public Builder clearCredentialsForApp() {
        bitField0_ = (bitField0_ & ~0x00000002);
        credentialsForApp_ = getDefaultInstance().getCredentialsForApp();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SystemCredentialsForAppsProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SystemCredentialsForAppsProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<SystemCredentialsForAppsProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<SystemCredentialsForAppsProto>() {
      @java.lang.Override
      public SystemCredentialsForAppsProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new SystemCredentialsForAppsProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<SystemCredentialsForAppsProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<SystemCredentialsForAppsProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SystemCredentialsForAppsProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface AppCollectorDataProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.AppCollectorDataProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    boolean hasAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder();

    /**
     * <code>optional string app_collector_addr = 2;</code>
     */
    boolean hasAppCollectorAddr();
    /**
     * <code>optional string app_collector_addr = 2;</code>
     */
    java.lang.String getAppCollectorAddr();
    /**
     * <code>optional string app_collector_addr = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getAppCollectorAddrBytes();

    /**
     * <code>optional int64 rm_identifier = 3 [default = -1];</code>
     */
    boolean hasRmIdentifier();
    /**
     * <code>optional int64 rm_identifier = 3 [default = -1];</code>
     */
    long getRmIdentifier();

    /**
     * <code>optional int64 version = 4 [default = -1];</code>
     */
    boolean hasVersion();
    /**
     * <code>optional int64 version = 4 [default = -1];</code>
     */
    long getVersion();

    /**
     * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
     */
    boolean hasAppCollectorToken();
    /**
     * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getAppCollectorToken();
    /**
     * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getAppCollectorTokenOrBuilder();
  }
  /**
   * <pre>
   *&#47;/////////////////////////////////////////////////////////////////////
   * //// From collector_nodemanager_protocol ////////////////////////////
   * //////////////////////////////////////////////////////////////////////
   * </pre>
   *
   * Protobuf type {@code hadoop.yarn.AppCollectorDataProto}
   */
  public  static final class AppCollectorDataProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.AppCollectorDataProto)
      AppCollectorDataProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use AppCollectorDataProto.newBuilder() to construct.
    private AppCollectorDataProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private AppCollectorDataProto() {
      appCollectorAddr_ = "";
      rmIdentifier_ = -1L;
      version_ = -1L;
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private AppCollectorDataProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = appId_.toBuilder();
              }
              appId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(appId_);
                appId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              appCollectorAddr_ = bs;
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              rmIdentifier_ = input.readInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              version_ = input.readInt64();
              break;
            }
            case 42: {
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) != 0)) {
                subBuilder = appCollectorToken_.toBuilder();
              }
              appCollectorToken_ = input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(appCollectorToken_);
                appCollectorToken_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_AppCollectorDataProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_AppCollectorDataProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder.class);
    }

    private int bitField0_;
    public static final int APP_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    public boolean hasAppId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }

    public static final int APP_COLLECTOR_ADDR_FIELD_NUMBER = 2;
    private volatile java.lang.Object appCollectorAddr_;
    /**
     * <code>optional string app_collector_addr = 2;</code>
     */
    public boolean hasAppCollectorAddr() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string app_collector_addr = 2;</code>
     */
    public java.lang.String getAppCollectorAddr() {
      java.lang.Object ref = appCollectorAddr_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          appCollectorAddr_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string app_collector_addr = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getAppCollectorAddrBytes() {
      java.lang.Object ref = appCollectorAddr_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        appCollectorAddr_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int RM_IDENTIFIER_FIELD_NUMBER = 3;
    private long rmIdentifier_;
    /**
     * <code>optional int64 rm_identifier = 3 [default = -1];</code>
     */
    public boolean hasRmIdentifier() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int64 rm_identifier = 3 [default = -1];</code>
     */
    public long getRmIdentifier() {
      return rmIdentifier_;
    }

    public static final int VERSION_FIELD_NUMBER = 4;
    private long version_;
    /**
     * <code>optional int64 version = 4 [default = -1];</code>
     */
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 version = 4 [default = -1];</code>
     */
    public long getVersion() {
      return version_;
    }

    public static final int APP_COLLECTOR_TOKEN_FIELD_NUMBER = 5;
    private org.apache.hadoop.security.proto.SecurityProtos.TokenProto appCollectorToken_;
    /**
     * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
     */
    public boolean hasAppCollectorToken() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getAppCollectorToken() {
      return appCollectorToken_ == null ? org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance() : appCollectorToken_;
    }
    /**
     * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getAppCollectorTokenOrBuilder() {
      return appCollectorToken_ == null ? org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance() : appCollectorToken_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasAppCollectorToken()) {
        if (!getAppCollectorToken().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAppId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 2, appCollectorAddr_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt64(3, rmIdentifier_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, version_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeMessage(5, getAppCollectorToken());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAppId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(2, appCollectorAddr_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(3, rmIdentifier_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(4, version_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(5, getAppCollectorToken());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto) obj;

      if (hasAppId() != other.hasAppId()) return false;
      if (hasAppId()) {
        if (!getAppId()
            .equals(other.getAppId())) return false;
      }
      if (hasAppCollectorAddr() != other.hasAppCollectorAddr()) return false;
      if (hasAppCollectorAddr()) {
        if (!getAppCollectorAddr()
            .equals(other.getAppCollectorAddr())) return false;
      }
      if (hasRmIdentifier() != other.hasRmIdentifier()) return false;
      if (hasRmIdentifier()) {
        if (getRmIdentifier()
            != other.getRmIdentifier()) return false;
      }
      if (hasVersion() != other.hasVersion()) return false;
      if (hasVersion()) {
        if (getVersion()
            != other.getVersion()) return false;
      }
      if (hasAppCollectorToken() != other.hasAppCollectorToken()) return false;
      if (hasAppCollectorToken()) {
        if (!getAppCollectorToken()
            .equals(other.getAppCollectorToken())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAppId()) {
        hash = (37 * hash) + APP_ID_FIELD_NUMBER;
        hash = (53 * hash) + getAppId().hashCode();
      }
      if (hasAppCollectorAddr()) {
        hash = (37 * hash) + APP_COLLECTOR_ADDR_FIELD_NUMBER;
        hash = (53 * hash) + getAppCollectorAddr().hashCode();
      }
      if (hasRmIdentifier()) {
        hash = (37 * hash) + RM_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getRmIdentifier());
      }
      if (hasVersion()) {
        hash = (37 * hash) + VERSION_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getVersion());
      }
      if (hasAppCollectorToken()) {
        hash = (37 * hash) + APP_COLLECTOR_TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getAppCollectorToken().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#47;/////////////////////////////////////////////////////////////////////
     * //// From collector_nodemanager_protocol ////////////////////////////
     * //////////////////////////////////////////////////////////////////////
     * </pre>
     *
     * Protobuf type {@code hadoop.yarn.AppCollectorDataProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.AppCollectorDataProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_AppCollectorDataProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_AppCollectorDataProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAppIdFieldBuilder();
          getAppCollectorTokenFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (appIdBuilder_ == null) {
          appId_ = null;
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        appCollectorAddr_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        rmIdentifier_ = -1L;
        bitField0_ = (bitField0_ & ~0x00000004);
        version_ = -1L;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (appCollectorTokenBuilder_ == null) {
          appCollectorToken_ = null;
        } else {
          appCollectorTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_AppCollectorDataProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (appIdBuilder_ == null) {
            result.appId_ = appId_;
          } else {
            result.appId_ = appIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.appCollectorAddr_ = appCollectorAddr_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.rmIdentifier_ = rmIdentifier_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.version_ = version_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          if (appCollectorTokenBuilder_ == null) {
            result.appCollectorToken_ = appCollectorToken_;
          } else {
            result.appCollectorToken_ = appCollectorTokenBuilder_.build();
          }
          to_bitField0_ |= 0x00000010;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance()) return this;
        if (other.hasAppId()) {
          mergeAppId(other.getAppId());
        }
        if (other.hasAppCollectorAddr()) {
          bitField0_ |= 0x00000002;
          appCollectorAddr_ = other.appCollectorAddr_;
          onChanged();
        }
        if (other.hasRmIdentifier()) {
          setRmIdentifier(other.getRmIdentifier());
        }
        if (other.hasVersion()) {
          setVersion(other.getVersion());
        }
        if (other.hasAppCollectorToken()) {
          mergeAppCollectorToken(other.getAppCollectorToken());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasAppCollectorToken()) {
          if (!getAppCollectorToken().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> appIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public boolean hasAppId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
        if (appIdBuilder_ == null) {
          return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        } else {
          return appIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder setAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          appId_ = value;
          onChanged();
        } else {
          appIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder setAppId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (appIdBuilder_ == null) {
          appId_ = builderForValue.build();
          onChanged();
        } else {
          appIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder mergeAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              appId_ != null &&
              appId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            appId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(appId_).mergeFrom(value).buildPartial();
          } else {
            appId_ = value;
          }
          onChanged();
        } else {
          appIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public Builder clearAppId() {
        if (appIdBuilder_ == null) {
          appId_ = null;
          onChanged();
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getAppIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAppIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
        if (appIdBuilder_ != null) {
          return appIdBuilder_.getMessageOrBuilder();
        } else {
          return appId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto app_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getAppIdFieldBuilder() {
        if (appIdBuilder_ == null) {
          appIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  getAppId(),
                  getParentForChildren(),
                  isClean());
          appId_ = null;
        }
        return appIdBuilder_;
      }

      private java.lang.Object appCollectorAddr_ = "";
      /**
       * <code>optional string app_collector_addr = 2;</code>
       */
      public boolean hasAppCollectorAddr() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string app_collector_addr = 2;</code>
       */
      public java.lang.String getAppCollectorAddr() {
        java.lang.Object ref = appCollectorAddr_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            appCollectorAddr_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string app_collector_addr = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getAppCollectorAddrBytes() {
        java.lang.Object ref = appCollectorAddr_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          appCollectorAddr_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string app_collector_addr = 2;</code>
       */
      public Builder setAppCollectorAddr(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        appCollectorAddr_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string app_collector_addr = 2;</code>
       */
      public Builder clearAppCollectorAddr() {
        bitField0_ = (bitField0_ & ~0x00000002);
        appCollectorAddr_ = getDefaultInstance().getAppCollectorAddr();
        onChanged();
        return this;
      }
      /**
       * <code>optional string app_collector_addr = 2;</code>
       */
      public Builder setAppCollectorAddrBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        appCollectorAddr_ = value;
        onChanged();
        return this;
      }

      private long rmIdentifier_ = -1L;
      /**
       * <code>optional int64 rm_identifier = 3 [default = -1];</code>
       */
      public boolean hasRmIdentifier() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int64 rm_identifier = 3 [default = -1];</code>
       */
      public long getRmIdentifier() {
        return rmIdentifier_;
      }
      /**
       * <code>optional int64 rm_identifier = 3 [default = -1];</code>
       */
      public Builder setRmIdentifier(long value) {
        bitField0_ |= 0x00000004;
        rmIdentifier_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 rm_identifier = 3 [default = -1];</code>
       */
      public Builder clearRmIdentifier() {
        bitField0_ = (bitField0_ & ~0x00000004);
        rmIdentifier_ = -1L;
        onChanged();
        return this;
      }

      private long version_ = -1L;
      /**
       * <code>optional int64 version = 4 [default = -1];</code>
       */
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 version = 4 [default = -1];</code>
       */
      public long getVersion() {
        return version_;
      }
      /**
       * <code>optional int64 version = 4 [default = -1];</code>
       */
      public Builder setVersion(long value) {
        bitField0_ |= 0x00000008;
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 version = 4 [default = -1];</code>
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000008);
        version_ = -1L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.security.proto.SecurityProtos.TokenProto appCollectorToken_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> appCollectorTokenBuilder_;
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public boolean hasAppCollectorToken() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getAppCollectorToken() {
        if (appCollectorTokenBuilder_ == null) {
          return appCollectorToken_ == null ? org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance() : appCollectorToken_;
        } else {
          return appCollectorTokenBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public Builder setAppCollectorToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (appCollectorTokenBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          appCollectorToken_ = value;
          onChanged();
        } else {
          appCollectorTokenBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public Builder setAppCollectorToken(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (appCollectorTokenBuilder_ == null) {
          appCollectorToken_ = builderForValue.build();
          onChanged();
        } else {
          appCollectorTokenBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public Builder mergeAppCollectorToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (appCollectorTokenBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0) &&
              appCollectorToken_ != null &&
              appCollectorToken_ != org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance()) {
            appCollectorToken_ =
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.newBuilder(appCollectorToken_).mergeFrom(value).buildPartial();
          } else {
            appCollectorToken_ = value;
          }
          onChanged();
        } else {
          appCollectorTokenBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public Builder clearAppCollectorToken() {
        if (appCollectorTokenBuilder_ == null) {
          appCollectorToken_ = null;
          onChanged();
        } else {
          appCollectorTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getAppCollectorTokenBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getAppCollectorTokenFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getAppCollectorTokenOrBuilder() {
        if (appCollectorTokenBuilder_ != null) {
          return appCollectorTokenBuilder_.getMessageOrBuilder();
        } else {
          return appCollectorToken_ == null ?
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance() : appCollectorToken_;
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto app_collector_token = 5;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getAppCollectorTokenFieldBuilder() {
        if (appCollectorTokenBuilder_ == null) {
          appCollectorTokenBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  getAppCollectorToken(),
                  getParentForChildren(),
                  isClean());
          appCollectorToken_ = null;
        }
        return appCollectorTokenBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.AppCollectorDataProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.AppCollectorDataProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<AppCollectorDataProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<AppCollectorDataProto>() {
      @java.lang.Override
      public AppCollectorDataProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new AppCollectorDataProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<AppCollectorDataProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<AppCollectorDataProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ReportNewCollectorInfoRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.ReportNewCollectorInfoRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> 
        getAppCollectorsList();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getAppCollectors(int index);
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    int getAppCollectorsCount();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
        getAppCollectorsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getAppCollectorsOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#47;///////////////////////////////////////////////////
   * ///// collector_nodemanager_protocol //////////////
   * ////////////////////////////////////////////////////
   * </pre>
   *
   * Protobuf type {@code hadoop.yarn.ReportNewCollectorInfoRequestProto}
   */
  public  static final class ReportNewCollectorInfoRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.ReportNewCollectorInfoRequestProto)
      ReportNewCollectorInfoRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ReportNewCollectorInfoRequestProto.newBuilder() to construct.
    private ReportNewCollectorInfoRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ReportNewCollectorInfoRequestProto() {
      appCollectors_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ReportNewCollectorInfoRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                appCollectors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              appCollectors_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          appCollectors_ = java.util.Collections.unmodifiableList(appCollectors_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.Builder.class);
    }

    public static final int APP_COLLECTORS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> appCollectors_;
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> getAppCollectorsList() {
      return appCollectors_;
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
        getAppCollectorsOrBuilderList() {
      return appCollectors_;
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    public int getAppCollectorsCount() {
      return appCollectors_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getAppCollectors(int index) {
      return appCollectors_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getAppCollectorsOrBuilder(
        int index) {
      return appCollectors_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      for (int i = 0; i < getAppCollectorsCount(); i++) {
        if (!getAppCollectors(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < appCollectors_.size(); i++) {
        output.writeMessage(1, appCollectors_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < appCollectors_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, appCollectors_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto) obj;

      if (!getAppCollectorsList()
          .equals(other.getAppCollectorsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getAppCollectorsCount() > 0) {
        hash = (37 * hash) + APP_COLLECTORS_FIELD_NUMBER;
        hash = (53 * hash) + getAppCollectorsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#47;///////////////////////////////////////////////////
     * ///// collector_nodemanager_protocol //////////////
     * ////////////////////////////////////////////////////
     * </pre>
     *
     * Protobuf type {@code hadoop.yarn.ReportNewCollectorInfoRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.ReportNewCollectorInfoRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAppCollectorsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (appCollectorsBuilder_ == null) {
          appCollectors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          appCollectorsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (appCollectorsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            appCollectors_ = java.util.Collections.unmodifiableList(appCollectors_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.appCollectors_ = appCollectors_;
        } else {
          result.appCollectors_ = appCollectorsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto.getDefaultInstance()) return this;
        if (appCollectorsBuilder_ == null) {
          if (!other.appCollectors_.isEmpty()) {
            if (appCollectors_.isEmpty()) {
              appCollectors_ = other.appCollectors_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureAppCollectorsIsMutable();
              appCollectors_.addAll(other.appCollectors_);
            }
            onChanged();
          }
        } else {
          if (!other.appCollectors_.isEmpty()) {
            if (appCollectorsBuilder_.isEmpty()) {
              appCollectorsBuilder_.dispose();
              appCollectorsBuilder_ = null;
              appCollectors_ = other.appCollectors_;
              bitField0_ = (bitField0_ & ~0x00000001);
              appCollectorsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getAppCollectorsFieldBuilder() : null;
            } else {
              appCollectorsBuilder_.addAllMessages(other.appCollectors_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        for (int i = 0; i < getAppCollectorsCount(); i++) {
          if (!getAppCollectors(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> appCollectors_ =
        java.util.Collections.emptyList();
      private void ensureAppCollectorsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          appCollectors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto>(appCollectors_);
          bitField0_ |= 0x00000001;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> appCollectorsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> getAppCollectorsList() {
        if (appCollectorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(appCollectors_);
        } else {
          return appCollectorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public int getAppCollectorsCount() {
        if (appCollectorsBuilder_ == null) {
          return appCollectors_.size();
        } else {
          return appCollectorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto getAppCollectors(int index) {
        if (appCollectorsBuilder_ == null) {
          return appCollectors_.get(index);
        } else {
          return appCollectorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder setAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (appCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAppCollectorsIsMutable();
          appCollectors_.set(index, value);
          onChanged();
        } else {
          appCollectorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder setAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.set(index, builderForValue.build());
          onChanged();
        } else {
          appCollectorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder addAppCollectors(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (appCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAppCollectorsIsMutable();
          appCollectors_.add(value);
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder addAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto value) {
        if (appCollectorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAppCollectorsIsMutable();
          appCollectors_.add(index, value);
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder addAppCollectors(
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.add(builderForValue.build());
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder addAppCollectors(
          int index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder builderForValue) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.add(index, builderForValue.build());
          onChanged();
        } else {
          appCollectorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder addAllAppCollectors(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto> values) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, appCollectors_);
          onChanged();
        } else {
          appCollectorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder clearAppCollectors() {
        if (appCollectorsBuilder_ == null) {
          appCollectors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          appCollectorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public Builder removeAppCollectors(int index) {
        if (appCollectorsBuilder_ == null) {
          ensureAppCollectorsIsMutable();
          appCollectors_.remove(index);
          onChanged();
        } else {
          appCollectorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder getAppCollectorsBuilder(
          int index) {
        return getAppCollectorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder getAppCollectorsOrBuilder(
          int index) {
        if (appCollectorsBuilder_ == null) {
          return appCollectors_.get(index);  } else {
          return appCollectorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
           getAppCollectorsOrBuilderList() {
        if (appCollectorsBuilder_ != null) {
          return appCollectorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(appCollectors_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder addAppCollectorsBuilder() {
        return getAppCollectorsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder addAppCollectorsBuilder(
          int index) {
        return getAppCollectorsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.AppCollectorDataProto app_collectors = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder> 
           getAppCollectorsBuilderList() {
        return getAppCollectorsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder> 
          getAppCollectorsFieldBuilder() {
        if (appCollectorsBuilder_ == null) {
          appCollectorsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.AppCollectorDataProtoOrBuilder>(
                  appCollectors_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          appCollectors_ = null;
        }
        return appCollectorsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReportNewCollectorInfoRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReportNewCollectorInfoRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<ReportNewCollectorInfoRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<ReportNewCollectorInfoRequestProto>() {
      @java.lang.Override
      public ReportNewCollectorInfoRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new ReportNewCollectorInfoRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<ReportNewCollectorInfoRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<ReportNewCollectorInfoRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ReportNewCollectorInfoResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.ReportNewCollectorInfoResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReportNewCollectorInfoResponseProto}
   */
  public  static final class ReportNewCollectorInfoResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.ReportNewCollectorInfoResponseProto)
      ReportNewCollectorInfoResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ReportNewCollectorInfoResponseProto.newBuilder() to construct.
    private ReportNewCollectorInfoResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ReportNewCollectorInfoResponseProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ReportNewCollectorInfoResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReportNewCollectorInfoResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.ReportNewCollectorInfoResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReportNewCollectorInfoResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReportNewCollectorInfoResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<ReportNewCollectorInfoResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<ReportNewCollectorInfoResponseProto>() {
      @java.lang.Override
      public ReportNewCollectorInfoResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new ReportNewCollectorInfoResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<ReportNewCollectorInfoResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<ReportNewCollectorInfoResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.ReportNewCollectorInfoResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetTimelineCollectorContextRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.GetTimelineCollectorContextRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    boolean hasAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetTimelineCollectorContextRequestProto}
   */
  public  static final class GetTimelineCollectorContextRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.GetTimelineCollectorContextRequestProto)
      GetTimelineCollectorContextRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetTimelineCollectorContextRequestProto.newBuilder() to construct.
    private GetTimelineCollectorContextRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetTimelineCollectorContextRequestProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetTimelineCollectorContextRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = appId_.toBuilder();
              }
              appId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(appId_);
                appId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int APPID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    public boolean hasAppId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
      return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAppId());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAppId());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto) obj;

      if (hasAppId() != other.hasAppId()) return false;
      if (hasAppId()) {
        if (!getAppId()
            .equals(other.getAppId())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAppId()) {
        hash = (37 * hash) + APPID_FIELD_NUMBER;
        hash = (53 * hash) + getAppId().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetTimelineCollectorContextRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.GetTimelineCollectorContextRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAppIdFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (appIdBuilder_ == null) {
          appId_ = null;
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (appIdBuilder_ == null) {
            result.appId_ = appId_;
          } else {
            result.appId_ = appIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto.getDefaultInstance()) return this;
        if (other.hasAppId()) {
          mergeAppId(other.getAppId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto appId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> appIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public boolean hasAppId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getAppId() {
        if (appIdBuilder_ == null) {
          return appId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        } else {
          return appIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder setAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          appId_ = value;
          onChanged();
        } else {
          appIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder setAppId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (appIdBuilder_ == null) {
          appId_ = builderForValue.build();
          onChanged();
        } else {
          appIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder mergeAppId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (appIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              appId_ != null &&
              appId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            appId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(appId_).mergeFrom(value).buildPartial();
          } else {
            appId_ = value;
          }
          onChanged();
        } else {
          appIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public Builder clearAppId() {
        if (appIdBuilder_ == null) {
          appId_ = null;
          onChanged();
        } else {
          appIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getAppIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAppIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getAppIdOrBuilder() {
        if (appIdBuilder_ != null) {
          return appIdBuilder_.getMessageOrBuilder();
        } else {
          return appId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance() : appId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto appId = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getAppIdFieldBuilder() {
        if (appIdBuilder_ == null) {
          appIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  getAppId(),
                  getParentForChildren(),
                  isClean());
          appId_ = null;
        }
        return appIdBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetTimelineCollectorContextRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetTimelineCollectorContextRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<GetTimelineCollectorContextRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<GetTimelineCollectorContextRequestProto>() {
      @java.lang.Override
      public GetTimelineCollectorContextRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new GetTimelineCollectorContextRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<GetTimelineCollectorContextRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<GetTimelineCollectorContextRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetTimelineCollectorContextResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.GetTimelineCollectorContextResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string user_id = 1;</code>
     */
    boolean hasUserId();
    /**
     * <code>optional string user_id = 1;</code>
     */
    java.lang.String getUserId();
    /**
     * <code>optional string user_id = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getUserIdBytes();

    /**
     * <code>optional string flow_name = 2;</code>
     */
    boolean hasFlowName();
    /**
     * <code>optional string flow_name = 2;</code>
     */
    java.lang.String getFlowName();
    /**
     * <code>optional string flow_name = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getFlowNameBytes();

    /**
     * <code>optional string flow_version = 3;</code>
     */
    boolean hasFlowVersion();
    /**
     * <code>optional string flow_version = 3;</code>
     */
    java.lang.String getFlowVersion();
    /**
     * <code>optional string flow_version = 3;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getFlowVersionBytes();

    /**
     * <code>optional int64 flow_run_id = 4;</code>
     */
    boolean hasFlowRunId();
    /**
     * <code>optional int64 flow_run_id = 4;</code>
     */
    long getFlowRunId();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetTimelineCollectorContextResponseProto}
   */
  public  static final class GetTimelineCollectorContextResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.GetTimelineCollectorContextResponseProto)
      GetTimelineCollectorContextResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetTimelineCollectorContextResponseProto.newBuilder() to construct.
    private GetTimelineCollectorContextResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetTimelineCollectorContextResponseProto() {
      userId_ = "";
      flowName_ = "";
      flowVersion_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetTimelineCollectorContextResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              userId_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              flowName_ = bs;
              break;
            }
            case 26: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              flowVersion_ = bs;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              flowRunId_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int USER_ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object userId_;
    /**
     * <code>optional string user_id = 1;</code>
     */
    public boolean hasUserId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string user_id = 1;</code>
     */
    public java.lang.String getUserId() {
      java.lang.Object ref = userId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          userId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string user_id = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getUserIdBytes() {
      java.lang.Object ref = userId_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        userId_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int FLOW_NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object flowName_;
    /**
     * <code>optional string flow_name = 2;</code>
     */
    public boolean hasFlowName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string flow_name = 2;</code>
     */
    public java.lang.String getFlowName() {
      java.lang.Object ref = flowName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          flowName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string flow_name = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getFlowNameBytes() {
      java.lang.Object ref = flowName_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        flowName_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int FLOW_VERSION_FIELD_NUMBER = 3;
    private volatile java.lang.Object flowVersion_;
    /**
     * <code>optional string flow_version = 3;</code>
     */
    public boolean hasFlowVersion() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional string flow_version = 3;</code>
     */
    public java.lang.String getFlowVersion() {
      java.lang.Object ref = flowVersion_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          flowVersion_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string flow_version = 3;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getFlowVersionBytes() {
      java.lang.Object ref = flowVersion_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        flowVersion_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int FLOW_RUN_ID_FIELD_NUMBER = 4;
    private long flowRunId_;
    /**
     * <code>optional int64 flow_run_id = 4;</code>
     */
    public boolean hasFlowRunId() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 flow_run_id = 4;</code>
     */
    public long getFlowRunId() {
      return flowRunId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, userId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 2, flowName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 3, flowVersion_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, flowRunId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, userId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(2, flowName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(3, flowVersion_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(4, flowRunId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto) obj;

      if (hasUserId() != other.hasUserId()) return false;
      if (hasUserId()) {
        if (!getUserId()
            .equals(other.getUserId())) return false;
      }
      if (hasFlowName() != other.hasFlowName()) return false;
      if (hasFlowName()) {
        if (!getFlowName()
            .equals(other.getFlowName())) return false;
      }
      if (hasFlowVersion() != other.hasFlowVersion()) return false;
      if (hasFlowVersion()) {
        if (!getFlowVersion()
            .equals(other.getFlowVersion())) return false;
      }
      if (hasFlowRunId() != other.hasFlowRunId()) return false;
      if (hasFlowRunId()) {
        if (getFlowRunId()
            != other.getFlowRunId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasUserId()) {
        hash = (37 * hash) + USER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getUserId().hashCode();
      }
      if (hasFlowName()) {
        hash = (37 * hash) + FLOW_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getFlowName().hashCode();
      }
      if (hasFlowVersion()) {
        hash = (37 * hash) + FLOW_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getFlowVersion().hashCode();
      }
      if (hasFlowRunId()) {
        hash = (37 * hash) + FLOW_RUN_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getFlowRunId());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetTimelineCollectorContextResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.GetTimelineCollectorContextResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        userId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        flowName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        flowVersion_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        flowRunId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.userId_ = userId_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.flowName_ = flowName_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.flowVersion_ = flowVersion_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.flowRunId_ = flowRunId_;
          to_bitField0_ |= 0x00000008;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto.getDefaultInstance()) return this;
        if (other.hasUserId()) {
          bitField0_ |= 0x00000001;
          userId_ = other.userId_;
          onChanged();
        }
        if (other.hasFlowName()) {
          bitField0_ |= 0x00000002;
          flowName_ = other.flowName_;
          onChanged();
        }
        if (other.hasFlowVersion()) {
          bitField0_ |= 0x00000004;
          flowVersion_ = other.flowVersion_;
          onChanged();
        }
        if (other.hasFlowRunId()) {
          setFlowRunId(other.getFlowRunId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object userId_ = "";
      /**
       * <code>optional string user_id = 1;</code>
       */
      public boolean hasUserId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string user_id = 1;</code>
       */
      public java.lang.String getUserId() {
        java.lang.Object ref = userId_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            userId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string user_id = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getUserIdBytes() {
        java.lang.Object ref = userId_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          userId_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string user_id = 1;</code>
       */
      public Builder setUserId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        userId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string user_id = 1;</code>
       */
      public Builder clearUserId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        userId_ = getDefaultInstance().getUserId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string user_id = 1;</code>
       */
      public Builder setUserIdBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        userId_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object flowName_ = "";
      /**
       * <code>optional string flow_name = 2;</code>
       */
      public boolean hasFlowName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string flow_name = 2;</code>
       */
      public java.lang.String getFlowName() {
        java.lang.Object ref = flowName_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            flowName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string flow_name = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getFlowNameBytes() {
        java.lang.Object ref = flowName_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          flowName_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string flow_name = 2;</code>
       */
      public Builder setFlowName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        flowName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string flow_name = 2;</code>
       */
      public Builder clearFlowName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        flowName_ = getDefaultInstance().getFlowName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string flow_name = 2;</code>
       */
      public Builder setFlowNameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        flowName_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object flowVersion_ = "";
      /**
       * <code>optional string flow_version = 3;</code>
       */
      public boolean hasFlowVersion() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional string flow_version = 3;</code>
       */
      public java.lang.String getFlowVersion() {
        java.lang.Object ref = flowVersion_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            flowVersion_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string flow_version = 3;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getFlowVersionBytes() {
        java.lang.Object ref = flowVersion_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          flowVersion_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string flow_version = 3;</code>
       */
      public Builder setFlowVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        flowVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string flow_version = 3;</code>
       */
      public Builder clearFlowVersion() {
        bitField0_ = (bitField0_ & ~0x00000004);
        flowVersion_ = getDefaultInstance().getFlowVersion();
        onChanged();
        return this;
      }
      /**
       * <code>optional string flow_version = 3;</code>
       */
      public Builder setFlowVersionBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        flowVersion_ = value;
        onChanged();
        return this;
      }

      private long flowRunId_ ;
      /**
       * <code>optional int64 flow_run_id = 4;</code>
       */
      public boolean hasFlowRunId() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 flow_run_id = 4;</code>
       */
      public long getFlowRunId() {
        return flowRunId_;
      }
      /**
       * <code>optional int64 flow_run_id = 4;</code>
       */
      public Builder setFlowRunId(long value) {
        bitField0_ |= 0x00000008;
        flowRunId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 flow_run_id = 4;</code>
       */
      public Builder clearFlowRunId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        flowRunId_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetTimelineCollectorContextResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetTimelineCollectorContextResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<GetTimelineCollectorContextResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<GetTimelineCollectorContextResponseProto>() {
      @java.lang.Override
      public GetTimelineCollectorContextResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new GetTimelineCollectorContextResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<GetTimelineCollectorContextResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<GetTimelineCollectorContextResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.GetTimelineCollectorContextResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NMContainerStatusProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.NMContainerStatusProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
     */
    boolean hasContainerState();
    /**
     * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto getContainerState();

    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
     */
    boolean hasResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource();
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder();

    /**
     * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
     */
    boolean hasPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getPriorityOrBuilder();

    /**
     * <code>optional string diagnostics = 5 [default = "N/A"];</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 5 [default = "N/A"];</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 5 [default = "N/A"];</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes();

    /**
     * <code>optional int32 container_exit_status = 6;</code>
     */
    boolean hasContainerExitStatus();
    /**
     * <code>optional int32 container_exit_status = 6;</code>
     */
    int getContainerExitStatus();

    /**
     * <code>optional int64 creation_time = 7;</code>
     */
    boolean hasCreationTime();
    /**
     * <code>optional int64 creation_time = 7;</code>
     */
    long getCreationTime();

    /**
     * <code>optional string nodeLabelExpression = 8;</code>
     */
    boolean hasNodeLabelExpression();
    /**
     * <code>optional string nodeLabelExpression = 8;</code>
     */
    java.lang.String getNodeLabelExpression();
    /**
     * <code>optional string nodeLabelExpression = 8;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodeLabelExpressionBytes();

    /**
     * <code>optional int32 version = 9;</code>
     */
    boolean hasVersion();
    /**
     * <code>optional int32 version = 9;</code>
     */
    int getVersion();

    /**
     * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
     */
    boolean hasExecutionType();
    /**
     * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto getExecutionType();

    /**
     * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
     */
    boolean hasAllocationRequestId();
    /**
     * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
     */
    long getAllocationRequestId();

    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    java.util.List<java.lang.String>
        getAllocationTagsList();
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    int getAllocationTagsCount();
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    java.lang.String getAllocationTags(int index);
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getAllocationTagsBytes(int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.NMContainerStatusProto}
   */
  public  static final class NMContainerStatusProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.NMContainerStatusProto)
      NMContainerStatusProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NMContainerStatusProto.newBuilder() to construct.
    private NMContainerStatusProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NMContainerStatusProto() {
      containerState_ = 1;
      diagnostics_ = "N/A";
      nodeLabelExpression_ = "";
      executionType_ = 1;
      allocationRequestId_ = -1L;
      allocationTags_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NMContainerStatusProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                containerState_ = rawValue;
              }
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = resource_.toBuilder();
              }
              resource_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(resource_);
                resource_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000008) != 0)) {
                subBuilder = priority_.toBuilder();
              }
              priority_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(priority_);
                priority_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000008;
              break;
            }
            case 42: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000010;
              diagnostics_ = bs;
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              containerExitStatus_ = input.readInt32();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              creationTime_ = input.readInt64();
              break;
            }
            case 66: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000080;
              nodeLabelExpression_ = bs;
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              version_ = input.readInt32();
              break;
            }
            case 80: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto value = org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(10, rawValue);
              } else {
                bitField0_ |= 0x00000200;
                executionType_ = rawValue;
              }
              break;
            }
            case 88: {
              bitField0_ |= 0x00000400;
              allocationRequestId_ = input.readInt64();
              break;
            }
            case 98: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000800) != 0)) {
                allocationTags_ = new org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000800;
              }
              allocationTags_.add(bs);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000800) != 0)) {
          allocationTags_ = allocationTags_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NMContainerStatusProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NMContainerStatusProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder.class);
    }

    private int bitField0_;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
    }

    public static final int CONTAINER_STATE_FIELD_NUMBER = 2;
    private int containerState_;
    /**
     * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
     */
    public boolean hasContainerState() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto getContainerState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto result = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto.valueOf(containerState_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto.C_NEW : result;
    }

    public static final int RESOURCE_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
     */
    public boolean hasResource() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
      return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
    }

    public static final int PRIORITY_FIELD_NUMBER = 4;
    private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto priority_;
    /**
     * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
     */
    public boolean hasPriority() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getPriority() {
      return priority_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : priority_;
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getPriorityOrBuilder() {
      return priority_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : priority_;
    }

    public static final int DIAGNOSTICS_FIELD_NUMBER = 5;
    private volatile java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 5 [default = "N/A"];</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional string diagnostics = 5 [default = "N/A"];</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 5 [default = "N/A"];</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int CONTAINER_EXIT_STATUS_FIELD_NUMBER = 6;
    private int containerExitStatus_;
    /**
     * <code>optional int32 container_exit_status = 6;</code>
     */
    public boolean hasContainerExitStatus() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional int32 container_exit_status = 6;</code>
     */
    public int getContainerExitStatus() {
      return containerExitStatus_;
    }

    public static final int CREATION_TIME_FIELD_NUMBER = 7;
    private long creationTime_;
    /**
     * <code>optional int64 creation_time = 7;</code>
     */
    public boolean hasCreationTime() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional int64 creation_time = 7;</code>
     */
    public long getCreationTime() {
      return creationTime_;
    }

    public static final int NODELABELEXPRESSION_FIELD_NUMBER = 8;
    private volatile java.lang.Object nodeLabelExpression_;
    /**
     * <code>optional string nodeLabelExpression = 8;</code>
     */
    public boolean hasNodeLabelExpression() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional string nodeLabelExpression = 8;</code>
     */
    public java.lang.String getNodeLabelExpression() {
      java.lang.Object ref = nodeLabelExpression_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          nodeLabelExpression_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string nodeLabelExpression = 8;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getNodeLabelExpressionBytes() {
      java.lang.Object ref = nodeLabelExpression_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        nodeLabelExpression_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int VERSION_FIELD_NUMBER = 9;
    private int version_;
    /**
     * <code>optional int32 version = 9;</code>
     */
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional int32 version = 9;</code>
     */
    public int getVersion() {
      return version_;
    }

    public static final int EXECUTIONTYPE_FIELD_NUMBER = 10;
    private int executionType_;
    /**
     * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
     */
    public boolean hasExecutionType() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto getExecutionType() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto result = org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto.valueOf(executionType_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto.GUARANTEED : result;
    }

    public static final int ALLOCATION_REQUEST_ID_FIELD_NUMBER = 11;
    private long allocationRequestId_;
    /**
     * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
     */
    public boolean hasAllocationRequestId() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
     */
    public long getAllocationRequestId() {
      return allocationRequestId_;
    }

    public static final int ALLOCATION_TAGS_FIELD_NUMBER = 12;
    private org.apache.hadoop.thirdparty.protobuf.LazyStringList allocationTags_;
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ProtocolStringList
        getAllocationTagsList() {
      return allocationTags_;
    }
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    public int getAllocationTagsCount() {
      return allocationTags_.size();
    }
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    public java.lang.String getAllocationTags(int index) {
      return allocationTags_.get(index);
    }
    /**
     * <code>repeated string allocation_tags = 12;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getAllocationTagsBytes(int index) {
      return allocationTags_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasResource()) {
        if (!getResource().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getContainerId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeEnum(2, containerState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getResource());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeMessage(4, getPriority());
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 5, diagnostics_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeInt32(6, containerExitStatus_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeInt64(7, creationTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 8, nodeLabelExpression_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeInt32(9, version_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeEnum(10, executionType_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeInt64(11, allocationRequestId_);
      }
      for (int i = 0; i < allocationTags_.size(); i++) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 12, allocationTags_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getContainerId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(2, containerState_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(3, getResource());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(4, getPriority());
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(5, diagnostics_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(6, containerExitStatus_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(7, creationTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(8, nodeLabelExpression_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(9, version_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(10, executionType_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(11, allocationRequestId_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < allocationTags_.size(); i++) {
          dataSize += computeStringSizeNoTag(allocationTags_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getAllocationTagsList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto) obj;

      if (hasContainerId() != other.hasContainerId()) return false;
      if (hasContainerId()) {
        if (!getContainerId()
            .equals(other.getContainerId())) return false;
      }
      if (hasContainerState() != other.hasContainerState()) return false;
      if (hasContainerState()) {
        if (containerState_ != other.containerState_) return false;
      }
      if (hasResource() != other.hasResource()) return false;
      if (hasResource()) {
        if (!getResource()
            .equals(other.getResource())) return false;
      }
      if (hasPriority() != other.hasPriority()) return false;
      if (hasPriority()) {
        if (!getPriority()
            .equals(other.getPriority())) return false;
      }
      if (hasDiagnostics() != other.hasDiagnostics()) return false;
      if (hasDiagnostics()) {
        if (!getDiagnostics()
            .equals(other.getDiagnostics())) return false;
      }
      if (hasContainerExitStatus() != other.hasContainerExitStatus()) return false;
      if (hasContainerExitStatus()) {
        if (getContainerExitStatus()
            != other.getContainerExitStatus()) return false;
      }
      if (hasCreationTime() != other.hasCreationTime()) return false;
      if (hasCreationTime()) {
        if (getCreationTime()
            != other.getCreationTime()) return false;
      }
      if (hasNodeLabelExpression() != other.hasNodeLabelExpression()) return false;
      if (hasNodeLabelExpression()) {
        if (!getNodeLabelExpression()
            .equals(other.getNodeLabelExpression())) return false;
      }
      if (hasVersion() != other.hasVersion()) return false;
      if (hasVersion()) {
        if (getVersion()
            != other.getVersion()) return false;
      }
      if (hasExecutionType() != other.hasExecutionType()) return false;
      if (hasExecutionType()) {
        if (executionType_ != other.executionType_) return false;
      }
      if (hasAllocationRequestId() != other.hasAllocationRequestId()) return false;
      if (hasAllocationRequestId()) {
        if (getAllocationRequestId()
            != other.getAllocationRequestId()) return false;
      }
      if (!getAllocationTagsList()
          .equals(other.getAllocationTagsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      if (hasContainerState()) {
        hash = (37 * hash) + CONTAINER_STATE_FIELD_NUMBER;
        hash = (53 * hash) + containerState_;
      }
      if (hasResource()) {
        hash = (37 * hash) + RESOURCE_FIELD_NUMBER;
        hash = (53 * hash) + getResource().hashCode();
      }
      if (hasPriority()) {
        hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
        hash = (53 * hash) + getPriority().hashCode();
      }
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      if (hasContainerExitStatus()) {
        hash = (37 * hash) + CONTAINER_EXIT_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getContainerExitStatus();
      }
      if (hasCreationTime()) {
        hash = (37 * hash) + CREATION_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getCreationTime());
      }
      if (hasNodeLabelExpression()) {
        hash = (37 * hash) + NODELABELEXPRESSION_FIELD_NUMBER;
        hash = (53 * hash) + getNodeLabelExpression().hashCode();
      }
      if (hasVersion()) {
        hash = (37 * hash) + VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getVersion();
      }
      if (hasExecutionType()) {
        hash = (37 * hash) + EXECUTIONTYPE_FIELD_NUMBER;
        hash = (53 * hash) + executionType_;
      }
      if (hasAllocationRequestId()) {
        hash = (37 * hash) + ALLOCATION_REQUEST_ID_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getAllocationRequestId());
      }
      if (getAllocationTagsCount() > 0) {
        hash = (37 * hash) + ALLOCATION_TAGS_FIELD_NUMBER;
        hash = (53 * hash) + getAllocationTagsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NMContainerStatusProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.NMContainerStatusProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NMContainerStatusProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NMContainerStatusProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
          getResourceFieldBuilder();
          getPriorityFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = null;
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        containerState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (resourceBuilder_ == null) {
          resource_ = null;
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        if (priorityBuilder_ == null) {
          priority_ = null;
        } else {
          priorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        diagnostics_ = "N/A";
        bitField0_ = (bitField0_ & ~0x00000010);
        containerExitStatus_ = 0;
        bitField0_ = (bitField0_ & ~0x00000020);
        creationTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        nodeLabelExpression_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        version_ = 0;
        bitField0_ = (bitField0_ & ~0x00000100);
        executionType_ = 1;
        bitField0_ = (bitField0_ & ~0x00000200);
        allocationRequestId_ = -1L;
        bitField0_ = (bitField0_ & ~0x00000400);
        allocationTags_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000800);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_NMContainerStatusProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (containerIdBuilder_ == null) {
            result.containerId_ = containerId_;
          } else {
            result.containerId_ = containerIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.containerState_ = containerState_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (resourceBuilder_ == null) {
            result.resource_ = resource_;
          } else {
            result.resource_ = resourceBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          if (priorityBuilder_ == null) {
            result.priority_ = priority_;
          } else {
            result.priority_ = priorityBuilder_.build();
          }
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.diagnostics_ = diagnostics_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.containerExitStatus_ = containerExitStatus_;
          to_bitField0_ |= 0x00000020;
        }
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.creationTime_ = creationTime_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000080;
        }
        result.nodeLabelExpression_ = nodeLabelExpression_;
        if (((from_bitField0_ & 0x00000100) != 0)) {
          result.version_ = version_;
          to_bitField0_ |= 0x00000100;
        }
        if (((from_bitField0_ & 0x00000200) != 0)) {
          to_bitField0_ |= 0x00000200;
        }
        result.executionType_ = executionType_;
        if (((from_bitField0_ & 0x00000400) != 0)) {
          to_bitField0_ |= 0x00000400;
        }
        result.allocationRequestId_ = allocationRequestId_;
        if (((bitField0_ & 0x00000800) != 0)) {
          allocationTags_ = allocationTags_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000800);
        }
        result.allocationTags_ = allocationTags_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        if (other.hasContainerState()) {
          setContainerState(other.getContainerState());
        }
        if (other.hasResource()) {
          mergeResource(other.getResource());
        }
        if (other.hasPriority()) {
          mergePriority(other.getPriority());
        }
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000010;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        if (other.hasContainerExitStatus()) {
          setContainerExitStatus(other.getContainerExitStatus());
        }
        if (other.hasCreationTime()) {
          setCreationTime(other.getCreationTime());
        }
        if (other.hasNodeLabelExpression()) {
          bitField0_ |= 0x00000080;
          nodeLabelExpression_ = other.nodeLabelExpression_;
          onChanged();
        }
        if (other.hasVersion()) {
          setVersion(other.getVersion());
        }
        if (other.hasExecutionType()) {
          setExecutionType(other.getExecutionType());
        }
        if (other.hasAllocationRequestId()) {
          setAllocationRequestId(other.getAllocationRequestId());
        }
        if (!other.allocationTags_.isEmpty()) {
          if (allocationTags_.isEmpty()) {
            allocationTags_ = other.allocationTags_;
            bitField0_ = (bitField0_ & ~0x00000800);
          } else {
            ensureAllocationTagsIsMutable();
            allocationTags_.addAll(other.allocationTags_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasResource()) {
          if (!getResource().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              containerId_ != null &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = null;
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance() : containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  getContainerId(),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      private int containerState_ = 1;
      /**
       * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
       */
      public boolean hasContainerState() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto getContainerState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto result = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto.valueOf(containerState_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto.C_NEW : result;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
       */
      public Builder setContainerState(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        containerState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStateProto container_state = 2;</code>
       */
      public Builder clearContainerState() {
        bitField0_ = (bitField0_ & ~0x00000002);
        containerState_ = 1;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto resource_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> resourceBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public boolean hasResource() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getResource() {
        if (resourceBuilder_ == null) {
          return resource_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        } else {
          return resourceBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public Builder setResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          resource_ = value;
          onChanged();
        } else {
          resourceBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public Builder setResource(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (resourceBuilder_ == null) {
          resource_ = builderForValue.build();
          onChanged();
        } else {
          resourceBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public Builder mergeResource(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (resourceBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              resource_ != null &&
              resource_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            resource_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(resource_).mergeFrom(value).buildPartial();
          } else {
            resource_ = value;
          }
          onChanged();
        } else {
          resourceBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public Builder clearResource() {
        if (resourceBuilder_ == null) {
          resource_ = null;
          onChanged();
        } else {
          resourceBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getResourceBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getResourceFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getResourceOrBuilder() {
        if (resourceBuilder_ != null) {
          return resourceBuilder_.getMessageOrBuilder();
        } else {
          return resource_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance() : resource_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto resource = 3;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getResourceFieldBuilder() {
        if (resourceBuilder_ == null) {
          resourceBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  getResource(),
                  getParentForChildren(),
                  isClean());
          resource_ = null;
        }
        return resourceBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto priority_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> priorityBuilder_;
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public boolean hasPriority() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getPriority() {
        if (priorityBuilder_ == null) {
          return priority_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : priority_;
        } else {
          return priorityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public Builder setPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (priorityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          priority_ = value;
          onChanged();
        } else {
          priorityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public Builder setPriority(
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder builderForValue) {
        if (priorityBuilder_ == null) {
          priority_ = builderForValue.build();
          onChanged();
        } else {
          priorityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public Builder mergePriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (priorityBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0) &&
              priority_ != null &&
              priority_ != org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance()) {
            priority_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.newBuilder(priority_).mergeFrom(value).buildPartial();
          } else {
            priority_ = value;
          }
          onChanged();
        } else {
          priorityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public Builder clearPriority() {
        if (priorityBuilder_ == null) {
          priority_ = null;
          onChanged();
        } else {
          priorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder getPriorityBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getPriorityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getPriorityOrBuilder() {
        if (priorityBuilder_ != null) {
          return priorityBuilder_.getMessageOrBuilder();
        } else {
          return priority_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance() : priority_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto priority = 4;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> 
          getPriorityFieldBuilder() {
        if (priorityBuilder_ == null) {
          priorityBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder>(
                  getPriority(),
                  getParentForChildren(),
                  isClean());
          priority_ = null;
        }
        return priorityBuilder_;
      }

      private java.lang.Object diagnostics_ = "N/A";
      /**
       * <code>optional string diagnostics = 5 [default = "N/A"];</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional string diagnostics = 5 [default = "N/A"];</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnostics_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 5 [default = "N/A"];</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 5 [default = "N/A"];</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 5 [default = "N/A"];</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000010);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 5 [default = "N/A"];</code>
       */
      public Builder setDiagnosticsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      private int containerExitStatus_ ;
      /**
       * <code>optional int32 container_exit_status = 6;</code>
       */
      public boolean hasContainerExitStatus() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional int32 container_exit_status = 6;</code>
       */
      public int getContainerExitStatus() {
        return containerExitStatus_;
      }
      /**
       * <code>optional int32 container_exit_status = 6;</code>
       */
      public Builder setContainerExitStatus(int value) {
        bitField0_ |= 0x00000020;
        containerExitStatus_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 container_exit_status = 6;</code>
       */
      public Builder clearContainerExitStatus() {
        bitField0_ = (bitField0_ & ~0x00000020);
        containerExitStatus_ = 0;
        onChanged();
        return this;
      }

      private long creationTime_ ;
      /**
       * <code>optional int64 creation_time = 7;</code>
       */
      public boolean hasCreationTime() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int64 creation_time = 7;</code>
       */
      public long getCreationTime() {
        return creationTime_;
      }
      /**
       * <code>optional int64 creation_time = 7;</code>
       */
      public Builder setCreationTime(long value) {
        bitField0_ |= 0x00000040;
        creationTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 creation_time = 7;</code>
       */
      public Builder clearCreationTime() {
        bitField0_ = (bitField0_ & ~0x00000040);
        creationTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object nodeLabelExpression_ = "";
      /**
       * <code>optional string nodeLabelExpression = 8;</code>
       */
      public boolean hasNodeLabelExpression() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional string nodeLabelExpression = 8;</code>
       */
      public java.lang.String getNodeLabelExpression() {
        java.lang.Object ref = nodeLabelExpression_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            nodeLabelExpression_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string nodeLabelExpression = 8;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getNodeLabelExpressionBytes() {
        java.lang.Object ref = nodeLabelExpression_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          nodeLabelExpression_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string nodeLabelExpression = 8;</code>
       */
      public Builder setNodeLabelExpression(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        nodeLabelExpression_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string nodeLabelExpression = 8;</code>
       */
      public Builder clearNodeLabelExpression() {
        bitField0_ = (bitField0_ & ~0x00000080);
        nodeLabelExpression_ = getDefaultInstance().getNodeLabelExpression();
        onChanged();
        return this;
      }
      /**
       * <code>optional string nodeLabelExpression = 8;</code>
       */
      public Builder setNodeLabelExpressionBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        nodeLabelExpression_ = value;
        onChanged();
        return this;
      }

      private int version_ ;
      /**
       * <code>optional int32 version = 9;</code>
       */
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional int32 version = 9;</code>
       */
      public int getVersion() {
        return version_;
      }
      /**
       * <code>optional int32 version = 9;</code>
       */
      public Builder setVersion(int value) {
        bitField0_ |= 0x00000100;
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 version = 9;</code>
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000100);
        version_ = 0;
        onChanged();
        return this;
      }

      private int executionType_ = 1;
      /**
       * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
       */
      public boolean hasExecutionType() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto getExecutionType() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto result = org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto.valueOf(executionType_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto.GUARANTEED : result;
      }
      /**
       * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
       */
      public Builder setExecutionType(org.apache.hadoop.yarn.proto.YarnProtos.ExecutionTypeProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000200;
        executionType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ExecutionTypeProto executionType = 10 [default = GUARANTEED];</code>
       */
      public Builder clearExecutionType() {
        bitField0_ = (bitField0_ & ~0x00000200);
        executionType_ = 1;
        onChanged();
        return this;
      }

      private long allocationRequestId_ = -1L;
      /**
       * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
       */
      public boolean hasAllocationRequestId() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
       */
      public long getAllocationRequestId() {
        return allocationRequestId_;
      }
      /**
       * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
       */
      public Builder setAllocationRequestId(long value) {
        bitField0_ |= 0x00000400;
        allocationRequestId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 allocation_request_id = 11 [default = -1];</code>
       */
      public Builder clearAllocationRequestId() {
        bitField0_ = (bitField0_ & ~0x00000400);
        allocationRequestId_ = -1L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.thirdparty.protobuf.LazyStringList allocationTags_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
      private void ensureAllocationTagsIsMutable() {
        if (!((bitField0_ & 0x00000800) != 0)) {
          allocationTags_ = new org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList(allocationTags_);
          bitField0_ |= 0x00000800;
         }
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ProtocolStringList
          getAllocationTagsList() {
        return allocationTags_.getUnmodifiableView();
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public int getAllocationTagsCount() {
        return allocationTags_.size();
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public java.lang.String getAllocationTags(int index) {
        return allocationTags_.get(index);
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getAllocationTagsBytes(int index) {
        return allocationTags_.getByteString(index);
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public Builder setAllocationTags(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureAllocationTagsIsMutable();
        allocationTags_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public Builder addAllocationTags(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureAllocationTagsIsMutable();
        allocationTags_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public Builder addAllAllocationTags(
          java.lang.Iterable<java.lang.String> values) {
        ensureAllocationTagsIsMutable();
        org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
            values, allocationTags_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public Builder clearAllocationTags() {
        allocationTags_ = org.apache.hadoop.thirdparty.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000800);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string allocation_tags = 12;</code>
       */
      public Builder addAllocationTagsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureAllocationTagsIsMutable();
        allocationTags_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NMContainerStatusProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NMContainerStatusProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<NMContainerStatusProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<NMContainerStatusProto>() {
      @java.lang.Override
      public NMContainerStatusProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new NMContainerStatusProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<NMContainerStatusProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<NMContainerStatusProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NMContainerStatusProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SCMUploaderNotifyRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.SCMUploaderNotifyRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string resource_key = 1;</code>
     */
    boolean hasResourceKey();
    /**
     * <code>optional string resource_key = 1;</code>
     */
    java.lang.String getResourceKey();
    /**
     * <code>optional string resource_key = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getResourceKeyBytes();

    /**
     * <code>optional string filename = 2;</code>
     */
    boolean hasFilename();
    /**
     * <code>optional string filename = 2;</code>
     */
    java.lang.String getFilename();
    /**
     * <code>optional string filename = 2;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getFilenameBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SCMUploaderNotifyRequestProto}
   */
  public  static final class SCMUploaderNotifyRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.SCMUploaderNotifyRequestProto)
      SCMUploaderNotifyRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SCMUploaderNotifyRequestProto.newBuilder() to construct.
    private SCMUploaderNotifyRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SCMUploaderNotifyRequestProto() {
      resourceKey_ = "";
      filename_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SCMUploaderNotifyRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              resourceKey_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              filename_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int RESOURCE_KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object resourceKey_;
    /**
     * <code>optional string resource_key = 1;</code>
     */
    public boolean hasResourceKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string resource_key = 1;</code>
     */
    public java.lang.String getResourceKey() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          resourceKey_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string resource_key = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getResourceKeyBytes() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        resourceKey_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int FILENAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object filename_;
    /**
     * <code>optional string filename = 2;</code>
     */
    public boolean hasFilename() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string filename = 2;</code>
     */
    public java.lang.String getFilename() {
      java.lang.Object ref = filename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          filename_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string filename = 2;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getFilenameBytes() {
      java.lang.Object ref = filename_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        filename_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, resourceKey_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 2, filename_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, resourceKey_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(2, filename_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto) obj;

      if (hasResourceKey() != other.hasResourceKey()) return false;
      if (hasResourceKey()) {
        if (!getResourceKey()
            .equals(other.getResourceKey())) return false;
      }
      if (hasFilename() != other.hasFilename()) return false;
      if (hasFilename()) {
        if (!getFilename()
            .equals(other.getFilename())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResourceKey()) {
        hash = (37 * hash) + RESOURCE_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getResourceKey().hashCode();
      }
      if (hasFilename()) {
        hash = (37 * hash) + FILENAME_FIELD_NUMBER;
        hash = (53 * hash) + getFilename().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SCMUploaderNotifyRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.SCMUploaderNotifyRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        resourceKey_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        filename_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.resourceKey_ = resourceKey_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.filename_ = filename_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto.getDefaultInstance()) return this;
        if (other.hasResourceKey()) {
          bitField0_ |= 0x00000001;
          resourceKey_ = other.resourceKey_;
          onChanged();
        }
        if (other.hasFilename()) {
          bitField0_ |= 0x00000002;
          filename_ = other.filename_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object resourceKey_ = "";
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public boolean hasResourceKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public java.lang.String getResourceKey() {
        java.lang.Object ref = resourceKey_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            resourceKey_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getResourceKeyBytes() {
        java.lang.Object ref = resourceKey_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          resourceKey_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public Builder setResourceKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        resourceKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public Builder clearResourceKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        resourceKey_ = getDefaultInstance().getResourceKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public Builder setResourceKeyBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        resourceKey_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object filename_ = "";
      /**
       * <code>optional string filename = 2;</code>
       */
      public boolean hasFilename() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string filename = 2;</code>
       */
      public java.lang.String getFilename() {
        java.lang.Object ref = filename_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            filename_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string filename = 2;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getFilenameBytes() {
        java.lang.Object ref = filename_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          filename_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string filename = 2;</code>
       */
      public Builder setFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        filename_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string filename = 2;</code>
       */
      public Builder clearFilename() {
        bitField0_ = (bitField0_ & ~0x00000002);
        filename_ = getDefaultInstance().getFilename();
        onChanged();
        return this;
      }
      /**
       * <code>optional string filename = 2;</code>
       */
      public Builder setFilenameBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        filename_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SCMUploaderNotifyRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SCMUploaderNotifyRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderNotifyRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<SCMUploaderNotifyRequestProto>() {
      @java.lang.Override
      public SCMUploaderNotifyRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new SCMUploaderNotifyRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderNotifyRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderNotifyRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SCMUploaderNotifyResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.SCMUploaderNotifyResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional bool accepted = 1;</code>
     */
    boolean hasAccepted();
    /**
     * <code>optional bool accepted = 1;</code>
     */
    boolean getAccepted();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SCMUploaderNotifyResponseProto}
   */
  public  static final class SCMUploaderNotifyResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.SCMUploaderNotifyResponseProto)
      SCMUploaderNotifyResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SCMUploaderNotifyResponseProto.newBuilder() to construct.
    private SCMUploaderNotifyResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SCMUploaderNotifyResponseProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SCMUploaderNotifyResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              accepted_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int ACCEPTED_FIELD_NUMBER = 1;
    private boolean accepted_;
    /**
     * <code>optional bool accepted = 1;</code>
     */
    public boolean hasAccepted() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional bool accepted = 1;</code>
     */
    public boolean getAccepted() {
      return accepted_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBool(1, accepted_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(1, accepted_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto) obj;

      if (hasAccepted() != other.hasAccepted()) return false;
      if (hasAccepted()) {
        if (getAccepted()
            != other.getAccepted()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAccepted()) {
        hash = (37 * hash) + ACCEPTED_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getAccepted());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SCMUploaderNotifyResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.SCMUploaderNotifyResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        accepted_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.accepted_ = accepted_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto.getDefaultInstance()) return this;
        if (other.hasAccepted()) {
          setAccepted(other.getAccepted());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private boolean accepted_ ;
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public boolean hasAccepted() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public boolean getAccepted() {
        return accepted_;
      }
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public Builder setAccepted(boolean value) {
        bitField0_ |= 0x00000001;
        accepted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public Builder clearAccepted() {
        bitField0_ = (bitField0_ & ~0x00000001);
        accepted_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SCMUploaderNotifyResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SCMUploaderNotifyResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderNotifyResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<SCMUploaderNotifyResponseProto>() {
      @java.lang.Override
      public SCMUploaderNotifyResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new SCMUploaderNotifyResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderNotifyResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderNotifyResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderNotifyResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SCMUploaderCanUploadRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.SCMUploaderCanUploadRequestProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional string resource_key = 1;</code>
     */
    boolean hasResourceKey();
    /**
     * <code>optional string resource_key = 1;</code>
     */
    java.lang.String getResourceKey();
    /**
     * <code>optional string resource_key = 1;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getResourceKeyBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SCMUploaderCanUploadRequestProto}
   */
  public  static final class SCMUploaderCanUploadRequestProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.SCMUploaderCanUploadRequestProto)
      SCMUploaderCanUploadRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SCMUploaderCanUploadRequestProto.newBuilder() to construct.
    private SCMUploaderCanUploadRequestProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SCMUploaderCanUploadRequestProto() {
      resourceKey_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SCMUploaderCanUploadRequestProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              resourceKey_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int RESOURCE_KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object resourceKey_;
    /**
     * <code>optional string resource_key = 1;</code>
     */
    public boolean hasResourceKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string resource_key = 1;</code>
     */
    public java.lang.String getResourceKey() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          resourceKey_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string resource_key = 1;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getResourceKeyBytes() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        resourceKey_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 1, resourceKey_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(1, resourceKey_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto) obj;

      if (hasResourceKey() != other.hasResourceKey()) return false;
      if (hasResourceKey()) {
        if (!getResourceKey()
            .equals(other.getResourceKey())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResourceKey()) {
        hash = (37 * hash) + RESOURCE_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getResourceKey().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SCMUploaderCanUploadRequestProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.SCMUploaderCanUploadRequestProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        resourceKey_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.resourceKey_ = resourceKey_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto.getDefaultInstance()) return this;
        if (other.hasResourceKey()) {
          bitField0_ |= 0x00000001;
          resourceKey_ = other.resourceKey_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object resourceKey_ = "";
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public boolean hasResourceKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public java.lang.String getResourceKey() {
        java.lang.Object ref = resourceKey_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            resourceKey_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getResourceKeyBytes() {
        java.lang.Object ref = resourceKey_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          resourceKey_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public Builder setResourceKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        resourceKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public Builder clearResourceKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        resourceKey_ = getDefaultInstance().getResourceKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string resource_key = 1;</code>
       */
      public Builder setResourceKeyBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        resourceKey_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SCMUploaderCanUploadRequestProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SCMUploaderCanUploadRequestProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderCanUploadRequestProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<SCMUploaderCanUploadRequestProto>() {
      @java.lang.Override
      public SCMUploaderCanUploadRequestProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new SCMUploaderCanUploadRequestProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderCanUploadRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderCanUploadRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SCMUploaderCanUploadResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.SCMUploaderCanUploadResponseProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional bool uploadable = 1;</code>
     */
    boolean hasUploadable();
    /**
     * <code>optional bool uploadable = 1;</code>
     */
    boolean getUploadable();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SCMUploaderCanUploadResponseProto}
   */
  public  static final class SCMUploaderCanUploadResponseProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.SCMUploaderCanUploadResponseProto)
      SCMUploaderCanUploadResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SCMUploaderCanUploadResponseProto.newBuilder() to construct.
    private SCMUploaderCanUploadResponseProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SCMUploaderCanUploadResponseProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SCMUploaderCanUploadResponseProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              uploadable_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int UPLOADABLE_FIELD_NUMBER = 1;
    private boolean uploadable_;
    /**
     * <code>optional bool uploadable = 1;</code>
     */
    public boolean hasUploadable() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional bool uploadable = 1;</code>
     */
    public boolean getUploadable() {
      return uploadable_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBool(1, uploadable_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBoolSize(1, uploadable_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto other = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto) obj;

      if (hasUploadable() != other.hasUploadable()) return false;
      if (hasUploadable()) {
        if (getUploadable()
            != other.getUploadable()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasUploadable()) {
        hash = (37 * hash) + UPLOADABLE_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashBoolean(
            getUploadable());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SCMUploaderCanUploadResponseProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.SCMUploaderCanUploadResponseProto)
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.class, org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        uploadable_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto result = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.uploadable_ = uploadable_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto.getDefaultInstance()) return this;
        if (other.hasUploadable()) {
          setUploadable(other.getUploadable());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private boolean uploadable_ ;
      /**
       * <code>optional bool uploadable = 1;</code>
       */
      public boolean hasUploadable() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional bool uploadable = 1;</code>
       */
      public boolean getUploadable() {
        return uploadable_;
      }
      /**
       * <code>optional bool uploadable = 1;</code>
       */
      public Builder setUploadable(boolean value) {
        bitField0_ |= 0x00000001;
        uploadable_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool uploadable = 1;</code>
       */
      public Builder clearUploadable() {
        bitField0_ = (bitField0_ & ~0x00000001);
        uploadable_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SCMUploaderCanUploadResponseProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SCMUploaderCanUploadResponseProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderCanUploadResponseProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<SCMUploaderCanUploadResponseProto>() {
      @java.lang.Override
      public SCMUploaderCanUploadResponseProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new SCMUploaderCanUploadResponseProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderCanUploadResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<SCMUploaderCanUploadResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.SCMUploaderCanUploadResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RemoteNodeProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_RemoteNodeProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NodeLabelsProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_NodeLabelsProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NodeAttributesProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_NodeAttributesProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NodeHeartbeatRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_NodeHeartbeatRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_LogAggregationReportProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_LogAggregationReportProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NodeHeartbeatResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_NodeHeartbeatResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ContainerQueuingLimitProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_ContainerQueuingLimitProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SystemCredentialsForAppsProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_SystemCredentialsForAppsProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_AppCollectorDataProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_AppCollectorDataProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NMContainerStatusProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_NMContainerStatusProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_fieldAccessorTable;

  public static org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\'yarn_server_common_service_protos.prot" +
      "o\022\013hadoop.yarn\032\016Security.proto\032\021yarn_pro" +
      "tos.proto\032\037yarn_server_common_protos.pro" +
      "to\032\031yarn_service_protos.proto\"}\n\017RemoteN" +
      "odeProto\022)\n\007node_id\030\001 \001(\0132\030.hadoop.yarn." +
      "NodeIdProto\022\024\n\014http_address\030\002 \001(\t\022\021\n\trac" +
      "k_name\030\003 \001(\t\022\026\n\016node_partition\030\004 \001(\t\"\264\003\n" +
      ",RegisterDistributedSchedulingAMResponse" +
      "Proto\022N\n\021register_response\030\001 \001(\01323.hadoo" +
      "p.yarn.RegisterApplicationMasterResponse" +
      "Proto\022:\n\026max_container_resource\030\002 \001(\0132\032." +
      "hadoop.yarn.ResourceProto\022:\n\026min_contain" +
      "er_resource\030\003 \001(\0132\032.hadoop.yarn.Resource" +
      "Proto\022;\n\027incr_container_resource\030\004 \001(\0132\032" +
      ".hadoop.yarn.ResourceProto\022\'\n\037container_" +
      "token_expiry_interval\030\005 \001(\005\022\032\n\022container" +
      "_id_start\030\006 \001(\003\022:\n\024nodes_for_scheduling\030" +
      "\007 \003(\0132\034.hadoop.yarn.RemoteNodeProto\"\247\001\n*" +
      "DistributedSchedulingAllocateResponsePro" +
      "to\022=\n\021allocate_response\030\001 \001(\0132\".hadoop.y" +
      "arn.AllocateResponseProto\022:\n\024nodes_for_s" +
      "cheduling\030\002 \003(\0132\034.hadoop.yarn.RemoteNode" +
      "Proto\"\243\001\n)DistributedSchedulingAllocateR" +
      "equestProto\022;\n\020allocate_request\030\001 \001(\0132!." +
      "hadoop.yarn.AllocateRequestProto\0229\n\024allo" +
      "cated_containers\030\002 \003(\0132\033.hadoop.yarn.Con" +
      "tainerProto\"B\n\017NodeLabelsProto\022/\n\nnodeLa" +
      "bels\030\001 \003(\0132\033.hadoop.yarn.NodeLabelProto\"" +
      "N\n\023NodeAttributesProto\0227\n\016nodeAttributes" +
      "\030\001 \003(\0132\037.hadoop.yarn.NodeAttributeProto\"" +
      "\306\004\n\037RegisterNodeManagerRequestProto\022)\n\007n" +
      "ode_id\030\001 \001(\0132\030.hadoop.yarn.NodeIdProto\022\021" +
      "\n\thttp_port\030\003 \001(\005\022,\n\010resource\030\004 \001(\0132\032.ha" +
      "doop.yarn.ResourceProto\022\022\n\nnm_version\030\005 " +
      "\001(\t\022?\n\022container_statuses\030\006 \003(\0132#.hadoop" +
      ".yarn.NMContainerStatusProto\022<\n\023runningA" +
      "pplications\030\007 \003(\0132\037.hadoop.yarn.Applicat" +
      "ionIdProto\0220\n\nnodeLabels\030\010 \001(\0132\034.hadoop." +
      "yarn.NodeLabelsProto\0224\n\020physicalResource" +
      "\030\t \001(\0132\032.hadoop.yarn.ResourceProto\022P\n lo" +
      "g_aggregation_reports_for_apps\030\n \003(\0132&.h" +
      "adoop.yarn.LogAggregationReportProto\0228\n\016" +
      "nodeAttributes\030\013 \001(\0132 .hadoop.yarn.NodeA" +
      "ttributesProto\0220\n\nnodeStatus\030\014 \001(\0132\034.had" +
      "oop.yarn.NodeStatusProto\"\235\003\n RegisterNod" +
      "eManagerResponseProto\022?\n\032container_token" +
      "_master_key\030\001 \001(\0132\033.hadoop.yarn.MasterKe" +
      "yProto\0228\n\023nm_token_master_key\030\002 \001(\0132\033.ha" +
      "doop.yarn.MasterKeyProto\0220\n\nnodeAction\030\003" +
      " \001(\0162\034.hadoop.yarn.NodeActionProto\022\025\n\rrm" +
      "_identifier\030\004 \001(\003\022\033\n\023diagnostics_message" +
      "\030\005 \001(\t\022\022\n\nrm_version\030\006 \001(\t\022(\n\031areNodeLab" +
      "elsAcceptedByRM\030\007 \001(\010:\005false\022,\n\010resource" +
      "\030\010 \001(\0132\032.hadoop.yarn.ResourceProto\022,\n\035ar" +
      "eNodeAttributesAcceptedByRM\030\t \001(\010:\005false" +
      "\"N\n!UnRegisterNodeManagerRequestProto\022)\n" +
      "\007node_id\030\001 \001(\0132\030.hadoop.yarn.NodeIdProto" +
      "\"$\n\"UnRegisterNodeManagerResponseProto\"\372" +
      "\003\n\031NodeHeartbeatRequestProto\0221\n\013node_sta" +
      "tus\030\001 \001(\0132\034.hadoop.yarn.NodeStatusProto\022" +
      "J\n%last_known_container_token_master_key" +
      "\030\002 \001(\0132\033.hadoop.yarn.MasterKeyProto\022C\n\036l" +
      "ast_known_nm_token_master_key\030\003 \001(\0132\033.ha" +
      "doop.yarn.MasterKeyProto\0220\n\nnodeLabels\030\004" +
      " \001(\0132\034.hadoop.yarn.NodeLabelsProto\022P\n lo" +
      "g_aggregation_reports_for_apps\030\005 \003(\0132&.h" +
      "adoop.yarn.LogAggregationReportProto\022B\n\026" +
      "registering_collectors\030\006 \003(\0132\".hadoop.ya" +
      "rn.AppCollectorDataProto\0228\n\016nodeAttribut" +
      "es\030\007 \001(\0132 .hadoop.yarn.NodeAttributesPro" +
      "to\022\027\n\017tokenSequenceNo\030\010 \001(\003\"\266\001\n\031LogAggre" +
      "gationReportProto\0227\n\016application_id\030\001 \001(" +
      "\0132\037.hadoop.yarn.ApplicationIdProto\022F\n\026lo" +
      "g_aggregation_status\030\002 \001(\0162&.hadoop.yarn" +
      ".LogAggregationStatusProto\022\030\n\013diagnostic" +
      "s\030\003 \001(\t:\003N/A\"\231\010\n\032NodeHeartbeatResponsePr" +
      "oto\022\023\n\013response_id\030\001 \001(\005\022?\n\032container_to" +
      "ken_master_key\030\002 \001(\0132\033.hadoop.yarn.Maste" +
      "rKeyProto\0228\n\023nm_token_master_key\030\003 \001(\0132\033" +
      ".hadoop.yarn.MasterKeyProto\0220\n\nnodeActio" +
      "n\030\004 \001(\0162\034.hadoop.yarn.NodeActionProto\022<\n" +
      "\025containers_to_cleanup\030\005 \003(\0132\035.hadoop.ya" +
      "rn.ContainerIdProto\022@\n\027applications_to_c" +
      "leanup\030\006 \003(\0132\037.hadoop.yarn.ApplicationId" +
      "Proto\022\035\n\025nextHeartBeatInterval\030\007 \001(\003\022\033\n\023" +
      "diagnostics_message\030\010 \001(\t\022G\n containers_" +
      "to_be_removed_from_nm\030\t \003(\0132\035.hadoop.yar" +
      "n.ContainerIdProto\022O\n\033system_credentials" +
      "_for_apps\030\n \003(\0132*.hadoop.yarn.SystemCred" +
      "entialsForAppsProto\022(\n\031areNodeLabelsAcce" +
      "ptedByRM\030\013 \001(\010:\005false\022;\n\026containers_to_d" +
      "ecrease\030\014 \003(\0132\033.hadoop.yarn.ContainerPro" +
      "to\022F\n\024containers_to_signal\030\r \003(\0132(.hadoo" +
      "p.yarn.SignalContainerRequestProto\022,\n\010re" +
      "source\030\016 \001(\0132\032.hadoop.yarn.ResourceProto" +
      "\022H\n\027container_queuing_limit\030\017 \001(\0132\'.hado" +
      "op.yarn.ContainerQueuingLimitProto\022:\n\016ap" +
      "p_collectors\030\020 \003(\0132\".hadoop.yarn.AppColl" +
      "ectorDataProto\0229\n\024containers_to_update\030\021" +
      " \003(\0132\033.hadoop.yarn.ContainerProto\022,\n\035are" +
      "NodeAttributesAcceptedByRM\030\022 \001(\010:\005false\022" +
      "\027\n\017tokenSequenceNo\030\023 \001(\003\"Y\n\032ContainerQue" +
      "uingLimitProto\022\030\n\020max_queue_length\030\001 \001(\005" +
      "\022!\n\031max_queue_wait_time_in_ms\030\002 \001(\005\"j\n\035S" +
      "ystemCredentialsForAppsProto\022.\n\005appId\030\001 " +
      "\001(\0132\037.hadoop.yarn.ApplicationIdProto\022\031\n\021" +
      "credentialsForApp\030\002 \001(\014\"\314\001\n\025AppCollector" +
      "DataProto\022/\n\006app_id\030\001 \001(\0132\037.hadoop.yarn." +
      "ApplicationIdProto\022\032\n\022app_collector_addr" +
      "\030\002 \001(\t\022\031\n\rrm_identifier\030\003 \001(\003:\002-1\022\023\n\007ver" +
      "sion\030\004 \001(\003:\002-1\0226\n\023app_collector_token\030\005 " +
      "\001(\0132\031.hadoop.common.TokenProto\"`\n\"Report" +
      "NewCollectorInfoRequestProto\022:\n\016app_coll" +
      "ectors\030\001 \003(\0132\".hadoop.yarn.AppCollectorD" +
      "ataProto\"%\n#ReportNewCollectorInfoRespon" +
      "seProto\"Y\n\'GetTimelineCollectorContextRe" +
      "questProto\022.\n\005appId\030\001 \001(\0132\037.hadoop.yarn." +
      "ApplicationIdProto\"y\n(GetTimelineCollect" +
      "orContextResponseProto\022\017\n\007user_id\030\001 \001(\t\022" +
      "\021\n\tflow_name\030\002 \001(\t\022\024\n\014flow_version\030\003 \001(\t" +
      "\022\023\n\013flow_run_id\030\004 \001(\003\"\342\003\n\026NMContainerSta" +
      "tusProto\0223\n\014container_id\030\001 \001(\0132\035.hadoop." +
      "yarn.ContainerIdProto\0229\n\017container_state" +
      "\030\002 \001(\0162 .hadoop.yarn.ContainerStateProto" +
      "\022,\n\010resource\030\003 \001(\0132\032.hadoop.yarn.Resourc" +
      "eProto\022,\n\010priority\030\004 \001(\0132\032.hadoop.yarn.P" +
      "riorityProto\022\030\n\013diagnostics\030\005 \001(\t:\003N/A\022\035" +
      "\n\025container_exit_status\030\006 \001(\005\022\025\n\rcreatio" +
      "n_time\030\007 \001(\003\022\033\n\023nodeLabelExpression\030\010 \001(" +
      "\t\022\017\n\007version\030\t \001(\005\022B\n\rexecutionType\030\n \001(" +
      "\0162\037.hadoop.yarn.ExecutionTypeProto:\nGUAR" +
      "ANTEED\022!\n\025allocation_request_id\030\013 \001(\003:\002-" +
      "1\022\027\n\017allocation_tags\030\014 \003(\t\"G\n\035SCMUploade" +
      "rNotifyRequestProto\022\024\n\014resource_key\030\001 \001(" +
      "\t\022\020\n\010filename\030\002 \001(\t\"2\n\036SCMUploaderNotify" +
      "ResponseProto\022\020\n\010accepted\030\001 \001(\010\"8\n SCMUp" +
      "loaderCanUploadRequestProto\022\024\n\014resource_" +
      "key\030\001 \001(\t\"7\n!SCMUploaderCanUploadRespons" +
      "eProto\022\022\n\nuploadable\030\001 \001(\010BC\n\034org.apache" +
      ".hadoop.yarn.protoB\035YarnServerCommonServ" +
      "iceProtos\210\001\001\240\001\001"
    };
    org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
          public org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry assignDescriptors(
              org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor root) {
            descriptor = root;
            return null;
          }
        };
    org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.security.proto.SecurityProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnServiceProtos.getDescriptor(),
        }, assigner);
    internal_static_hadoop_yarn_RemoteNodeProto_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_hadoop_yarn_RemoteNodeProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_RemoteNodeProto_descriptor,
        new java.lang.String[] { "NodeId", "HttpAddress", "RackName", "NodePartition", });
    internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_RegisterDistributedSchedulingAMResponseProto_descriptor,
        new java.lang.String[] { "RegisterResponse", "MaxContainerResource", "MinContainerResource", "IncrContainerResource", "ContainerTokenExpiryInterval", "ContainerIdStart", "NodesForScheduling", });
    internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_DistributedSchedulingAllocateResponseProto_descriptor,
        new java.lang.String[] { "AllocateResponse", "NodesForScheduling", });
    internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_DistributedSchedulingAllocateRequestProto_descriptor,
        new java.lang.String[] { "AllocateRequest", "AllocatedContainers", });
    internal_static_hadoop_yarn_NodeLabelsProto_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_hadoop_yarn_NodeLabelsProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_NodeLabelsProto_descriptor,
        new java.lang.String[] { "NodeLabels", });
    internal_static_hadoop_yarn_NodeAttributesProto_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_hadoop_yarn_NodeAttributesProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_NodeAttributesProto_descriptor,
        new java.lang.String[] { "NodeAttributes", });
    internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_RegisterNodeManagerRequestProto_descriptor,
        new java.lang.String[] { "NodeId", "HttpPort", "Resource", "NmVersion", "ContainerStatuses", "RunningApplications", "NodeLabels", "PhysicalResource", "LogAggregationReportsForApps", "NodeAttributes", "NodeStatus", });
    internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_RegisterNodeManagerResponseProto_descriptor,
        new java.lang.String[] { "ContainerTokenMasterKey", "NmTokenMasterKey", "NodeAction", "RmIdentifier", "DiagnosticsMessage", "RmVersion", "AreNodeLabelsAcceptedByRM", "Resource", "AreNodeAttributesAcceptedByRM", });
    internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_UnRegisterNodeManagerRequestProto_descriptor,
        new java.lang.String[] { "NodeId", });
    internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_UnRegisterNodeManagerResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_hadoop_yarn_NodeHeartbeatRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_hadoop_yarn_NodeHeartbeatRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_NodeHeartbeatRequestProto_descriptor,
        new java.lang.String[] { "NodeStatus", "LastKnownContainerTokenMasterKey", "LastKnownNmTokenMasterKey", "NodeLabels", "LogAggregationReportsForApps", "RegisteringCollectors", "NodeAttributes", "TokenSequenceNo", });
    internal_static_hadoop_yarn_LogAggregationReportProto_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_hadoop_yarn_LogAggregationReportProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_LogAggregationReportProto_descriptor,
        new java.lang.String[] { "ApplicationId", "LogAggregationStatus", "Diagnostics", });
    internal_static_hadoop_yarn_NodeHeartbeatResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_hadoop_yarn_NodeHeartbeatResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_NodeHeartbeatResponseProto_descriptor,
        new java.lang.String[] { "ResponseId", "ContainerTokenMasterKey", "NmTokenMasterKey", "NodeAction", "ContainersToCleanup", "ApplicationsToCleanup", "NextHeartBeatInterval", "DiagnosticsMessage", "ContainersToBeRemovedFromNm", "SystemCredentialsForApps", "AreNodeLabelsAcceptedByRM", "ContainersToDecrease", "ContainersToSignal", "Resource", "ContainerQueuingLimit", "AppCollectors", "ContainersToUpdate", "AreNodeAttributesAcceptedByRM", "TokenSequenceNo", });
    internal_static_hadoop_yarn_ContainerQueuingLimitProto_descriptor =
      getDescriptor().getMessageTypes().get(13);
    internal_static_hadoop_yarn_ContainerQueuingLimitProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_ContainerQueuingLimitProto_descriptor,
        new java.lang.String[] { "MaxQueueLength", "MaxQueueWaitTimeInMs", });
    internal_static_hadoop_yarn_SystemCredentialsForAppsProto_descriptor =
      getDescriptor().getMessageTypes().get(14);
    internal_static_hadoop_yarn_SystemCredentialsForAppsProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_SystemCredentialsForAppsProto_descriptor,
        new java.lang.String[] { "AppId", "CredentialsForApp", });
    internal_static_hadoop_yarn_AppCollectorDataProto_descriptor =
      getDescriptor().getMessageTypes().get(15);
    internal_static_hadoop_yarn_AppCollectorDataProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_AppCollectorDataProto_descriptor,
        new java.lang.String[] { "AppId", "AppCollectorAddr", "RmIdentifier", "Version", "AppCollectorToken", });
    internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(16);
    internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_ReportNewCollectorInfoRequestProto_descriptor,
        new java.lang.String[] { "AppCollectors", });
    internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(17);
    internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_ReportNewCollectorInfoResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(18);
    internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_GetTimelineCollectorContextRequestProto_descriptor,
        new java.lang.String[] { "AppId", });
    internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(19);
    internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_GetTimelineCollectorContextResponseProto_descriptor,
        new java.lang.String[] { "UserId", "FlowName", "FlowVersion", "FlowRunId", });
    internal_static_hadoop_yarn_NMContainerStatusProto_descriptor =
      getDescriptor().getMessageTypes().get(20);
    internal_static_hadoop_yarn_NMContainerStatusProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_NMContainerStatusProto_descriptor,
        new java.lang.String[] { "ContainerId", "ContainerState", "Resource", "Priority", "Diagnostics", "ContainerExitStatus", "CreationTime", "NodeLabelExpression", "Version", "ExecutionType", "AllocationRequestId", "AllocationTags", });
    internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(21);
    internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_SCMUploaderNotifyRequestProto_descriptor,
        new java.lang.String[] { "ResourceKey", "Filename", });
    internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(22);
    internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_SCMUploaderNotifyResponseProto_descriptor,
        new java.lang.String[] { "Accepted", });
    internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(23);
    internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_SCMUploaderCanUploadRequestProto_descriptor,
        new java.lang.String[] { "ResourceKey", });
    internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(24);
    internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_SCMUploaderCanUploadResponseProto_descriptor,
        new java.lang.String[] { "Uploadable", });
    org.apache.hadoop.security.proto.SecurityProtos.getDescriptor();
    org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor();
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.getDescriptor();
    org.apache.hadoop.yarn.proto.YarnServiceProtos.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}
