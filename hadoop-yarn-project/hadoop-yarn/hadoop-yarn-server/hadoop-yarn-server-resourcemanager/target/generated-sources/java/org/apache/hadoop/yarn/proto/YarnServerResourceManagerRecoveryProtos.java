// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: yarn_server_resourcemanager_recovery.proto

package org.apache.hadoop.yarn.proto;

public final class YarnServerResourceManagerRecoveryProtos {
  private YarnServerResourceManagerRecoveryProtos() {}
  public static void registerAllExtensions(
      org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * <pre>
   *&#47;/////////////////////////////////////////////////////////////////////
   * //// RM recovery related records /////////////////////////////////////
   * //////////////////////////////////////////////////////////////////////
   * </pre>
   *
   * Protobuf enum {@code hadoop.yarn.RMAppAttemptStateProto}
   */
  public enum RMAppAttemptStateProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>RMATTEMPT_NEW = 1;</code>
     */
    RMATTEMPT_NEW(1),
    /**
     * <code>RMATTEMPT_SUBMITTED = 2;</code>
     */
    RMATTEMPT_SUBMITTED(2),
    /**
     * <code>RMATTEMPT_SCHEDULED = 3;</code>
     */
    RMATTEMPT_SCHEDULED(3),
    /**
     * <code>RMATTEMPT_ALLOCATED = 4;</code>
     */
    RMATTEMPT_ALLOCATED(4),
    /**
     * <code>RMATTEMPT_LAUNCHED = 5;</code>
     */
    RMATTEMPT_LAUNCHED(5),
    /**
     * <code>RMATTEMPT_FAILED = 6;</code>
     */
    RMATTEMPT_FAILED(6),
    /**
     * <code>RMATTEMPT_RUNNING = 7;</code>
     */
    RMATTEMPT_RUNNING(7),
    /**
     * <code>RMATTEMPT_FINISHING = 8;</code>
     */
    RMATTEMPT_FINISHING(8),
    /**
     * <code>RMATTEMPT_FINISHED = 9;</code>
     */
    RMATTEMPT_FINISHED(9),
    /**
     * <code>RMATTEMPT_KILLED = 10;</code>
     */
    RMATTEMPT_KILLED(10),
    /**
     * <code>RMATTEMPT_ALLOCATED_SAVING = 11;</code>
     */
    RMATTEMPT_ALLOCATED_SAVING(11),
    /**
     * <code>RMATTEMPT_LAUNCHED_UNMANAGED_SAVING = 12;</code>
     */
    RMATTEMPT_LAUNCHED_UNMANAGED_SAVING(12),
    /**
     * <code>RMATTEMPT_RECOVERED = 13;</code>
     */
    RMATTEMPT_RECOVERED(13),
    /**
     * <code>RMATTEMPT_FINAL_SAVING = 14;</code>
     */
    RMATTEMPT_FINAL_SAVING(14),
    ;

    /**
     * <code>RMATTEMPT_NEW = 1;</code>
     */
    public static final int RMATTEMPT_NEW_VALUE = 1;
    /**
     * <code>RMATTEMPT_SUBMITTED = 2;</code>
     */
    public static final int RMATTEMPT_SUBMITTED_VALUE = 2;
    /**
     * <code>RMATTEMPT_SCHEDULED = 3;</code>
     */
    public static final int RMATTEMPT_SCHEDULED_VALUE = 3;
    /**
     * <code>RMATTEMPT_ALLOCATED = 4;</code>
     */
    public static final int RMATTEMPT_ALLOCATED_VALUE = 4;
    /**
     * <code>RMATTEMPT_LAUNCHED = 5;</code>
     */
    public static final int RMATTEMPT_LAUNCHED_VALUE = 5;
    /**
     * <code>RMATTEMPT_FAILED = 6;</code>
     */
    public static final int RMATTEMPT_FAILED_VALUE = 6;
    /**
     * <code>RMATTEMPT_RUNNING = 7;</code>
     */
    public static final int RMATTEMPT_RUNNING_VALUE = 7;
    /**
     * <code>RMATTEMPT_FINISHING = 8;</code>
     */
    public static final int RMATTEMPT_FINISHING_VALUE = 8;
    /**
     * <code>RMATTEMPT_FINISHED = 9;</code>
     */
    public static final int RMATTEMPT_FINISHED_VALUE = 9;
    /**
     * <code>RMATTEMPT_KILLED = 10;</code>
     */
    public static final int RMATTEMPT_KILLED_VALUE = 10;
    /**
     * <code>RMATTEMPT_ALLOCATED_SAVING = 11;</code>
     */
    public static final int RMATTEMPT_ALLOCATED_SAVING_VALUE = 11;
    /**
     * <code>RMATTEMPT_LAUNCHED_UNMANAGED_SAVING = 12;</code>
     */
    public static final int RMATTEMPT_LAUNCHED_UNMANAGED_SAVING_VALUE = 12;
    /**
     * <code>RMATTEMPT_RECOVERED = 13;</code>
     */
    public static final int RMATTEMPT_RECOVERED_VALUE = 13;
    /**
     * <code>RMATTEMPT_FINAL_SAVING = 14;</code>
     */
    public static final int RMATTEMPT_FINAL_SAVING_VALUE = 14;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static RMAppAttemptStateProto valueOf(int value) {
      return forNumber(value);
    }

    public static RMAppAttemptStateProto forNumber(int value) {
      switch (value) {
        case 1: return RMATTEMPT_NEW;
        case 2: return RMATTEMPT_SUBMITTED;
        case 3: return RMATTEMPT_SCHEDULED;
        case 4: return RMATTEMPT_ALLOCATED;
        case 5: return RMATTEMPT_LAUNCHED;
        case 6: return RMATTEMPT_FAILED;
        case 7: return RMATTEMPT_RUNNING;
        case 8: return RMATTEMPT_FINISHING;
        case 9: return RMATTEMPT_FINISHED;
        case 10: return RMATTEMPT_KILLED;
        case 11: return RMATTEMPT_ALLOCATED_SAVING;
        case 12: return RMATTEMPT_LAUNCHED_UNMANAGED_SAVING;
        case 13: return RMATTEMPT_RECOVERED;
        case 14: return RMATTEMPT_FINAL_SAVING;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<RMAppAttemptStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        RMAppAttemptStateProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<RMAppAttemptStateProto>() {
            public RMAppAttemptStateProto findValueByNumber(int number) {
              return RMAppAttemptStateProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final RMAppAttemptStateProto[] VALUES = values();

    public static RMAppAttemptStateProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private RMAppAttemptStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.yarn.RMAppAttemptStateProto)
  }

  /**
   * Protobuf enum {@code hadoop.yarn.RMAppStateProto}
   */
  public enum RMAppStateProto
      implements org.apache.hadoop.thirdparty.protobuf.ProtocolMessageEnum {
    /**
     * <code>RMAPP_NEW = 1;</code>
     */
    RMAPP_NEW(1),
    /**
     * <code>RMAPP_NEW_SAVING = 2;</code>
     */
    RMAPP_NEW_SAVING(2),
    /**
     * <code>RMAPP_SUBMITTED = 3;</code>
     */
    RMAPP_SUBMITTED(3),
    /**
     * <code>RMAPP_ACCEPTED = 4;</code>
     */
    RMAPP_ACCEPTED(4),
    /**
     * <code>RMAPP_RUNNING = 5;</code>
     */
    RMAPP_RUNNING(5),
    /**
     * <code>RMAPP_FINAL_SAVING = 6;</code>
     */
    RMAPP_FINAL_SAVING(6),
    /**
     * <code>RMAPP_FINISHING = 7;</code>
     */
    RMAPP_FINISHING(7),
    /**
     * <code>RMAPP_FINISHED = 8;</code>
     */
    RMAPP_FINISHED(8),
    /**
     * <code>RMAPP_FAILED = 9;</code>
     */
    RMAPP_FAILED(9),
    /**
     * <code>RMAPP_KILLED = 10;</code>
     */
    RMAPP_KILLED(10),
    ;

    /**
     * <code>RMAPP_NEW = 1;</code>
     */
    public static final int RMAPP_NEW_VALUE = 1;
    /**
     * <code>RMAPP_NEW_SAVING = 2;</code>
     */
    public static final int RMAPP_NEW_SAVING_VALUE = 2;
    /**
     * <code>RMAPP_SUBMITTED = 3;</code>
     */
    public static final int RMAPP_SUBMITTED_VALUE = 3;
    /**
     * <code>RMAPP_ACCEPTED = 4;</code>
     */
    public static final int RMAPP_ACCEPTED_VALUE = 4;
    /**
     * <code>RMAPP_RUNNING = 5;</code>
     */
    public static final int RMAPP_RUNNING_VALUE = 5;
    /**
     * <code>RMAPP_FINAL_SAVING = 6;</code>
     */
    public static final int RMAPP_FINAL_SAVING_VALUE = 6;
    /**
     * <code>RMAPP_FINISHING = 7;</code>
     */
    public static final int RMAPP_FINISHING_VALUE = 7;
    /**
     * <code>RMAPP_FINISHED = 8;</code>
     */
    public static final int RMAPP_FINISHED_VALUE = 8;
    /**
     * <code>RMAPP_FAILED = 9;</code>
     */
    public static final int RMAPP_FAILED_VALUE = 9;
    /**
     * <code>RMAPP_KILLED = 10;</code>
     */
    public static final int RMAPP_KILLED_VALUE = 10;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static RMAppStateProto valueOf(int value) {
      return forNumber(value);
    }

    public static RMAppStateProto forNumber(int value) {
      switch (value) {
        case 1: return RMAPP_NEW;
        case 2: return RMAPP_NEW_SAVING;
        case 3: return RMAPP_SUBMITTED;
        case 4: return RMAPP_ACCEPTED;
        case 5: return RMAPP_RUNNING;
        case 6: return RMAPP_FINAL_SAVING;
        case 7: return RMAPP_FINISHING;
        case 8: return RMAPP_FINISHED;
        case 9: return RMAPP_FAILED;
        case 10: return RMAPP_KILLED;
        default: return null;
      }
    }

    public static org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<RMAppStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<
        RMAppStateProto> internalValueMap =
          new org.apache.hadoop.thirdparty.protobuf.Internal.EnumLiteMap<RMAppStateProto>() {
            public RMAppStateProto findValueByNumber(int number) {
              return RMAppStateProto.forNumber(number);
            }
          };

    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.getDescriptor().getEnumTypes().get(1);
    }

    private static final RMAppStateProto[] VALUES = values();

    public static RMAppStateProto valueOf(
        org.apache.hadoop.thirdparty.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private RMAppStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.yarn.RMAppStateProto)
  }

  public interface ApplicationStateDataProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.ApplicationStateDataProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional int64 submit_time = 1;</code>
     */
    boolean hasSubmitTime();
    /**
     * <code>optional int64 submit_time = 1;</code>
     */
    long getSubmitTime();

    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
     */
    boolean hasApplicationSubmissionContext();
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext();
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder();

    /**
     * <code>optional string user = 3;</code>
     */
    boolean hasUser();
    /**
     * <code>optional string user = 3;</code>
     */
    java.lang.String getUser();
    /**
     * <code>optional string user = 3;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getUserBytes();

    /**
     * <code>optional int64 start_time = 4;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    long getStartTime();

    /**
     * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
     */
    boolean hasApplicationState();
    /**
     * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto getApplicationState();

    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes();

    /**
     * <code>optional int64 finish_time = 7;</code>
     */
    boolean hasFinishTime();
    /**
     * <code>optional int64 finish_time = 7;</code>
     */
    long getFinishTime();

    /**
     * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
     */
    boolean hasCallerContext();
    /**
     * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
     */
    org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto getCallerContext();
    /**
     * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
     */
    org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProtoOrBuilder getCallerContextOrBuilder();

    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto> 
        getApplicationTimeoutsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto getApplicationTimeouts(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    int getApplicationTimeoutsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder> 
        getApplicationTimeoutsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder getApplicationTimeoutsOrBuilder(
        int index);

    /**
     * <code>optional int64 launch_time = 10;</code>
     */
    boolean hasLaunchTime();
    /**
     * <code>optional int64 launch_time = 10;</code>
     */
    long getLaunchTime();

    /**
     * <code>optional string real_user = 11;</code>
     */
    boolean hasRealUser();
    /**
     * <code>optional string real_user = 11;</code>
     */
    java.lang.String getRealUser();
    /**
     * <code>optional string real_user = 11;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getRealUserBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ApplicationStateDataProto}
   */
  public  static final class ApplicationStateDataProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.ApplicationStateDataProto)
      ApplicationStateDataProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationStateDataProto.newBuilder() to construct.
    private ApplicationStateDataProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationStateDataProto() {
      user_ = "";
      applicationState_ = 1;
      diagnostics_ = "N/A";
      applicationTimeouts_ = java.util.Collections.emptyList();
      realUser_ = "";
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationStateDataProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              submitTime_ = input.readInt64();
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = applicationSubmissionContext_.toBuilder();
              }
              applicationSubmissionContext_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationSubmissionContext_);
                applicationSubmissionContext_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              user_ = bs;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              startTime_ = input.readInt64();
              break;
            }
            case 40: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto value = org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(5, rawValue);
              } else {
                bitField0_ |= 0x00000010;
                applicationState_ = rawValue;
              }
              break;
            }
            case 50: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000020;
              diagnostics_ = bs;
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              finishTime_ = input.readInt64();
              break;
            }
            case 66: {
              org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) != 0)) {
                subBuilder = callerContext_.toBuilder();
              }
              callerContext_ = input.readMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(callerContext_);
                callerContext_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                applicationTimeouts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto>();
                mutable_bitField0_ |= 0x00000100;
              }
              applicationTimeouts_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.PARSER, extensionRegistry));
              break;
            }
            case 80: {
              bitField0_ |= 0x00000100;
              launchTime_ = input.readInt64();
              break;
            }
            case 90: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000200;
              realUser_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          applicationTimeouts_ = java.util.Collections.unmodifiableList(applicationTimeouts_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationStateDataProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationStateDataProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.Builder.class);
    }

    private int bitField0_;
    public static final int SUBMIT_TIME_FIELD_NUMBER = 1;
    private long submitTime_;
    /**
     * <code>optional int64 submit_time = 1;</code>
     */
    public boolean hasSubmitTime() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int64 submit_time = 1;</code>
     */
    public long getSubmitTime() {
      return submitTime_;
    }

    public static final int APPLICATION_SUBMISSION_CONTEXT_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto applicationSubmissionContext_;
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
     */
    public boolean hasApplicationSubmissionContext() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext() {
      return applicationSubmissionContext_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance() : applicationSubmissionContext_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder() {
      return applicationSubmissionContext_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance() : applicationSubmissionContext_;
    }

    public static final int USER_FIELD_NUMBER = 3;
    private volatile java.lang.Object user_;
    /**
     * <code>optional string user = 3;</code>
     */
    public boolean hasUser() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional string user = 3;</code>
     */
    public java.lang.String getUser() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          user_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string user = 3;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getUserBytes() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        user_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int START_TIME_FIELD_NUMBER = 4;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    public static final int APPLICATION_STATE_FIELD_NUMBER = 5;
    private int applicationState_;
    /**
     * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
     */
    public boolean hasApplicationState() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto getApplicationState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto result = org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto.valueOf(applicationState_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto.RMAPP_NEW : result;
    }

    public static final int DIAGNOSTICS_FIELD_NUMBER = 6;
    private volatile java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int FINISH_TIME_FIELD_NUMBER = 7;
    private long finishTime_;
    /**
     * <code>optional int64 finish_time = 7;</code>
     */
    public boolean hasFinishTime() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional int64 finish_time = 7;</code>
     */
    public long getFinishTime() {
      return finishTime_;
    }

    public static final int CALLER_CONTEXT_FIELD_NUMBER = 8;
    private org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto callerContext_;
    /**
     * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
     */
    public boolean hasCallerContext() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
     */
    public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto getCallerContext() {
      return callerContext_ == null ? org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.getDefaultInstance() : callerContext_;
    }
    /**
     * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
     */
    public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProtoOrBuilder getCallerContextOrBuilder() {
      return callerContext_ == null ? org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.getDefaultInstance() : callerContext_;
    }

    public static final int APPLICATION_TIMEOUTS_FIELD_NUMBER = 9;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto> applicationTimeouts_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto> getApplicationTimeoutsList() {
      return applicationTimeouts_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder> 
        getApplicationTimeoutsOrBuilderList() {
      return applicationTimeouts_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    public int getApplicationTimeoutsCount() {
      return applicationTimeouts_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto getApplicationTimeouts(int index) {
      return applicationTimeouts_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder getApplicationTimeoutsOrBuilder(
        int index) {
      return applicationTimeouts_.get(index);
    }

    public static final int LAUNCH_TIME_FIELD_NUMBER = 10;
    private long launchTime_;
    /**
     * <code>optional int64 launch_time = 10;</code>
     */
    public boolean hasLaunchTime() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional int64 launch_time = 10;</code>
     */
    public long getLaunchTime() {
      return launchTime_;
    }

    public static final int REAL_USER_FIELD_NUMBER = 11;
    private volatile java.lang.Object realUser_;
    /**
     * <code>optional string real_user = 11;</code>
     */
    public boolean hasRealUser() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>optional string real_user = 11;</code>
     */
    public java.lang.String getRealUser() {
      java.lang.Object ref = realUser_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          realUser_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string real_user = 11;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getRealUserBytes() {
      java.lang.Object ref = realUser_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        realUser_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasApplicationSubmissionContext()) {
        if (!getApplicationSubmissionContext().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasCallerContext()) {
        if (!getCallerContext().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(1, submitTime_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getApplicationSubmissionContext());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 3, user_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, startTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeEnum(5, applicationState_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 6, diagnostics_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeInt64(7, finishTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeMessage(8, getCallerContext());
      }
      for (int i = 0; i < applicationTimeouts_.size(); i++) {
        output.writeMessage(9, applicationTimeouts_.get(i));
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeInt64(10, launchTime_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 11, realUser_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(1, submitTime_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getApplicationSubmissionContext());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(3, user_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(4, startTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(5, applicationState_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(6, diagnostics_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(7, finishTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(8, getCallerContext());
      }
      for (int i = 0; i < applicationTimeouts_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(9, applicationTimeouts_.get(i));
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(10, launchTime_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(11, realUser_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto other = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto) obj;

      if (hasSubmitTime() != other.hasSubmitTime()) return false;
      if (hasSubmitTime()) {
        if (getSubmitTime()
            != other.getSubmitTime()) return false;
      }
      if (hasApplicationSubmissionContext() != other.hasApplicationSubmissionContext()) return false;
      if (hasApplicationSubmissionContext()) {
        if (!getApplicationSubmissionContext()
            .equals(other.getApplicationSubmissionContext())) return false;
      }
      if (hasUser() != other.hasUser()) return false;
      if (hasUser()) {
        if (!getUser()
            .equals(other.getUser())) return false;
      }
      if (hasStartTime() != other.hasStartTime()) return false;
      if (hasStartTime()) {
        if (getStartTime()
            != other.getStartTime()) return false;
      }
      if (hasApplicationState() != other.hasApplicationState()) return false;
      if (hasApplicationState()) {
        if (applicationState_ != other.applicationState_) return false;
      }
      if (hasDiagnostics() != other.hasDiagnostics()) return false;
      if (hasDiagnostics()) {
        if (!getDiagnostics()
            .equals(other.getDiagnostics())) return false;
      }
      if (hasFinishTime() != other.hasFinishTime()) return false;
      if (hasFinishTime()) {
        if (getFinishTime()
            != other.getFinishTime()) return false;
      }
      if (hasCallerContext() != other.hasCallerContext()) return false;
      if (hasCallerContext()) {
        if (!getCallerContext()
            .equals(other.getCallerContext())) return false;
      }
      if (!getApplicationTimeoutsList()
          .equals(other.getApplicationTimeoutsList())) return false;
      if (hasLaunchTime() != other.hasLaunchTime()) return false;
      if (hasLaunchTime()) {
        if (getLaunchTime()
            != other.getLaunchTime()) return false;
      }
      if (hasRealUser() != other.hasRealUser()) return false;
      if (hasRealUser()) {
        if (!getRealUser()
            .equals(other.getRealUser())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSubmitTime()) {
        hash = (37 * hash) + SUBMIT_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getSubmitTime());
      }
      if (hasApplicationSubmissionContext()) {
        hash = (37 * hash) + APPLICATION_SUBMISSION_CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationSubmissionContext().hashCode();
      }
      if (hasUser()) {
        hash = (37 * hash) + USER_FIELD_NUMBER;
        hash = (53 * hash) + getUser().hashCode();
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getStartTime());
      }
      if (hasApplicationState()) {
        hash = (37 * hash) + APPLICATION_STATE_FIELD_NUMBER;
        hash = (53 * hash) + applicationState_;
      }
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      if (hasFinishTime()) {
        hash = (37 * hash) + FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getFinishTime());
      }
      if (hasCallerContext()) {
        hash = (37 * hash) + CALLER_CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getCallerContext().hashCode();
      }
      if (getApplicationTimeoutsCount() > 0) {
        hash = (37 * hash) + APPLICATION_TIMEOUTS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationTimeoutsList().hashCode();
      }
      if (hasLaunchTime()) {
        hash = (37 * hash) + LAUNCH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getLaunchTime());
      }
      if (hasRealUser()) {
        hash = (37 * hash) + REAL_USER_FIELD_NUMBER;
        hash = (53 * hash) + getRealUser().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ApplicationStateDataProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.ApplicationStateDataProto)
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationStateDataProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationStateDataProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getApplicationSubmissionContextFieldBuilder();
          getCallerContextFieldBuilder();
          getApplicationTimeoutsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        submitTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = null;
        } else {
          applicationSubmissionContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        user_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        applicationState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000010);
        diagnostics_ = "N/A";
        bitField0_ = (bitField0_ & ~0x00000020);
        finishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (callerContextBuilder_ == null) {
          callerContext_ = null;
        } else {
          callerContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (applicationTimeoutsBuilder_ == null) {
          applicationTimeouts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          applicationTimeoutsBuilder_.clear();
        }
        launchTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        realUser_ = "";
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationStateDataProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto build() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto result = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.submitTime_ = submitTime_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (applicationSubmissionContextBuilder_ == null) {
            result.applicationSubmissionContext_ = applicationSubmissionContext_;
          } else {
            result.applicationSubmissionContext_ = applicationSubmissionContextBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.user_ = user_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.startTime_ = startTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.applicationState_ = applicationState_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.diagnostics_ = diagnostics_;
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.finishTime_ = finishTime_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          if (callerContextBuilder_ == null) {
            result.callerContext_ = callerContext_;
          } else {
            result.callerContext_ = callerContextBuilder_.build();
          }
          to_bitField0_ |= 0x00000080;
        }
        if (applicationTimeoutsBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0)) {
            applicationTimeouts_ = java.util.Collections.unmodifiableList(applicationTimeouts_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.applicationTimeouts_ = applicationTimeouts_;
        } else {
          result.applicationTimeouts_ = applicationTimeoutsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000200) != 0)) {
          result.launchTime_ = launchTime_;
          to_bitField0_ |= 0x00000100;
        }
        if (((from_bitField0_ & 0x00000400) != 0)) {
          to_bitField0_ |= 0x00000200;
        }
        result.realUser_ = realUser_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto.getDefaultInstance()) return this;
        if (other.hasSubmitTime()) {
          setSubmitTime(other.getSubmitTime());
        }
        if (other.hasApplicationSubmissionContext()) {
          mergeApplicationSubmissionContext(other.getApplicationSubmissionContext());
        }
        if (other.hasUser()) {
          bitField0_ |= 0x00000004;
          user_ = other.user_;
          onChanged();
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasApplicationState()) {
          setApplicationState(other.getApplicationState());
        }
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000020;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        if (other.hasFinishTime()) {
          setFinishTime(other.getFinishTime());
        }
        if (other.hasCallerContext()) {
          mergeCallerContext(other.getCallerContext());
        }
        if (applicationTimeoutsBuilder_ == null) {
          if (!other.applicationTimeouts_.isEmpty()) {
            if (applicationTimeouts_.isEmpty()) {
              applicationTimeouts_ = other.applicationTimeouts_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureApplicationTimeoutsIsMutable();
              applicationTimeouts_.addAll(other.applicationTimeouts_);
            }
            onChanged();
          }
        } else {
          if (!other.applicationTimeouts_.isEmpty()) {
            if (applicationTimeoutsBuilder_.isEmpty()) {
              applicationTimeoutsBuilder_.dispose();
              applicationTimeoutsBuilder_ = null;
              applicationTimeouts_ = other.applicationTimeouts_;
              bitField0_ = (bitField0_ & ~0x00000100);
              applicationTimeoutsBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getApplicationTimeoutsFieldBuilder() : null;
            } else {
              applicationTimeoutsBuilder_.addAllMessages(other.applicationTimeouts_);
            }
          }
        }
        if (other.hasLaunchTime()) {
          setLaunchTime(other.getLaunchTime());
        }
        if (other.hasRealUser()) {
          bitField0_ |= 0x00000400;
          realUser_ = other.realUser_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasApplicationSubmissionContext()) {
          if (!getApplicationSubmissionContext().isInitialized()) {
            return false;
          }
        }
        if (hasCallerContext()) {
          if (!getCallerContext().isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long submitTime_ ;
      /**
       * <code>optional int64 submit_time = 1;</code>
       */
      public boolean hasSubmitTime() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int64 submit_time = 1;</code>
       */
      public long getSubmitTime() {
        return submitTime_;
      }
      /**
       * <code>optional int64 submit_time = 1;</code>
       */
      public Builder setSubmitTime(long value) {
        bitField0_ |= 0x00000001;
        submitTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 submit_time = 1;</code>
       */
      public Builder clearSubmitTime() {
        bitField0_ = (bitField0_ & ~0x00000001);
        submitTime_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto applicationSubmissionContext_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder> applicationSubmissionContextBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public boolean hasApplicationSubmissionContext() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext() {
        if (applicationSubmissionContextBuilder_ == null) {
          return applicationSubmissionContext_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance() : applicationSubmissionContext_;
        } else {
          return applicationSubmissionContextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public Builder setApplicationSubmissionContext(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto value) {
        if (applicationSubmissionContextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationSubmissionContext_ = value;
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public Builder setApplicationSubmissionContext(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder builderForValue) {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = builderForValue.build();
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public Builder mergeApplicationSubmissionContext(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto value) {
        if (applicationSubmissionContextBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              applicationSubmissionContext_ != null &&
              applicationSubmissionContext_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance()) {
            applicationSubmissionContext_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.newBuilder(applicationSubmissionContext_).mergeFrom(value).buildPartial();
          } else {
            applicationSubmissionContext_ = value;
          }
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public Builder clearApplicationSubmissionContext() {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = null;
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder getApplicationSubmissionContextBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getApplicationSubmissionContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder() {
        if (applicationSubmissionContextBuilder_ != null) {
          return applicationSubmissionContextBuilder_.getMessageOrBuilder();
        } else {
          return applicationSubmissionContext_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance() : applicationSubmissionContext_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder> 
          getApplicationSubmissionContextFieldBuilder() {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContextBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder>(
                  getApplicationSubmissionContext(),
                  getParentForChildren(),
                  isClean());
          applicationSubmissionContext_ = null;
        }
        return applicationSubmissionContextBuilder_;
      }

      private java.lang.Object user_ = "";
      /**
       * <code>optional string user = 3;</code>
       */
      public boolean hasUser() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional string user = 3;</code>
       */
      public java.lang.String getUser() {
        java.lang.Object ref = user_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            user_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string user = 3;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getUserBytes() {
        java.lang.Object ref = user_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          user_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string user = 3;</code>
       */
      public Builder setUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        user_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 3;</code>
       */
      public Builder clearUser() {
        bitField0_ = (bitField0_ & ~0x00000004);
        user_ = getDefaultInstance().getUser();
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 3;</code>
       */
      public Builder setUserBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        user_ = value;
        onChanged();
        return this;
      }

      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000008;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private int applicationState_ = 1;
      /**
       * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
       */
      public boolean hasApplicationState() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto getApplicationState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto result = org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto.valueOf(applicationState_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto.RMAPP_NEW : result;
      }
      /**
       * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
       */
      public Builder setApplicationState(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000010;
        applicationState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.RMAppStateProto application_state = 5;</code>
       */
      public Builder clearApplicationState() {
        bitField0_ = (bitField0_ & ~0x00000010);
        applicationState_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object diagnostics_ = "N/A";
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnostics_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000020);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public Builder setDiagnosticsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      private long finishTime_ ;
      /**
       * <code>optional int64 finish_time = 7;</code>
       */
      public boolean hasFinishTime() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int64 finish_time = 7;</code>
       */
      public long getFinishTime() {
        return finishTime_;
      }
      /**
       * <code>optional int64 finish_time = 7;</code>
       */
      public Builder setFinishTime(long value) {
        bitField0_ |= 0x00000040;
        finishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_time = 7;</code>
       */
      public Builder clearFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000040);
        finishTime_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto callerContext_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.Builder, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProtoOrBuilder> callerContextBuilder_;
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public boolean hasCallerContext() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto getCallerContext() {
        if (callerContextBuilder_ == null) {
          return callerContext_ == null ? org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.getDefaultInstance() : callerContext_;
        } else {
          return callerContextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public Builder setCallerContext(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto value) {
        if (callerContextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          callerContext_ = value;
          onChanged();
        } else {
          callerContextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public Builder setCallerContext(
          org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.Builder builderForValue) {
        if (callerContextBuilder_ == null) {
          callerContext_ = builderForValue.build();
          onChanged();
        } else {
          callerContextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public Builder mergeCallerContext(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto value) {
        if (callerContextBuilder_ == null) {
          if (((bitField0_ & 0x00000080) != 0) &&
              callerContext_ != null &&
              callerContext_ != org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.getDefaultInstance()) {
            callerContext_ =
              org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.newBuilder(callerContext_).mergeFrom(value).buildPartial();
          } else {
            callerContext_ = value;
          }
          onChanged();
        } else {
          callerContextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public Builder clearCallerContext() {
        if (callerContextBuilder_ == null) {
          callerContext_ = null;
          onChanged();
        } else {
          callerContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.Builder getCallerContextBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getCallerContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProtoOrBuilder getCallerContextOrBuilder() {
        if (callerContextBuilder_ != null) {
          return callerContextBuilder_.getMessageOrBuilder();
        } else {
          return callerContext_ == null ?
              org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.getDefaultInstance() : callerContext_;
        }
      }
      /**
       * <code>optional .hadoop.common.RPCCallerContextProto caller_context = 8;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.Builder, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProtoOrBuilder> 
          getCallerContextFieldBuilder() {
        if (callerContextBuilder_ == null) {
          callerContextBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProto.Builder, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.RPCCallerContextProtoOrBuilder>(
                  getCallerContext(),
                  getParentForChildren(),
                  isClean());
          callerContext_ = null;
        }
        return callerContextBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto> applicationTimeouts_ =
        java.util.Collections.emptyList();
      private void ensureApplicationTimeoutsIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          applicationTimeouts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto>(applicationTimeouts_);
          bitField0_ |= 0x00000100;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder> applicationTimeoutsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto> getApplicationTimeoutsList() {
        if (applicationTimeoutsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applicationTimeouts_);
        } else {
          return applicationTimeoutsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public int getApplicationTimeoutsCount() {
        if (applicationTimeoutsBuilder_ == null) {
          return applicationTimeouts_.size();
        } else {
          return applicationTimeoutsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto getApplicationTimeouts(int index) {
        if (applicationTimeoutsBuilder_ == null) {
          return applicationTimeouts_.get(index);
        } else {
          return applicationTimeoutsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder setApplicationTimeouts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto value) {
        if (applicationTimeoutsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.set(index, value);
          onChanged();
        } else {
          applicationTimeoutsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder setApplicationTimeouts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder builderForValue) {
        if (applicationTimeoutsBuilder_ == null) {
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationTimeoutsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder addApplicationTimeouts(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto value) {
        if (applicationTimeoutsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.add(value);
          onChanged();
        } else {
          applicationTimeoutsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder addApplicationTimeouts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto value) {
        if (applicationTimeoutsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.add(index, value);
          onChanged();
        } else {
          applicationTimeoutsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder addApplicationTimeouts(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder builderForValue) {
        if (applicationTimeoutsBuilder_ == null) {
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.add(builderForValue.build());
          onChanged();
        } else {
          applicationTimeoutsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder addApplicationTimeouts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder builderForValue) {
        if (applicationTimeoutsBuilder_ == null) {
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationTimeoutsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder addAllApplicationTimeouts(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto> values) {
        if (applicationTimeoutsBuilder_ == null) {
          ensureApplicationTimeoutsIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, applicationTimeouts_);
          onChanged();
        } else {
          applicationTimeoutsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder clearApplicationTimeouts() {
        if (applicationTimeoutsBuilder_ == null) {
          applicationTimeouts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          applicationTimeoutsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public Builder removeApplicationTimeouts(int index) {
        if (applicationTimeoutsBuilder_ == null) {
          ensureApplicationTimeoutsIsMutable();
          applicationTimeouts_.remove(index);
          onChanged();
        } else {
          applicationTimeoutsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder getApplicationTimeoutsBuilder(
          int index) {
        return getApplicationTimeoutsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder getApplicationTimeoutsOrBuilder(
          int index) {
        if (applicationTimeoutsBuilder_ == null) {
          return applicationTimeouts_.get(index);  } else {
          return applicationTimeoutsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder> 
           getApplicationTimeoutsOrBuilderList() {
        if (applicationTimeoutsBuilder_ != null) {
          return applicationTimeoutsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applicationTimeouts_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder addApplicationTimeoutsBuilder() {
        return getApplicationTimeoutsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder addApplicationTimeoutsBuilder(
          int index) {
        return getApplicationTimeoutsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationTimeoutMapProto application_timeouts = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder> 
           getApplicationTimeoutsBuilderList() {
        return getApplicationTimeoutsFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder> 
          getApplicationTimeoutsFieldBuilder() {
        if (applicationTimeoutsBuilder_ == null) {
          applicationTimeoutsBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationTimeoutMapProtoOrBuilder>(
                  applicationTimeouts_,
                  ((bitField0_ & 0x00000100) != 0),
                  getParentForChildren(),
                  isClean());
          applicationTimeouts_ = null;
        }
        return applicationTimeoutsBuilder_;
      }

      private long launchTime_ ;
      /**
       * <code>optional int64 launch_time = 10;</code>
       */
      public boolean hasLaunchTime() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <code>optional int64 launch_time = 10;</code>
       */
      public long getLaunchTime() {
        return launchTime_;
      }
      /**
       * <code>optional int64 launch_time = 10;</code>
       */
      public Builder setLaunchTime(long value) {
        bitField0_ |= 0x00000200;
        launchTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 launch_time = 10;</code>
       */
      public Builder clearLaunchTime() {
        bitField0_ = (bitField0_ & ~0x00000200);
        launchTime_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object realUser_ = "";
      /**
       * <code>optional string real_user = 11;</code>
       */
      public boolean hasRealUser() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional string real_user = 11;</code>
       */
      public java.lang.String getRealUser() {
        java.lang.Object ref = realUser_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            realUser_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string real_user = 11;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getRealUserBytes() {
        java.lang.Object ref = realUser_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          realUser_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string real_user = 11;</code>
       */
      public Builder setRealUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        realUser_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string real_user = 11;</code>
       */
      public Builder clearRealUser() {
        bitField0_ = (bitField0_ & ~0x00000400);
        realUser_ = getDefaultInstance().getRealUser();
        onChanged();
        return this;
      }
      /**
       * <code>optional string real_user = 11;</code>
       */
      public Builder setRealUserBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        realUser_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ApplicationStateDataProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ApplicationStateDataProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<ApplicationStateDataProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<ApplicationStateDataProto>() {
      @java.lang.Override
      public ApplicationStateDataProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new ApplicationStateDataProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<ApplicationStateDataProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<ApplicationStateDataProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationStateDataProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ApplicationAttemptStateDataProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.ApplicationAttemptStateDataProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
     */
    boolean hasAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getAttemptIdOrBuilder();

    /**
     * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
     */
    boolean hasMasterContainer();
    /**
     * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getMasterContainer();
    /**
     * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getMasterContainerOrBuilder();

    /**
     * <code>optional bytes app_attempt_tokens = 3;</code>
     */
    boolean hasAppAttemptTokens();
    /**
     * <code>optional bytes app_attempt_tokens = 3;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString getAppAttemptTokens();

    /**
     * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
     */
    boolean hasAppAttemptState();
    /**
     * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto getAppAttemptState();

    /**
     * <code>optional string final_tracking_url = 5;</code>
     */
    boolean hasFinalTrackingUrl();
    /**
     * <code>optional string final_tracking_url = 5;</code>
     */
    java.lang.String getFinalTrackingUrl();
    /**
     * <code>optional string final_tracking_url = 5;</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getFinalTrackingUrlBytes();

    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes();

    /**
     * <code>optional int64 start_time = 7;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 7;</code>
     */
    long getStartTime();

    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
     */
    boolean hasFinalApplicationStatus();
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus();

    /**
     * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
     */
    boolean hasAmContainerExitStatus();
    /**
     * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
     */
    int getAmContainerExitStatus();

    /**
     * <code>optional int64 memory_seconds = 10;</code>
     */
    boolean hasMemorySeconds();
    /**
     * <code>optional int64 memory_seconds = 10;</code>
     */
    long getMemorySeconds();

    /**
     * <code>optional int64 vcore_seconds = 11;</code>
     */
    boolean hasVcoreSeconds();
    /**
     * <code>optional int64 vcore_seconds = 11;</code>
     */
    long getVcoreSeconds();

    /**
     * <code>optional int64 finish_time = 12;</code>
     */
    boolean hasFinishTime();
    /**
     * <code>optional int64 finish_time = 12;</code>
     */
    long getFinishTime();

    /**
     * <code>optional int64 preempted_memory_seconds = 13;</code>
     */
    boolean hasPreemptedMemorySeconds();
    /**
     * <code>optional int64 preempted_memory_seconds = 13;</code>
     */
    long getPreemptedMemorySeconds();

    /**
     * <code>optional int64 preempted_vcore_seconds = 14;</code>
     */
    boolean hasPreemptedVcoreSeconds();
    /**
     * <code>optional int64 preempted_vcore_seconds = 14;</code>
     */
    long getPreemptedVcoreSeconds();

    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> 
        getApplicationResourceUsageMapList();
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto getApplicationResourceUsageMap(int index);
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    int getApplicationResourceUsageMapCount();
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
        getApplicationResourceUsageMapOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder getApplicationResourceUsageMapOrBuilder(
        int index);

    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> 
        getPreemptedResourceUsageMapList();
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto getPreemptedResourceUsageMap(int index);
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    int getPreemptedResourceUsageMapCount();
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
        getPreemptedResourceUsageMapOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder getPreemptedResourceUsageMapOrBuilder(
        int index);

    /**
     * <code>optional int32 total_allocated_containers = 17;</code>
     */
    boolean hasTotalAllocatedContainers();
    /**
     * <code>optional int32 total_allocated_containers = 17;</code>
     */
    int getTotalAllocatedContainers();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ApplicationAttemptStateDataProto}
   */
  public  static final class ApplicationAttemptStateDataProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.ApplicationAttemptStateDataProto)
      ApplicationAttemptStateDataProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ApplicationAttemptStateDataProto.newBuilder() to construct.
    private ApplicationAttemptStateDataProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ApplicationAttemptStateDataProto() {
      appAttemptTokens_ = org.apache.hadoop.thirdparty.protobuf.ByteString.EMPTY;
      appAttemptState_ = 1;
      finalTrackingUrl_ = "";
      diagnostics_ = "N/A";
      finalApplicationStatus_ = 0;
      amContainerExitStatus_ = -1000;
      applicationResourceUsageMap_ = java.util.Collections.emptyList();
      preemptedResourceUsageMap_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ApplicationAttemptStateDataProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = attemptId_.toBuilder();
              }
              attemptId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(attemptId_);
                attemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = masterContainer_.toBuilder();
              }
              masterContainer_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(masterContainer_);
                masterContainer_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              appAttemptTokens_ = input.readBytes();
              break;
            }
            case 32: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto value = org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(4, rawValue);
              } else {
                bitField0_ |= 0x00000008;
                appAttemptState_ = rawValue;
              }
              break;
            }
            case 42: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000010;
              finalTrackingUrl_ = bs;
              break;
            }
            case 50: {
              org.apache.hadoop.thirdparty.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000020;
              diagnostics_ = bs;
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              startTime_ = input.readInt64();
              break;
            }
            case 64: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto value = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(8, rawValue);
              } else {
                bitField0_ |= 0x00000080;
                finalApplicationStatus_ = rawValue;
              }
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              amContainerExitStatus_ = input.readInt32();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000200;
              memorySeconds_ = input.readInt64();
              break;
            }
            case 88: {
              bitField0_ |= 0x00000400;
              vcoreSeconds_ = input.readInt64();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000800;
              finishTime_ = input.readInt64();
              break;
            }
            case 104: {
              bitField0_ |= 0x00001000;
              preemptedMemorySeconds_ = input.readInt64();
              break;
            }
            case 112: {
              bitField0_ |= 0x00002000;
              preemptedVcoreSeconds_ = input.readInt64();
              break;
            }
            case 122: {
              if (!((mutable_bitField0_ & 0x00004000) != 0)) {
                applicationResourceUsageMap_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto>();
                mutable_bitField0_ |= 0x00004000;
              }
              applicationResourceUsageMap_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.PARSER, extensionRegistry));
              break;
            }
            case 130: {
              if (!((mutable_bitField0_ & 0x00008000) != 0)) {
                preemptedResourceUsageMap_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto>();
                mutable_bitField0_ |= 0x00008000;
              }
              preemptedResourceUsageMap_.add(
                  input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.PARSER, extensionRegistry));
              break;
            }
            case 136: {
              bitField0_ |= 0x00004000;
              totalAllocatedContainers_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00004000) != 0)) {
          applicationResourceUsageMap_ = java.util.Collections.unmodifiableList(applicationResourceUsageMap_);
        }
        if (((mutable_bitField0_ & 0x00008000) != 0)) {
          preemptedResourceUsageMap_ = java.util.Collections.unmodifiableList(preemptedResourceUsageMap_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.Builder.class);
    }

    private int bitField0_;
    public static final int ATTEMPTID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto attemptId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
     */
    public boolean hasAttemptId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getAttemptId() {
      return attemptId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : attemptId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getAttemptIdOrBuilder() {
      return attemptId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : attemptId_;
    }

    public static final int MASTER_CONTAINER_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto masterContainer_;
    /**
     * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
     */
    public boolean hasMasterContainer() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getMasterContainer() {
      return masterContainer_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance() : masterContainer_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getMasterContainerOrBuilder() {
      return masterContainer_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance() : masterContainer_;
    }

    public static final int APP_ATTEMPT_TOKENS_FIELD_NUMBER = 3;
    private org.apache.hadoop.thirdparty.protobuf.ByteString appAttemptTokens_;
    /**
     * <code>optional bytes app_attempt_tokens = 3;</code>
     */
    public boolean hasAppAttemptTokens() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional bytes app_attempt_tokens = 3;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString getAppAttemptTokens() {
      return appAttemptTokens_;
    }

    public static final int APP_ATTEMPT_STATE_FIELD_NUMBER = 4;
    private int appAttemptState_;
    /**
     * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
     */
    public boolean hasAppAttemptState() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto getAppAttemptState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto result = org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto.valueOf(appAttemptState_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto.RMATTEMPT_NEW : result;
    }

    public static final int FINAL_TRACKING_URL_FIELD_NUMBER = 5;
    private volatile java.lang.Object finalTrackingUrl_;
    /**
     * <code>optional string final_tracking_url = 5;</code>
     */
    public boolean hasFinalTrackingUrl() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional string final_tracking_url = 5;</code>
     */
    public java.lang.String getFinalTrackingUrl() {
      java.lang.Object ref = finalTrackingUrl_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          finalTrackingUrl_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string final_tracking_url = 5;</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getFinalTrackingUrlBytes() {
      java.lang.Object ref = finalTrackingUrl_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        finalTrackingUrl_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int DIAGNOSTICS_FIELD_NUMBER = 6;
    private volatile java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        org.apache.hadoop.thirdparty.protobuf.ByteString bs = 
            (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 6 [default = "N/A"];</code>
     */
    public org.apache.hadoop.thirdparty.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        org.apache.hadoop.thirdparty.protobuf.ByteString b = 
            org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
      }
    }

    public static final int START_TIME_FIELD_NUMBER = 7;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 7;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional int64 start_time = 7;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    public static final int FINAL_APPLICATION_STATUS_FIELD_NUMBER = 8;
    private int finalApplicationStatus_;
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
     */
    public boolean hasFinalApplicationStatus() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto result = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.valueOf(finalApplicationStatus_);
      return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED : result;
    }

    public static final int AM_CONTAINER_EXIT_STATUS_FIELD_NUMBER = 9;
    private int amContainerExitStatus_;
    /**
     * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
     */
    public boolean hasAmContainerExitStatus() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
     */
    public int getAmContainerExitStatus() {
      return amContainerExitStatus_;
    }

    public static final int MEMORY_SECONDS_FIELD_NUMBER = 10;
    private long memorySeconds_;
    /**
     * <code>optional int64 memory_seconds = 10;</code>
     */
    public boolean hasMemorySeconds() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <code>optional int64 memory_seconds = 10;</code>
     */
    public long getMemorySeconds() {
      return memorySeconds_;
    }

    public static final int VCORE_SECONDS_FIELD_NUMBER = 11;
    private long vcoreSeconds_;
    /**
     * <code>optional int64 vcore_seconds = 11;</code>
     */
    public boolean hasVcoreSeconds() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional int64 vcore_seconds = 11;</code>
     */
    public long getVcoreSeconds() {
      return vcoreSeconds_;
    }

    public static final int FINISH_TIME_FIELD_NUMBER = 12;
    private long finishTime_;
    /**
     * <code>optional int64 finish_time = 12;</code>
     */
    public boolean hasFinishTime() {
      return ((bitField0_ & 0x00000800) != 0);
    }
    /**
     * <code>optional int64 finish_time = 12;</code>
     */
    public long getFinishTime() {
      return finishTime_;
    }

    public static final int PREEMPTED_MEMORY_SECONDS_FIELD_NUMBER = 13;
    private long preemptedMemorySeconds_;
    /**
     * <code>optional int64 preempted_memory_seconds = 13;</code>
     */
    public boolean hasPreemptedMemorySeconds() {
      return ((bitField0_ & 0x00001000) != 0);
    }
    /**
     * <code>optional int64 preempted_memory_seconds = 13;</code>
     */
    public long getPreemptedMemorySeconds() {
      return preemptedMemorySeconds_;
    }

    public static final int PREEMPTED_VCORE_SECONDS_FIELD_NUMBER = 14;
    private long preemptedVcoreSeconds_;
    /**
     * <code>optional int64 preempted_vcore_seconds = 14;</code>
     */
    public boolean hasPreemptedVcoreSeconds() {
      return ((bitField0_ & 0x00002000) != 0);
    }
    /**
     * <code>optional int64 preempted_vcore_seconds = 14;</code>
     */
    public long getPreemptedVcoreSeconds() {
      return preemptedVcoreSeconds_;
    }

    public static final int APPLICATION_RESOURCE_USAGE_MAP_FIELD_NUMBER = 15;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> applicationResourceUsageMap_;
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> getApplicationResourceUsageMapList() {
      return applicationResourceUsageMap_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
        getApplicationResourceUsageMapOrBuilderList() {
      return applicationResourceUsageMap_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    public int getApplicationResourceUsageMapCount() {
      return applicationResourceUsageMap_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto getApplicationResourceUsageMap(int index) {
      return applicationResourceUsageMap_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder getApplicationResourceUsageMapOrBuilder(
        int index) {
      return applicationResourceUsageMap_.get(index);
    }

    public static final int PREEMPTED_RESOURCE_USAGE_MAP_FIELD_NUMBER = 16;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> preemptedResourceUsageMap_;
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> getPreemptedResourceUsageMapList() {
      return preemptedResourceUsageMap_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
        getPreemptedResourceUsageMapOrBuilderList() {
      return preemptedResourceUsageMap_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    public int getPreemptedResourceUsageMapCount() {
      return preemptedResourceUsageMap_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto getPreemptedResourceUsageMap(int index) {
      return preemptedResourceUsageMap_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder getPreemptedResourceUsageMapOrBuilder(
        int index) {
      return preemptedResourceUsageMap_.get(index);
    }

    public static final int TOTAL_ALLOCATED_CONTAINERS_FIELD_NUMBER = 17;
    private int totalAllocatedContainers_;
    /**
     * <code>optional int32 total_allocated_containers = 17;</code>
     */
    public boolean hasTotalAllocatedContainers() {
      return ((bitField0_ & 0x00004000) != 0);
    }
    /**
     * <code>optional int32 total_allocated_containers = 17;</code>
     */
    public int getTotalAllocatedContainers() {
      return totalAllocatedContainers_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (hasMasterContainer()) {
        if (!getMasterContainer().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getApplicationResourceUsageMapCount(); i++) {
        if (!getApplicationResourceUsageMap(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getPreemptedResourceUsageMapCount(); i++) {
        if (!getPreemptedResourceUsageMap(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getMasterContainer());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeBytes(3, appAttemptTokens_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeEnum(4, appAttemptState_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 5, finalTrackingUrl_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.writeString(output, 6, diagnostics_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeInt64(7, startTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeEnum(8, finalApplicationStatus_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeInt32(9, amContainerExitStatus_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeInt64(10, memorySeconds_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeInt64(11, vcoreSeconds_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        output.writeInt64(12, finishTime_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        output.writeInt64(13, preemptedMemorySeconds_);
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        output.writeInt64(14, preemptedVcoreSeconds_);
      }
      for (int i = 0; i < applicationResourceUsageMap_.size(); i++) {
        output.writeMessage(15, applicationResourceUsageMap_.get(i));
      }
      for (int i = 0; i < preemptedResourceUsageMap_.size(); i++) {
        output.writeMessage(16, preemptedResourceUsageMap_.get(i));
      }
      if (((bitField0_ & 0x00004000) != 0)) {
        output.writeInt32(17, totalAllocatedContainers_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getAttemptId());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getMasterContainer());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeBytesSize(3, appAttemptTokens_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(4, appAttemptState_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(5, finalTrackingUrl_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.computeStringSize(6, diagnostics_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(7, startTime_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeEnumSize(8, finalApplicationStatus_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(9, amContainerExitStatus_);
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(10, memorySeconds_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(11, vcoreSeconds_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(12, finishTime_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(13, preemptedMemorySeconds_);
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(14, preemptedVcoreSeconds_);
      }
      for (int i = 0; i < applicationResourceUsageMap_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(15, applicationResourceUsageMap_.get(i));
      }
      for (int i = 0; i < preemptedResourceUsageMap_.size(); i++) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(16, preemptedResourceUsageMap_.get(i));
      }
      if (((bitField0_ & 0x00004000) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt32Size(17, totalAllocatedContainers_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto other = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto) obj;

      if (hasAttemptId() != other.hasAttemptId()) return false;
      if (hasAttemptId()) {
        if (!getAttemptId()
            .equals(other.getAttemptId())) return false;
      }
      if (hasMasterContainer() != other.hasMasterContainer()) return false;
      if (hasMasterContainer()) {
        if (!getMasterContainer()
            .equals(other.getMasterContainer())) return false;
      }
      if (hasAppAttemptTokens() != other.hasAppAttemptTokens()) return false;
      if (hasAppAttemptTokens()) {
        if (!getAppAttemptTokens()
            .equals(other.getAppAttemptTokens())) return false;
      }
      if (hasAppAttemptState() != other.hasAppAttemptState()) return false;
      if (hasAppAttemptState()) {
        if (appAttemptState_ != other.appAttemptState_) return false;
      }
      if (hasFinalTrackingUrl() != other.hasFinalTrackingUrl()) return false;
      if (hasFinalTrackingUrl()) {
        if (!getFinalTrackingUrl()
            .equals(other.getFinalTrackingUrl())) return false;
      }
      if (hasDiagnostics() != other.hasDiagnostics()) return false;
      if (hasDiagnostics()) {
        if (!getDiagnostics()
            .equals(other.getDiagnostics())) return false;
      }
      if (hasStartTime() != other.hasStartTime()) return false;
      if (hasStartTime()) {
        if (getStartTime()
            != other.getStartTime()) return false;
      }
      if (hasFinalApplicationStatus() != other.hasFinalApplicationStatus()) return false;
      if (hasFinalApplicationStatus()) {
        if (finalApplicationStatus_ != other.finalApplicationStatus_) return false;
      }
      if (hasAmContainerExitStatus() != other.hasAmContainerExitStatus()) return false;
      if (hasAmContainerExitStatus()) {
        if (getAmContainerExitStatus()
            != other.getAmContainerExitStatus()) return false;
      }
      if (hasMemorySeconds() != other.hasMemorySeconds()) return false;
      if (hasMemorySeconds()) {
        if (getMemorySeconds()
            != other.getMemorySeconds()) return false;
      }
      if (hasVcoreSeconds() != other.hasVcoreSeconds()) return false;
      if (hasVcoreSeconds()) {
        if (getVcoreSeconds()
            != other.getVcoreSeconds()) return false;
      }
      if (hasFinishTime() != other.hasFinishTime()) return false;
      if (hasFinishTime()) {
        if (getFinishTime()
            != other.getFinishTime()) return false;
      }
      if (hasPreemptedMemorySeconds() != other.hasPreemptedMemorySeconds()) return false;
      if (hasPreemptedMemorySeconds()) {
        if (getPreemptedMemorySeconds()
            != other.getPreemptedMemorySeconds()) return false;
      }
      if (hasPreemptedVcoreSeconds() != other.hasPreemptedVcoreSeconds()) return false;
      if (hasPreemptedVcoreSeconds()) {
        if (getPreemptedVcoreSeconds()
            != other.getPreemptedVcoreSeconds()) return false;
      }
      if (!getApplicationResourceUsageMapList()
          .equals(other.getApplicationResourceUsageMapList())) return false;
      if (!getPreemptedResourceUsageMapList()
          .equals(other.getPreemptedResourceUsageMapList())) return false;
      if (hasTotalAllocatedContainers() != other.hasTotalAllocatedContainers()) return false;
      if (hasTotalAllocatedContainers()) {
        if (getTotalAllocatedContainers()
            != other.getTotalAllocatedContainers()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAttemptId()) {
        hash = (37 * hash) + ATTEMPTID_FIELD_NUMBER;
        hash = (53 * hash) + getAttemptId().hashCode();
      }
      if (hasMasterContainer()) {
        hash = (37 * hash) + MASTER_CONTAINER_FIELD_NUMBER;
        hash = (53 * hash) + getMasterContainer().hashCode();
      }
      if (hasAppAttemptTokens()) {
        hash = (37 * hash) + APP_ATTEMPT_TOKENS_FIELD_NUMBER;
        hash = (53 * hash) + getAppAttemptTokens().hashCode();
      }
      if (hasAppAttemptState()) {
        hash = (37 * hash) + APP_ATTEMPT_STATE_FIELD_NUMBER;
        hash = (53 * hash) + appAttemptState_;
      }
      if (hasFinalTrackingUrl()) {
        hash = (37 * hash) + FINAL_TRACKING_URL_FIELD_NUMBER;
        hash = (53 * hash) + getFinalTrackingUrl().hashCode();
      }
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getStartTime());
      }
      if (hasFinalApplicationStatus()) {
        hash = (37 * hash) + FINAL_APPLICATION_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + finalApplicationStatus_;
      }
      if (hasAmContainerExitStatus()) {
        hash = (37 * hash) + AM_CONTAINER_EXIT_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getAmContainerExitStatus();
      }
      if (hasMemorySeconds()) {
        hash = (37 * hash) + MEMORY_SECONDS_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getMemorySeconds());
      }
      if (hasVcoreSeconds()) {
        hash = (37 * hash) + VCORE_SECONDS_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getVcoreSeconds());
      }
      if (hasFinishTime()) {
        hash = (37 * hash) + FINISH_TIME_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getFinishTime());
      }
      if (hasPreemptedMemorySeconds()) {
        hash = (37 * hash) + PREEMPTED_MEMORY_SECONDS_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getPreemptedMemorySeconds());
      }
      if (hasPreemptedVcoreSeconds()) {
        hash = (37 * hash) + PREEMPTED_VCORE_SECONDS_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getPreemptedVcoreSeconds());
      }
      if (getApplicationResourceUsageMapCount() > 0) {
        hash = (37 * hash) + APPLICATION_RESOURCE_USAGE_MAP_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationResourceUsageMapList().hashCode();
      }
      if (getPreemptedResourceUsageMapCount() > 0) {
        hash = (37 * hash) + PREEMPTED_RESOURCE_USAGE_MAP_FIELD_NUMBER;
        hash = (53 * hash) + getPreemptedResourceUsageMapList().hashCode();
      }
      if (hasTotalAllocatedContainers()) {
        hash = (37 * hash) + TOTAL_ALLOCATED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getTotalAllocatedContainers();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ApplicationAttemptStateDataProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.ApplicationAttemptStateDataProto)
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getAttemptIdFieldBuilder();
          getMasterContainerFieldBuilder();
          getApplicationResourceUsageMapFieldBuilder();
          getPreemptedResourceUsageMapFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (attemptIdBuilder_ == null) {
          attemptId_ = null;
        } else {
          attemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (masterContainerBuilder_ == null) {
          masterContainer_ = null;
        } else {
          masterContainerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        appAttemptTokens_ = org.apache.hadoop.thirdparty.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        appAttemptState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000008);
        finalTrackingUrl_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        diagnostics_ = "N/A";
        bitField0_ = (bitField0_ & ~0x00000020);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        finalApplicationStatus_ = 0;
        bitField0_ = (bitField0_ & ~0x00000080);
        amContainerExitStatus_ = -1000;
        bitField0_ = (bitField0_ & ~0x00000100);
        memorySeconds_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        vcoreSeconds_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000400);
        finishTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000800);
        preemptedMemorySeconds_ = 0L;
        bitField0_ = (bitField0_ & ~0x00001000);
        preemptedVcoreSeconds_ = 0L;
        bitField0_ = (bitField0_ & ~0x00002000);
        if (applicationResourceUsageMapBuilder_ == null) {
          applicationResourceUsageMap_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00004000);
        } else {
          applicationResourceUsageMapBuilder_.clear();
        }
        if (preemptedResourceUsageMapBuilder_ == null) {
          preemptedResourceUsageMap_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00008000);
        } else {
          preemptedResourceUsageMapBuilder_.clear();
        }
        totalAllocatedContainers_ = 0;
        bitField0_ = (bitField0_ & ~0x00010000);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto build() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto result = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (attemptIdBuilder_ == null) {
            result.attemptId_ = attemptId_;
          } else {
            result.attemptId_ = attemptIdBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (masterContainerBuilder_ == null) {
            result.masterContainer_ = masterContainer_;
          } else {
            result.masterContainer_ = masterContainerBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.appAttemptTokens_ = appAttemptTokens_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.appAttemptState_ = appAttemptState_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.finalTrackingUrl_ = finalTrackingUrl_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.diagnostics_ = diagnostics_;
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.startTime_ = startTime_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000080;
        }
        result.finalApplicationStatus_ = finalApplicationStatus_;
        if (((from_bitField0_ & 0x00000100) != 0)) {
          to_bitField0_ |= 0x00000100;
        }
        result.amContainerExitStatus_ = amContainerExitStatus_;
        if (((from_bitField0_ & 0x00000200) != 0)) {
          result.memorySeconds_ = memorySeconds_;
          to_bitField0_ |= 0x00000200;
        }
        if (((from_bitField0_ & 0x00000400) != 0)) {
          result.vcoreSeconds_ = vcoreSeconds_;
          to_bitField0_ |= 0x00000400;
        }
        if (((from_bitField0_ & 0x00000800) != 0)) {
          result.finishTime_ = finishTime_;
          to_bitField0_ |= 0x00000800;
        }
        if (((from_bitField0_ & 0x00001000) != 0)) {
          result.preemptedMemorySeconds_ = preemptedMemorySeconds_;
          to_bitField0_ |= 0x00001000;
        }
        if (((from_bitField0_ & 0x00002000) != 0)) {
          result.preemptedVcoreSeconds_ = preemptedVcoreSeconds_;
          to_bitField0_ |= 0x00002000;
        }
        if (applicationResourceUsageMapBuilder_ == null) {
          if (((bitField0_ & 0x00004000) != 0)) {
            applicationResourceUsageMap_ = java.util.Collections.unmodifiableList(applicationResourceUsageMap_);
            bitField0_ = (bitField0_ & ~0x00004000);
          }
          result.applicationResourceUsageMap_ = applicationResourceUsageMap_;
        } else {
          result.applicationResourceUsageMap_ = applicationResourceUsageMapBuilder_.build();
        }
        if (preemptedResourceUsageMapBuilder_ == null) {
          if (((bitField0_ & 0x00008000) != 0)) {
            preemptedResourceUsageMap_ = java.util.Collections.unmodifiableList(preemptedResourceUsageMap_);
            bitField0_ = (bitField0_ & ~0x00008000);
          }
          result.preemptedResourceUsageMap_ = preemptedResourceUsageMap_;
        } else {
          result.preemptedResourceUsageMap_ = preemptedResourceUsageMapBuilder_.build();
        }
        if (((from_bitField0_ & 0x00010000) != 0)) {
          result.totalAllocatedContainers_ = totalAllocatedContainers_;
          to_bitField0_ |= 0x00004000;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto.getDefaultInstance()) return this;
        if (other.hasAttemptId()) {
          mergeAttemptId(other.getAttemptId());
        }
        if (other.hasMasterContainer()) {
          mergeMasterContainer(other.getMasterContainer());
        }
        if (other.hasAppAttemptTokens()) {
          setAppAttemptTokens(other.getAppAttemptTokens());
        }
        if (other.hasAppAttemptState()) {
          setAppAttemptState(other.getAppAttemptState());
        }
        if (other.hasFinalTrackingUrl()) {
          bitField0_ |= 0x00000010;
          finalTrackingUrl_ = other.finalTrackingUrl_;
          onChanged();
        }
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000020;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasFinalApplicationStatus()) {
          setFinalApplicationStatus(other.getFinalApplicationStatus());
        }
        if (other.hasAmContainerExitStatus()) {
          setAmContainerExitStatus(other.getAmContainerExitStatus());
        }
        if (other.hasMemorySeconds()) {
          setMemorySeconds(other.getMemorySeconds());
        }
        if (other.hasVcoreSeconds()) {
          setVcoreSeconds(other.getVcoreSeconds());
        }
        if (other.hasFinishTime()) {
          setFinishTime(other.getFinishTime());
        }
        if (other.hasPreemptedMemorySeconds()) {
          setPreemptedMemorySeconds(other.getPreemptedMemorySeconds());
        }
        if (other.hasPreemptedVcoreSeconds()) {
          setPreemptedVcoreSeconds(other.getPreemptedVcoreSeconds());
        }
        if (applicationResourceUsageMapBuilder_ == null) {
          if (!other.applicationResourceUsageMap_.isEmpty()) {
            if (applicationResourceUsageMap_.isEmpty()) {
              applicationResourceUsageMap_ = other.applicationResourceUsageMap_;
              bitField0_ = (bitField0_ & ~0x00004000);
            } else {
              ensureApplicationResourceUsageMapIsMutable();
              applicationResourceUsageMap_.addAll(other.applicationResourceUsageMap_);
            }
            onChanged();
          }
        } else {
          if (!other.applicationResourceUsageMap_.isEmpty()) {
            if (applicationResourceUsageMapBuilder_.isEmpty()) {
              applicationResourceUsageMapBuilder_.dispose();
              applicationResourceUsageMapBuilder_ = null;
              applicationResourceUsageMap_ = other.applicationResourceUsageMap_;
              bitField0_ = (bitField0_ & ~0x00004000);
              applicationResourceUsageMapBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getApplicationResourceUsageMapFieldBuilder() : null;
            } else {
              applicationResourceUsageMapBuilder_.addAllMessages(other.applicationResourceUsageMap_);
            }
          }
        }
        if (preemptedResourceUsageMapBuilder_ == null) {
          if (!other.preemptedResourceUsageMap_.isEmpty()) {
            if (preemptedResourceUsageMap_.isEmpty()) {
              preemptedResourceUsageMap_ = other.preemptedResourceUsageMap_;
              bitField0_ = (bitField0_ & ~0x00008000);
            } else {
              ensurePreemptedResourceUsageMapIsMutable();
              preemptedResourceUsageMap_.addAll(other.preemptedResourceUsageMap_);
            }
            onChanged();
          }
        } else {
          if (!other.preemptedResourceUsageMap_.isEmpty()) {
            if (preemptedResourceUsageMapBuilder_.isEmpty()) {
              preemptedResourceUsageMapBuilder_.dispose();
              preemptedResourceUsageMapBuilder_ = null;
              preemptedResourceUsageMap_ = other.preemptedResourceUsageMap_;
              bitField0_ = (bitField0_ & ~0x00008000);
              preemptedResourceUsageMapBuilder_ = 
                org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getPreemptedResourceUsageMapFieldBuilder() : null;
            } else {
              preemptedResourceUsageMapBuilder_.addAllMessages(other.preemptedResourceUsageMap_);
            }
          }
        }
        if (other.hasTotalAllocatedContainers()) {
          setTotalAllocatedContainers(other.getTotalAllocatedContainers());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (hasMasterContainer()) {
          if (!getMasterContainer().isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getApplicationResourceUsageMapCount(); i++) {
          if (!getApplicationResourceUsageMap(i).isInitialized()) {
            return false;
          }
        }
        for (int i = 0; i < getPreemptedResourceUsageMapCount(); i++) {
          if (!getPreemptedResourceUsageMap(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto attemptId_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> attemptIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public boolean hasAttemptId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getAttemptId() {
        if (attemptIdBuilder_ == null) {
          return attemptId_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : attemptId_;
        } else {
          return attemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public Builder setAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (attemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          attemptId_ = value;
          onChanged();
        } else {
          attemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public Builder setAttemptId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder builderForValue) {
        if (attemptIdBuilder_ == null) {
          attemptId_ = builderForValue.build();
          onChanged();
        } else {
          attemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public Builder mergeAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (attemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              attemptId_ != null &&
              attemptId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance()) {
            attemptId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.newBuilder(attemptId_).mergeFrom(value).buildPartial();
          } else {
            attemptId_ = value;
          }
          onChanged();
        } else {
          attemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public Builder clearAttemptId() {
        if (attemptIdBuilder_ == null) {
          attemptId_ = null;
          onChanged();
        } else {
          attemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder getAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getAttemptIdOrBuilder() {
        if (attemptIdBuilder_ != null) {
          return attemptIdBuilder_.getMessageOrBuilder();
        } else {
          return attemptId_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance() : attemptId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto attemptId = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> 
          getAttemptIdFieldBuilder() {
        if (attemptIdBuilder_ == null) {
          attemptIdBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder>(
                  getAttemptId(),
                  getParentForChildren(),
                  isClean());
          attemptId_ = null;
        }
        return attemptIdBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto masterContainer_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> masterContainerBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public boolean hasMasterContainer() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getMasterContainer() {
        if (masterContainerBuilder_ == null) {
          return masterContainer_ == null ? org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance() : masterContainer_;
        } else {
          return masterContainerBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public Builder setMasterContainer(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (masterContainerBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          masterContainer_ = value;
          onChanged();
        } else {
          masterContainerBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public Builder setMasterContainer(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (masterContainerBuilder_ == null) {
          masterContainer_ = builderForValue.build();
          onChanged();
        } else {
          masterContainerBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public Builder mergeMasterContainer(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (masterContainerBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              masterContainer_ != null &&
              masterContainer_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance()) {
            masterContainer_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.newBuilder(masterContainer_).mergeFrom(value).buildPartial();
          } else {
            masterContainer_ = value;
          }
          onChanged();
        } else {
          masterContainerBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public Builder clearMasterContainer() {
        if (masterContainerBuilder_ == null) {
          masterContainer_ = null;
          onChanged();
        } else {
          masterContainerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getMasterContainerBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getMasterContainerFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getMasterContainerOrBuilder() {
        if (masterContainerBuilder_ != null) {
          return masterContainerBuilder_.getMessageOrBuilder();
        } else {
          return masterContainer_ == null ?
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance() : masterContainer_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerProto master_container = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getMasterContainerFieldBuilder() {
        if (masterContainerBuilder_ == null) {
          masterContainerBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  getMasterContainer(),
                  getParentForChildren(),
                  isClean());
          masterContainer_ = null;
        }
        return masterContainerBuilder_;
      }

      private org.apache.hadoop.thirdparty.protobuf.ByteString appAttemptTokens_ = org.apache.hadoop.thirdparty.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes app_attempt_tokens = 3;</code>
       */
      public boolean hasAppAttemptTokens() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional bytes app_attempt_tokens = 3;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString getAppAttemptTokens() {
        return appAttemptTokens_;
      }
      /**
       * <code>optional bytes app_attempt_tokens = 3;</code>
       */
      public Builder setAppAttemptTokens(org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        appAttemptTokens_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes app_attempt_tokens = 3;</code>
       */
      public Builder clearAppAttemptTokens() {
        bitField0_ = (bitField0_ & ~0x00000004);
        appAttemptTokens_ = getDefaultInstance().getAppAttemptTokens();
        onChanged();
        return this;
      }

      private int appAttemptState_ = 1;
      /**
       * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
       */
      public boolean hasAppAttemptState() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto getAppAttemptState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto result = org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto.valueOf(appAttemptState_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto.RMATTEMPT_NEW : result;
      }
      /**
       * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
       */
      public Builder setAppAttemptState(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMAppAttemptStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000008;
        appAttemptState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.RMAppAttemptStateProto app_attempt_state = 4;</code>
       */
      public Builder clearAppAttemptState() {
        bitField0_ = (bitField0_ & ~0x00000008);
        appAttemptState_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object finalTrackingUrl_ = "";
      /**
       * <code>optional string final_tracking_url = 5;</code>
       */
      public boolean hasFinalTrackingUrl() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional string final_tracking_url = 5;</code>
       */
      public java.lang.String getFinalTrackingUrl() {
        java.lang.Object ref = finalTrackingUrl_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            finalTrackingUrl_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string final_tracking_url = 5;</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getFinalTrackingUrlBytes() {
        java.lang.Object ref = finalTrackingUrl_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          finalTrackingUrl_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string final_tracking_url = 5;</code>
       */
      public Builder setFinalTrackingUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        finalTrackingUrl_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string final_tracking_url = 5;</code>
       */
      public Builder clearFinalTrackingUrl() {
        bitField0_ = (bitField0_ & ~0x00000010);
        finalTrackingUrl_ = getDefaultInstance().getFinalTrackingUrl();
        onChanged();
        return this;
      }
      /**
       * <code>optional string final_tracking_url = 5;</code>
       */
      public Builder setFinalTrackingUrlBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        finalTrackingUrl_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object diagnostics_ = "N/A";
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          org.apache.hadoop.thirdparty.protobuf.ByteString bs =
              (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            diagnostics_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public org.apache.hadoop.thirdparty.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          org.apache.hadoop.thirdparty.protobuf.ByteString b = 
              org.apache.hadoop.thirdparty.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (org.apache.hadoop.thirdparty.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000020);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 6 [default = "N/A"];</code>
       */
      public Builder setDiagnosticsBytes(
          org.apache.hadoop.thirdparty.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000040;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 7;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000040);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      private int finalApplicationStatus_ = 0;
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
       */
      public boolean hasFinalApplicationStatus() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto result = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.valueOf(finalApplicationStatus_);
        return result == null ? org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED : result;
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
       */
      public Builder setFinalApplicationStatus(org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000080;
        finalApplicationStatus_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 8;</code>
       */
      public Builder clearFinalApplicationStatus() {
        bitField0_ = (bitField0_ & ~0x00000080);
        finalApplicationStatus_ = 0;
        onChanged();
        return this;
      }

      private int amContainerExitStatus_ = -1000;
      /**
       * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
       */
      public boolean hasAmContainerExitStatus() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
       */
      public int getAmContainerExitStatus() {
        return amContainerExitStatus_;
      }
      /**
       * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
       */
      public Builder setAmContainerExitStatus(int value) {
        bitField0_ |= 0x00000100;
        amContainerExitStatus_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 am_container_exit_status = 9 [default = -1000];</code>
       */
      public Builder clearAmContainerExitStatus() {
        bitField0_ = (bitField0_ & ~0x00000100);
        amContainerExitStatus_ = -1000;
        onChanged();
        return this;
      }

      private long memorySeconds_ ;
      /**
       * <code>optional int64 memory_seconds = 10;</code>
       */
      public boolean hasMemorySeconds() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <code>optional int64 memory_seconds = 10;</code>
       */
      public long getMemorySeconds() {
        return memorySeconds_;
      }
      /**
       * <code>optional int64 memory_seconds = 10;</code>
       */
      public Builder setMemorySeconds(long value) {
        bitField0_ |= 0x00000200;
        memorySeconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 memory_seconds = 10;</code>
       */
      public Builder clearMemorySeconds() {
        bitField0_ = (bitField0_ & ~0x00000200);
        memorySeconds_ = 0L;
        onChanged();
        return this;
      }

      private long vcoreSeconds_ ;
      /**
       * <code>optional int64 vcore_seconds = 11;</code>
       */
      public boolean hasVcoreSeconds() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional int64 vcore_seconds = 11;</code>
       */
      public long getVcoreSeconds() {
        return vcoreSeconds_;
      }
      /**
       * <code>optional int64 vcore_seconds = 11;</code>
       */
      public Builder setVcoreSeconds(long value) {
        bitField0_ |= 0x00000400;
        vcoreSeconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 vcore_seconds = 11;</code>
       */
      public Builder clearVcoreSeconds() {
        bitField0_ = (bitField0_ & ~0x00000400);
        vcoreSeconds_ = 0L;
        onChanged();
        return this;
      }

      private long finishTime_ ;
      /**
       * <code>optional int64 finish_time = 12;</code>
       */
      public boolean hasFinishTime() {
        return ((bitField0_ & 0x00000800) != 0);
      }
      /**
       * <code>optional int64 finish_time = 12;</code>
       */
      public long getFinishTime() {
        return finishTime_;
      }
      /**
       * <code>optional int64 finish_time = 12;</code>
       */
      public Builder setFinishTime(long value) {
        bitField0_ |= 0x00000800;
        finishTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_time = 12;</code>
       */
      public Builder clearFinishTime() {
        bitField0_ = (bitField0_ & ~0x00000800);
        finishTime_ = 0L;
        onChanged();
        return this;
      }

      private long preemptedMemorySeconds_ ;
      /**
       * <code>optional int64 preempted_memory_seconds = 13;</code>
       */
      public boolean hasPreemptedMemorySeconds() {
        return ((bitField0_ & 0x00001000) != 0);
      }
      /**
       * <code>optional int64 preempted_memory_seconds = 13;</code>
       */
      public long getPreemptedMemorySeconds() {
        return preemptedMemorySeconds_;
      }
      /**
       * <code>optional int64 preempted_memory_seconds = 13;</code>
       */
      public Builder setPreemptedMemorySeconds(long value) {
        bitField0_ |= 0x00001000;
        preemptedMemorySeconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 preempted_memory_seconds = 13;</code>
       */
      public Builder clearPreemptedMemorySeconds() {
        bitField0_ = (bitField0_ & ~0x00001000);
        preemptedMemorySeconds_ = 0L;
        onChanged();
        return this;
      }

      private long preemptedVcoreSeconds_ ;
      /**
       * <code>optional int64 preempted_vcore_seconds = 14;</code>
       */
      public boolean hasPreemptedVcoreSeconds() {
        return ((bitField0_ & 0x00002000) != 0);
      }
      /**
       * <code>optional int64 preempted_vcore_seconds = 14;</code>
       */
      public long getPreemptedVcoreSeconds() {
        return preemptedVcoreSeconds_;
      }
      /**
       * <code>optional int64 preempted_vcore_seconds = 14;</code>
       */
      public Builder setPreemptedVcoreSeconds(long value) {
        bitField0_ |= 0x00002000;
        preemptedVcoreSeconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 preempted_vcore_seconds = 14;</code>
       */
      public Builder clearPreemptedVcoreSeconds() {
        bitField0_ = (bitField0_ & ~0x00002000);
        preemptedVcoreSeconds_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> applicationResourceUsageMap_ =
        java.util.Collections.emptyList();
      private void ensureApplicationResourceUsageMapIsMutable() {
        if (!((bitField0_ & 0x00004000) != 0)) {
          applicationResourceUsageMap_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto>(applicationResourceUsageMap_);
          bitField0_ |= 0x00004000;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> applicationResourceUsageMapBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> getApplicationResourceUsageMapList() {
        if (applicationResourceUsageMapBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applicationResourceUsageMap_);
        } else {
          return applicationResourceUsageMapBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public int getApplicationResourceUsageMapCount() {
        if (applicationResourceUsageMapBuilder_ == null) {
          return applicationResourceUsageMap_.size();
        } else {
          return applicationResourceUsageMapBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto getApplicationResourceUsageMap(int index) {
        if (applicationResourceUsageMapBuilder_ == null) {
          return applicationResourceUsageMap_.get(index);
        } else {
          return applicationResourceUsageMapBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder setApplicationResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto value) {
        if (applicationResourceUsageMapBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.set(index, value);
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder setApplicationResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder builderForValue) {
        if (applicationResourceUsageMapBuilder_ == null) {
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder addApplicationResourceUsageMap(org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto value) {
        if (applicationResourceUsageMapBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.add(value);
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder addApplicationResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto value) {
        if (applicationResourceUsageMapBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.add(index, value);
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder addApplicationResourceUsageMap(
          org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder builderForValue) {
        if (applicationResourceUsageMapBuilder_ == null) {
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.add(builderForValue.build());
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder addApplicationResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder builderForValue) {
        if (applicationResourceUsageMapBuilder_ == null) {
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder addAllApplicationResourceUsageMap(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> values) {
        if (applicationResourceUsageMapBuilder_ == null) {
          ensureApplicationResourceUsageMapIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, applicationResourceUsageMap_);
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder clearApplicationResourceUsageMap() {
        if (applicationResourceUsageMapBuilder_ == null) {
          applicationResourceUsageMap_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00004000);
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public Builder removeApplicationResourceUsageMap(int index) {
        if (applicationResourceUsageMapBuilder_ == null) {
          ensureApplicationResourceUsageMapIsMutable();
          applicationResourceUsageMap_.remove(index);
          onChanged();
        } else {
          applicationResourceUsageMapBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder getApplicationResourceUsageMapBuilder(
          int index) {
        return getApplicationResourceUsageMapFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder getApplicationResourceUsageMapOrBuilder(
          int index) {
        if (applicationResourceUsageMapBuilder_ == null) {
          return applicationResourceUsageMap_.get(index);  } else {
          return applicationResourceUsageMapBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
           getApplicationResourceUsageMapOrBuilderList() {
        if (applicationResourceUsageMapBuilder_ != null) {
          return applicationResourceUsageMapBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applicationResourceUsageMap_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder addApplicationResourceUsageMapBuilder() {
        return getApplicationResourceUsageMapFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder addApplicationResourceUsageMapBuilder(
          int index) {
        return getApplicationResourceUsageMapFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto application_resource_usage_map = 15;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder> 
           getApplicationResourceUsageMapBuilderList() {
        return getApplicationResourceUsageMapFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
          getApplicationResourceUsageMapFieldBuilder() {
        if (applicationResourceUsageMapBuilder_ == null) {
          applicationResourceUsageMapBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder>(
                  applicationResourceUsageMap_,
                  ((bitField0_ & 0x00004000) != 0),
                  getParentForChildren(),
                  isClean());
          applicationResourceUsageMap_ = null;
        }
        return applicationResourceUsageMapBuilder_;
      }

      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> preemptedResourceUsageMap_ =
        java.util.Collections.emptyList();
      private void ensurePreemptedResourceUsageMapIsMutable() {
        if (!((bitField0_ & 0x00008000) != 0)) {
          preemptedResourceUsageMap_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto>(preemptedResourceUsageMap_);
          bitField0_ |= 0x00008000;
         }
      }

      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> preemptedResourceUsageMapBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> getPreemptedResourceUsageMapList() {
        if (preemptedResourceUsageMapBuilder_ == null) {
          return java.util.Collections.unmodifiableList(preemptedResourceUsageMap_);
        } else {
          return preemptedResourceUsageMapBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public int getPreemptedResourceUsageMapCount() {
        if (preemptedResourceUsageMapBuilder_ == null) {
          return preemptedResourceUsageMap_.size();
        } else {
          return preemptedResourceUsageMapBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto getPreemptedResourceUsageMap(int index) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          return preemptedResourceUsageMap_.get(index);
        } else {
          return preemptedResourceUsageMapBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder setPreemptedResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto value) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.set(index, value);
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder setPreemptedResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder builderForValue) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.set(index, builderForValue.build());
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder addPreemptedResourceUsageMap(org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto value) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.add(value);
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder addPreemptedResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto value) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.add(index, value);
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder addPreemptedResourceUsageMap(
          org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder builderForValue) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.add(builderForValue.build());
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder addPreemptedResourceUsageMap(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder builderForValue) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.add(index, builderForValue.build());
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder addAllPreemptedResourceUsageMap(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto> values) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          ensurePreemptedResourceUsageMapIsMutable();
          org.apache.hadoop.thirdparty.protobuf.AbstractMessageLite.Builder.addAll(
              values, preemptedResourceUsageMap_);
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder clearPreemptedResourceUsageMap() {
        if (preemptedResourceUsageMapBuilder_ == null) {
          preemptedResourceUsageMap_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00008000);
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public Builder removePreemptedResourceUsageMap(int index) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          ensurePreemptedResourceUsageMapIsMutable();
          preemptedResourceUsageMap_.remove(index);
          onChanged();
        } else {
          preemptedResourceUsageMapBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder getPreemptedResourceUsageMapBuilder(
          int index) {
        return getPreemptedResourceUsageMapFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder getPreemptedResourceUsageMapOrBuilder(
          int index) {
        if (preemptedResourceUsageMapBuilder_ == null) {
          return preemptedResourceUsageMap_.get(index);  } else {
          return preemptedResourceUsageMapBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
           getPreemptedResourceUsageMapOrBuilderList() {
        if (preemptedResourceUsageMapBuilder_ != null) {
          return preemptedResourceUsageMapBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(preemptedResourceUsageMap_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder addPreemptedResourceUsageMapBuilder() {
        return getPreemptedResourceUsageMapFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder addPreemptedResourceUsageMapBuilder(
          int index) {
        return getPreemptedResourceUsageMapFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringLongMapProto preempted_resource_usage_map = 16;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder> 
           getPreemptedResourceUsageMapBuilderList() {
        return getPreemptedResourceUsageMapFieldBuilder().getBuilderList();
      }
      private org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder> 
          getPreemptedResourceUsageMapFieldBuilder() {
        if (preemptedResourceUsageMapBuilder_ == null) {
          preemptedResourceUsageMapBuilder_ = new org.apache.hadoop.thirdparty.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringLongMapProtoOrBuilder>(
                  preemptedResourceUsageMap_,
                  ((bitField0_ & 0x00008000) != 0),
                  getParentForChildren(),
                  isClean());
          preemptedResourceUsageMap_ = null;
        }
        return preemptedResourceUsageMapBuilder_;
      }

      private int totalAllocatedContainers_ ;
      /**
       * <code>optional int32 total_allocated_containers = 17;</code>
       */
      public boolean hasTotalAllocatedContainers() {
        return ((bitField0_ & 0x00010000) != 0);
      }
      /**
       * <code>optional int32 total_allocated_containers = 17;</code>
       */
      public int getTotalAllocatedContainers() {
        return totalAllocatedContainers_;
      }
      /**
       * <code>optional int32 total_allocated_containers = 17;</code>
       */
      public Builder setTotalAllocatedContainers(int value) {
        bitField0_ |= 0x00010000;
        totalAllocatedContainers_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 total_allocated_containers = 17;</code>
       */
      public Builder clearTotalAllocatedContainers() {
        bitField0_ = (bitField0_ & ~0x00010000);
        totalAllocatedContainers_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ApplicationAttemptStateDataProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ApplicationAttemptStateDataProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<ApplicationAttemptStateDataProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<ApplicationAttemptStateDataProto>() {
      @java.lang.Override
      public ApplicationAttemptStateDataProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new ApplicationAttemptStateDataProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<ApplicationAttemptStateDataProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<ApplicationAttemptStateDataProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.ApplicationAttemptStateDataProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface EpochProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.EpochProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional int64 epoch = 1;</code>
     */
    boolean hasEpoch();
    /**
     * <code>optional int64 epoch = 1;</code>
     */
    long getEpoch();
  }
  /**
   * Protobuf type {@code hadoop.yarn.EpochProto}
   */
  public  static final class EpochProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.EpochProto)
      EpochProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use EpochProto.newBuilder() to construct.
    private EpochProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private EpochProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private EpochProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              epoch_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_EpochProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_EpochProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.Builder.class);
    }

    private int bitField0_;
    public static final int EPOCH_FIELD_NUMBER = 1;
    private long epoch_;
    /**
     * <code>optional int64 epoch = 1;</code>
     */
    public boolean hasEpoch() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int64 epoch = 1;</code>
     */
    public long getEpoch() {
      return epoch_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(1, epoch_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(1, epoch_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto other = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto) obj;

      if (hasEpoch() != other.hasEpoch()) return false;
      if (hasEpoch()) {
        if (getEpoch()
            != other.getEpoch()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasEpoch()) {
        hash = (37 * hash) + EPOCH_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getEpoch());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.EpochProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.EpochProto)
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_EpochProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_EpochProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        epoch_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_EpochProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto build() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto result = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.epoch_ = epoch_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto.getDefaultInstance()) return this;
        if (other.hasEpoch()) {
          setEpoch(other.getEpoch());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long epoch_ ;
      /**
       * <code>optional int64 epoch = 1;</code>
       */
      public boolean hasEpoch() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int64 epoch = 1;</code>
       */
      public long getEpoch() {
        return epoch_;
      }
      /**
       * <code>optional int64 epoch = 1;</code>
       */
      public Builder setEpoch(long value) {
        bitField0_ |= 0x00000001;
        epoch_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 epoch = 1;</code>
       */
      public Builder clearEpoch() {
        bitField0_ = (bitField0_ & ~0x00000001);
        epoch_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.EpochProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.EpochProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<EpochProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<EpochProto>() {
      @java.lang.Override
      public EpochProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new EpochProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<EpochProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<EpochProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.EpochProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface AMRMTokenSecretManagerStateProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.AMRMTokenSecretManagerStateProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
     */
    boolean hasCurrentMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getCurrentMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getCurrentMasterKeyOrBuilder();

    /**
     * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
     */
    boolean hasNextMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNextMasterKey();
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNextMasterKeyOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.AMRMTokenSecretManagerStateProto}
   */
  public  static final class AMRMTokenSecretManagerStateProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.AMRMTokenSecretManagerStateProto)
      AMRMTokenSecretManagerStateProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use AMRMTokenSecretManagerStateProto.newBuilder() to construct.
    private AMRMTokenSecretManagerStateProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private AMRMTokenSecretManagerStateProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private AMRMTokenSecretManagerStateProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = currentMasterKey_.toBuilder();
              }
              currentMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(currentMasterKey_);
                currentMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = nextMasterKey_.toBuilder();
              }
              nextMasterKey_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nextMasterKey_);
                nextMasterKey_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.Builder.class);
    }

    private int bitField0_;
    public static final int CURRENT_MASTER_KEY_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto currentMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
     */
    public boolean hasCurrentMasterKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getCurrentMasterKey() {
      return currentMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : currentMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getCurrentMasterKeyOrBuilder() {
      return currentMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : currentMasterKey_;
    }

    public static final int NEXT_MASTER_KEY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto nextMasterKey_;
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
     */
    public boolean hasNextMasterKey() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNextMasterKey() {
      return nextMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nextMasterKey_;
    }
    /**
     * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNextMasterKeyOrBuilder() {
      return nextMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nextMasterKey_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getCurrentMasterKey());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getNextMasterKey());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getCurrentMasterKey());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(2, getNextMasterKey());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto other = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto) obj;

      if (hasCurrentMasterKey() != other.hasCurrentMasterKey()) return false;
      if (hasCurrentMasterKey()) {
        if (!getCurrentMasterKey()
            .equals(other.getCurrentMasterKey())) return false;
      }
      if (hasNextMasterKey() != other.hasNextMasterKey()) return false;
      if (hasNextMasterKey()) {
        if (!getNextMasterKey()
            .equals(other.getNextMasterKey())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasCurrentMasterKey()) {
        hash = (37 * hash) + CURRENT_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getCurrentMasterKey().hashCode();
      }
      if (hasNextMasterKey()) {
        hash = (37 * hash) + NEXT_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getNextMasterKey().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.AMRMTokenSecretManagerStateProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.AMRMTokenSecretManagerStateProto)
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCurrentMasterKeyFieldBuilder();
          getNextMasterKeyFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (currentMasterKeyBuilder_ == null) {
          currentMasterKey_ = null;
        } else {
          currentMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (nextMasterKeyBuilder_ == null) {
          nextMasterKey_ = null;
        } else {
          nextMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto build() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto result = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (currentMasterKeyBuilder_ == null) {
            result.currentMasterKey_ = currentMasterKey_;
          } else {
            result.currentMasterKey_ = currentMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (nextMasterKeyBuilder_ == null) {
            result.nextMasterKey_ = nextMasterKey_;
          } else {
            result.nextMasterKey_ = nextMasterKeyBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto.getDefaultInstance()) return this;
        if (other.hasCurrentMasterKey()) {
          mergeCurrentMasterKey(other.getCurrentMasterKey());
        }
        if (other.hasNextMasterKey()) {
          mergeNextMasterKey(other.getNextMasterKey());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto currentMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> currentMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public boolean hasCurrentMasterKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getCurrentMasterKey() {
        if (currentMasterKeyBuilder_ == null) {
          return currentMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : currentMasterKey_;
        } else {
          return currentMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public Builder setCurrentMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (currentMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          currentMasterKey_ = value;
          onChanged();
        } else {
          currentMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public Builder setCurrentMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (currentMasterKeyBuilder_ == null) {
          currentMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          currentMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public Builder mergeCurrentMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (currentMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              currentMasterKey_ != null &&
              currentMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            currentMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(currentMasterKey_).mergeFrom(value).buildPartial();
          } else {
            currentMasterKey_ = value;
          }
          onChanged();
        } else {
          currentMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public Builder clearCurrentMasterKey() {
        if (currentMasterKeyBuilder_ == null) {
          currentMasterKey_ = null;
          onChanged();
        } else {
          currentMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getCurrentMasterKeyBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getCurrentMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getCurrentMasterKeyOrBuilder() {
        if (currentMasterKeyBuilder_ != null) {
          return currentMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return currentMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : currentMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto current_master_key = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getCurrentMasterKeyFieldBuilder() {
        if (currentMasterKeyBuilder_ == null) {
          currentMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getCurrentMasterKey(),
                  getParentForChildren(),
                  isClean());
          currentMasterKey_ = null;
        }
        return currentMasterKeyBuilder_;
      }

      private org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto nextMasterKey_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> nextMasterKeyBuilder_;
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public boolean hasNextMasterKey() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto getNextMasterKey() {
        if (nextMasterKeyBuilder_ == null) {
          return nextMasterKey_ == null ? org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nextMasterKey_;
        } else {
          return nextMasterKeyBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public Builder setNextMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (nextMasterKeyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nextMasterKey_ = value;
          onChanged();
        } else {
          nextMasterKeyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public Builder setNextMasterKey(
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder builderForValue) {
        if (nextMasterKeyBuilder_ == null) {
          nextMasterKey_ = builderForValue.build();
          onChanged();
        } else {
          nextMasterKeyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public Builder mergeNextMasterKey(org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto value) {
        if (nextMasterKeyBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              nextMasterKey_ != null &&
              nextMasterKey_ != org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance()) {
            nextMasterKey_ =
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.newBuilder(nextMasterKey_).mergeFrom(value).buildPartial();
          } else {
            nextMasterKey_ = value;
          }
          onChanged();
        } else {
          nextMasterKeyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public Builder clearNextMasterKey() {
        if (nextMasterKeyBuilder_ == null) {
          nextMasterKey_ = null;
          onChanged();
        } else {
          nextMasterKeyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder getNextMasterKeyBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getNextMasterKeyFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder getNextMasterKeyOrBuilder() {
        if (nextMasterKeyBuilder_ != null) {
          return nextMasterKeyBuilder_.getMessageOrBuilder();
        } else {
          return nextMasterKey_ == null ?
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.getDefaultInstance() : nextMasterKey_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.MasterKeyProto next_master_key = 2;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder> 
          getNextMasterKeyFieldBuilder() {
        if (nextMasterKeyBuilder_ == null) {
          nextMasterKeyBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto.Builder, org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProtoOrBuilder>(
                  getNextMasterKey(),
                  getParentForChildren(),
                  isClean());
          nextMasterKey_ = null;
        }
        return nextMasterKeyBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.AMRMTokenSecretManagerStateProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.AMRMTokenSecretManagerStateProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<AMRMTokenSecretManagerStateProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<AMRMTokenSecretManagerStateProto>() {
      @java.lang.Override
      public AMRMTokenSecretManagerStateProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new AMRMTokenSecretManagerStateProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<AMRMTokenSecretManagerStateProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<AMRMTokenSecretManagerStateProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.AMRMTokenSecretManagerStateProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RMDelegationTokenIdentifierDataProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:hadoop.yarn.RMDelegationTokenIdentifierDataProto)
      org.apache.hadoop.thirdparty.protobuf.MessageOrBuilder {

    /**
     * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
     */
    boolean hasTokenIdentifier();
    /**
     * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto getTokenIdentifier();
    /**
     * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProtoOrBuilder getTokenIdentifierOrBuilder();

    /**
     * <code>optional int64 renewDate = 2;</code>
     */
    boolean hasRenewDate();
    /**
     * <code>optional int64 renewDate = 2;</code>
     */
    long getRenewDate();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RMDelegationTokenIdentifierDataProto}
   */
  public  static final class RMDelegationTokenIdentifierDataProto extends
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:hadoop.yarn.RMDelegationTokenIdentifierDataProto)
      RMDelegationTokenIdentifierDataProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RMDelegationTokenIdentifierDataProto.newBuilder() to construct.
    private RMDelegationTokenIdentifierDataProto(org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RMDelegationTokenIdentifierDataProto() {
    }

    @java.lang.Override
    public final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RMDelegationTokenIdentifierDataProto(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.Builder unknownFields =
          org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = tokenIdentifier_.toBuilder();
              }
              tokenIdentifier_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(tokenIdentifier_);
                tokenIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              renewDate_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_descriptor;
    }

    @java.lang.Override
    protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.Builder.class);
    }

    private int bitField0_;
    public static final int TOKEN_IDENTIFIER_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto tokenIdentifier_;
    /**
     * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
     */
    public boolean hasTokenIdentifier() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto getTokenIdentifier() {
      return tokenIdentifier_ == null ? org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.getDefaultInstance() : tokenIdentifier_;
    }
    /**
     * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProtoOrBuilder getTokenIdentifierOrBuilder() {
      return tokenIdentifier_ == null ? org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.getDefaultInstance() : tokenIdentifier_;
    }

    public static final int RENEWDATE_FIELD_NUMBER = 2;
    private long renewDate_;
    /**
     * <code>optional int64 renewDate = 2;</code>
     */
    public boolean hasRenewDate() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int64 renewDate = 2;</code>
     */
    public long getRenewDate() {
      return renewDate_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(org.apache.hadoop.thirdparty.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getTokenIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(2, renewDate_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeMessageSize(1, getTokenIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += org.apache.hadoop.thirdparty.protobuf.CodedOutputStream
          .computeInt64Size(2, renewDate_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto other = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto) obj;

      if (hasTokenIdentifier() != other.hasTokenIdentifier()) return false;
      if (hasTokenIdentifier()) {
        if (!getTokenIdentifier()
            .equals(other.getTokenIdentifier())) return false;
      }
      if (hasRenewDate() != other.hasRenewDate()) return false;
      if (hasRenewDate()) {
        if (getRenewDate()
            != other.getRenewDate()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTokenIdentifier()) {
        hash = (37 * hash) + TOKEN_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getTokenIdentifier().hashCode();
      }
      if (hasRenewDate()) {
        hash = (37 * hash) + RENEWDATE_FIELD_NUMBER;
        hash = (53 * hash) + org.apache.hadoop.thirdparty.protobuf.Internal.hashLong(
            getRenewDate());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        java.nio.ByteBuffer data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        java.nio.ByteBuffer data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.ByteString data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(byte[] data)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        byte[] data,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseDelimitedFrom(
        java.io.InputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parseFrom(
        org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
        org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RMDelegationTokenIdentifierDataProto}
     */
    public static final class Builder extends
        org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:hadoop.yarn.RMDelegationTokenIdentifierDataProto)
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProtoOrBuilder {
      public static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_descriptor;
      }

      @java.lang.Override
      protected org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.class, org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getTokenIdentifierFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (tokenIdentifierBuilder_ == null) {
          tokenIdentifier_ = null;
        } else {
          tokenIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        renewDate_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto build() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto result = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (tokenIdentifierBuilder_ == null) {
            result.tokenIdentifier_ = tokenIdentifier_;
          } else {
            result.tokenIdentifier_ = tokenIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.renewDate_ = renewDate_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          org.apache.hadoop.thirdparty.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(org.apache.hadoop.thirdparty.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.getDefaultInstance()) return this;
        if (other.hasTokenIdentifier()) {
          mergeTokenIdentifier(other.getTokenIdentifier());
        }
        if (other.hasRenewDate()) {
          setRenewDate(other.getRenewDate());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto tokenIdentifier_;
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto, org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.Builder, org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProtoOrBuilder> tokenIdentifierBuilder_;
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public boolean hasTokenIdentifier() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto getTokenIdentifier() {
        if (tokenIdentifierBuilder_ == null) {
          return tokenIdentifier_ == null ? org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.getDefaultInstance() : tokenIdentifier_;
        } else {
          return tokenIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public Builder setTokenIdentifier(org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto value) {
        if (tokenIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          tokenIdentifier_ = value;
          onChanged();
        } else {
          tokenIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public Builder setTokenIdentifier(
          org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.Builder builderForValue) {
        if (tokenIdentifierBuilder_ == null) {
          tokenIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          tokenIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public Builder mergeTokenIdentifier(org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto value) {
        if (tokenIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              tokenIdentifier_ != null &&
              tokenIdentifier_ != org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.getDefaultInstance()) {
            tokenIdentifier_ =
              org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.newBuilder(tokenIdentifier_).mergeFrom(value).buildPartial();
          } else {
            tokenIdentifier_ = value;
          }
          onChanged();
        } else {
          tokenIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public Builder clearTokenIdentifier() {
        if (tokenIdentifierBuilder_ == null) {
          tokenIdentifier_ = null;
          onChanged();
        } else {
          tokenIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.Builder getTokenIdentifierBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getTokenIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProtoOrBuilder getTokenIdentifierOrBuilder() {
        if (tokenIdentifierBuilder_ != null) {
          return tokenIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return tokenIdentifier_ == null ?
              org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.getDefaultInstance() : tokenIdentifier_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.YARNDelegationTokenIdentifierProto token_identifier = 1;</code>
       */
      private org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto, org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.Builder, org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProtoOrBuilder> 
          getTokenIdentifierFieldBuilder() {
        if (tokenIdentifierBuilder_ == null) {
          tokenIdentifierBuilder_ = new org.apache.hadoop.thirdparty.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto, org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProto.Builder, org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.YARNDelegationTokenIdentifierProtoOrBuilder>(
                  getTokenIdentifier(),
                  getParentForChildren(),
                  isClean());
          tokenIdentifier_ = null;
        }
        return tokenIdentifierBuilder_;
      }

      private long renewDate_ ;
      /**
       * <code>optional int64 renewDate = 2;</code>
       */
      public boolean hasRenewDate() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int64 renewDate = 2;</code>
       */
      public long getRenewDate() {
        return renewDate_;
      }
      /**
       * <code>optional int64 renewDate = 2;</code>
       */
      public Builder setRenewDate(long value) {
        bitField0_ |= 0x00000002;
        renewDate_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 renewDate = 2;</code>
       */
      public Builder clearRenewDate() {
        bitField0_ = (bitField0_ & ~0x00000002);
        renewDate_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RMDelegationTokenIdentifierDataProto)
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RMDelegationTokenIdentifierDataProto)
    private static final org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto();
    }

    public static org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final org.apache.hadoop.thirdparty.protobuf.Parser<RMDelegationTokenIdentifierDataProto>
        PARSER = new org.apache.hadoop.thirdparty.protobuf.AbstractParser<RMDelegationTokenIdentifierDataProto>() {
      @java.lang.Override
      public RMDelegationTokenIdentifierDataProto parsePartialFrom(
          org.apache.hadoop.thirdparty.protobuf.CodedInputStream input,
          org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite extensionRegistry)
          throws org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException {
        return new RMDelegationTokenIdentifierDataProto(input, extensionRegistry);
      }
    };

    public static org.apache.hadoop.thirdparty.protobuf.Parser<RMDelegationTokenIdentifierDataProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.thirdparty.protobuf.Parser<RMDelegationTokenIdentifierDataProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ApplicationStateDataProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_ApplicationStateDataProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_EpochProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_EpochProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_fieldAccessorTable;
  private static final org.apache.hadoop.thirdparty.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_descriptor;
  private static final 
    org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_fieldAccessorTable;

  public static org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n*yarn_server_resourcemanager_recovery.p" +
      "roto\022\013hadoop.yarn\032\037yarn_server_common_pr" +
      "otos.proto\032\021yarn_protos.proto\032\031yarn_secu" +
      "rity_token.proto\032\017RpcHeader.proto\"\277\003\n\031Ap" +
      "plicationStateDataProto\022\023\n\013submit_time\030\001" +
      " \001(\003\022V\n\036application_submission_context\030\002" +
      " \001(\0132..hadoop.yarn.ApplicationSubmission" +
      "ContextProto\022\014\n\004user\030\003 \001(\t\022\022\n\nstart_time" +
      "\030\004 \001(\003\0227\n\021application_state\030\005 \001(\0162\034.hado" +
      "op.yarn.RMAppStateProto\022\030\n\013diagnostics\030\006" +
      " \001(\t:\003N/A\022\023\n\013finish_time\030\007 \001(\003\022<\n\016caller" +
      "_context\030\010 \001(\0132$.hadoop.common.RPCCaller" +
      "ContextProto\022E\n\024application_timeouts\030\t \003" +
      "(\0132\'.hadoop.yarn.ApplicationTimeoutMapPr" +
      "oto\022\023\n\013launch_time\030\n \001(\003\022\021\n\treal_user\030\013 " +
      "\001(\t\"\352\005\n ApplicationAttemptStateDataProto" +
      "\0229\n\tattemptId\030\001 \001(\0132&.hadoop.yarn.Applic" +
      "ationAttemptIdProto\0225\n\020master_container\030" +
      "\002 \001(\0132\033.hadoop.yarn.ContainerProto\022\032\n\022ap" +
      "p_attempt_tokens\030\003 \001(\014\022>\n\021app_attempt_st" +
      "ate\030\004 \001(\0162#.hadoop.yarn.RMAppAttemptStat" +
      "eProto\022\032\n\022final_tracking_url\030\005 \001(\t\022\030\n\013di" +
      "agnostics\030\006 \001(\t:\003N/A\022\022\n\nstart_time\030\007 \001(\003" +
      "\022J\n\030final_application_status\030\010 \001(\0162(.had" +
      "oop.yarn.FinalApplicationStatusProto\022\'\n\030" +
      "am_container_exit_status\030\t \001(\005:\005-1000\022\026\n" +
      "\016memory_seconds\030\n \001(\003\022\025\n\rvcore_seconds\030\013" +
      " \001(\003\022\023\n\013finish_time\030\014 \001(\003\022 \n\030preempted_m" +
      "emory_seconds\030\r \001(\003\022\037\n\027preempted_vcore_s" +
      "econds\030\016 \001(\003\022G\n\036application_resource_usa" +
      "ge_map\030\017 \003(\0132\037.hadoop.yarn.StringLongMap" +
      "Proto\022E\n\034preempted_resource_usage_map\030\020 " +
      "\003(\0132\037.hadoop.yarn.StringLongMapProto\022\"\n\032" +
      "total_allocated_containers\030\021 \001(\005\"\033\n\nEpoc" +
      "hProto\022\r\n\005epoch\030\001 \001(\003\"\221\001\n AMRMTokenSecre" +
      "tManagerStateProto\0227\n\022current_master_key" +
      "\030\001 \001(\0132\033.hadoop.yarn.MasterKeyProto\0224\n\017n" +
      "ext_master_key\030\002 \001(\0132\033.hadoop.yarn.Maste" +
      "rKeyProto\"\204\001\n$RMDelegationTokenIdentifie" +
      "rDataProto\022I\n\020token_identifier\030\001 \001(\0132/.h" +
      "adoop.yarn.YARNDelegationTokenIdentifier" +
      "Proto\022\021\n\trenewDate\030\002 \001(\003*\200\003\n\026RMAppAttemp" +
      "tStateProto\022\021\n\rRMATTEMPT_NEW\020\001\022\027\n\023RMATTE" +
      "MPT_SUBMITTED\020\002\022\027\n\023RMATTEMPT_SCHEDULED\020\003" +
      "\022\027\n\023RMATTEMPT_ALLOCATED\020\004\022\026\n\022RMATTEMPT_L" +
      "AUNCHED\020\005\022\024\n\020RMATTEMPT_FAILED\020\006\022\025\n\021RMATT" +
      "EMPT_RUNNING\020\007\022\027\n\023RMATTEMPT_FINISHING\020\010\022" +
      "\026\n\022RMATTEMPT_FINISHED\020\t\022\024\n\020RMATTEMPT_KIL" +
      "LED\020\n\022\036\n\032RMATTEMPT_ALLOCATED_SAVING\020\013\022\'\n" +
      "#RMATTEMPT_LAUNCHED_UNMANAGED_SAVING\020\014\022\027" +
      "\n\023RMATTEMPT_RECOVERED\020\r\022\032\n\026RMATTEMPT_FIN" +
      "AL_SAVING\020\016*\327\001\n\017RMAppStateProto\022\r\n\tRMAPP" +
      "_NEW\020\001\022\024\n\020RMAPP_NEW_SAVING\020\002\022\023\n\017RMAPP_SU" +
      "BMITTED\020\003\022\022\n\016RMAPP_ACCEPTED\020\004\022\021\n\rRMAPP_R" +
      "UNNING\020\005\022\026\n\022RMAPP_FINAL_SAVING\020\006\022\023\n\017RMAP" +
      "P_FINISHING\020\007\022\022\n\016RMAPP_FINISHED\020\010\022\020\n\014RMA" +
      "PP_FAILED\020\t\022\020\n\014RMAPP_KILLED\020\nBM\n\034org.apa" +
      "che.hadoop.yarn.protoB\'YarnServerResourc" +
      "eManagerRecoveryProtos\210\001\001\240\001\001"
    };
    org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
          public org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry assignDescriptors(
              org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor root) {
            descriptor = root;
            return null;
          }
        };
    org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new org.apache.hadoop.thirdparty.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.yarn.proto.YarnServerCommonProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.getDescriptor(),
          org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.getDescriptor(),
        }, assigner);
    internal_static_hadoop_yarn_ApplicationStateDataProto_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_hadoop_yarn_ApplicationStateDataProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_ApplicationStateDataProto_descriptor,
        new java.lang.String[] { "SubmitTime", "ApplicationSubmissionContext", "User", "StartTime", "ApplicationState", "Diagnostics", "FinishTime", "CallerContext", "ApplicationTimeouts", "LaunchTime", "RealUser", });
    internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_ApplicationAttemptStateDataProto_descriptor,
        new java.lang.String[] { "AttemptId", "MasterContainer", "AppAttemptTokens", "AppAttemptState", "FinalTrackingUrl", "Diagnostics", "StartTime", "FinalApplicationStatus", "AmContainerExitStatus", "MemorySeconds", "VcoreSeconds", "FinishTime", "PreemptedMemorySeconds", "PreemptedVcoreSeconds", "ApplicationResourceUsageMap", "PreemptedResourceUsageMap", "TotalAllocatedContainers", });
    internal_static_hadoop_yarn_EpochProto_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_hadoop_yarn_EpochProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_EpochProto_descriptor,
        new java.lang.String[] { "Epoch", });
    internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_AMRMTokenSecretManagerStateProto_descriptor,
        new java.lang.String[] { "CurrentMasterKey", "NextMasterKey", });
    internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_fieldAccessorTable = new
      org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_hadoop_yarn_RMDelegationTokenIdentifierDataProto_descriptor,
        new java.lang.String[] { "TokenIdentifier", "RenewDate", });
    org.apache.hadoop.yarn.proto.YarnServerCommonProtos.getDescriptor();
    org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor();
    org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos.getDescriptor();
    org.apache.hadoop.ipc.protobuf.RpcHeaderProtos.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}
